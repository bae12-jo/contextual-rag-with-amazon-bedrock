From e7b14a4b0681c64df45e2cd65ba077974baa19c7 Mon Sep 17 00:00:00 2001
From: EC2 Default User <ec2-user@ip-172-31-43-154.us-west-2.compute.internal>
Date: Mon, 24 Mar 2025 01:43:11 +0000
Subject: [PATCH] Update file structure.

---
 .../sagemakerNotebookStack.ts"                |   26 +-
 notebook/0_configuration.ipynb                |  664 ++++------
 notebook/1_file_processor.ipynb               | 1113 ++++++++++-------
 notebook/2_qustion_generator.ipynb            |  834 ++++++------
 notebook/3_rag.ipynb                          |  993 ++++++++++-----
 notebook/config.py                            |    2 +-
 notebook/libs/contextual_rag_service.py       |   35 +-
 notebook/libs/opensearch_service.py           |    4 +-
 notebook/requirements.txt                     |    1 +
 {utils => notebook/utils}/bedrock.py          |    0
 {utils => notebook/utils}/opensearch.py       |    0
 {utils => notebook/utils}/ssm.py              |    0
 package-lock.json                             |    7 +-
 package.json                                  |    2 +-
 test/contextual-retrieval.test.ts             |   17 -
 15 files changed, 2061 insertions(+), 1637 deletions(-)
 rename {utils => notebook/utils}/bedrock.py (100%)
 rename {utils => notebook/utils}/opensearch.py (100%)
 rename {utils => notebook/utils}/ssm.py (100%)
 delete mode 100644 test/contextual-retrieval.test.ts

diff --git "a/lib/\bsagemakerNotebookStack/sagemakerNotebookStack.ts" "b/lib/\bsagemakerNotebookStack/sagemakerNotebookStack.ts"
index 5520d73..2441532 100644
--- "a/lib/\bsagemakerNotebookStack/sagemakerNotebookStack.ts"
+++ "b/lib/\bsagemakerNotebookStack/sagemakerNotebookStack.ts"
@@ -11,7 +11,7 @@ export class SagemakerNotebookStack extends cdk.Stack {
     super(scope, id, props);
 
     // The code that defines your stack goes here
-    
+
     // IAM Role
     const SageMakerNotebookinstanceRole = new iam.Role(this, 'SageMakerNotebookInstanceRole', {
       assumedBy: new iam.ServicePrincipal('sagemaker.amazonaws.com'),
@@ -24,32 +24,32 @@ export class SagemakerNotebookStack extends cdk.Stack {
         iam.ManagedPolicy.fromAwsManagedPolicyName('SecretsManagerReadWrite')
       ],
     });
-    
-    
+
+
     // SageMaker Notebook Instance Lifecycle Configuration
-    
+
     const onCreateScriptPath1 = path.join(__dirname, 'install_packages.sh')
     const onCreateScriptContent = fs.readFileSync(onCreateScriptPath1, 'utf-8')
-    
+
     const cfnNotebookInstanceLifecycleConfig = new sagemaker.CfnNotebookInstanceLifecycleConfig(this, 'MyCfnNotebookInstanceLifecycleConfig', /* all optional props */ {
       notebookInstanceLifecycleConfigName: 'notebookInstanceLifecycleConfig',
       onCreate: [{
-          content: cdk.Fn.base64(onCreateScriptContent),
-        }],
+        content: cdk.Fn.base64(onCreateScriptContent),
+      }],
       onStart: [],
     });
-    
-    
+
+
     // SageMaker Notebook Instance
-    
+
     const cfnNotebookInstance = new sagemaker.CfnNotebookInstance(this, 'MyCfnNotebookInstance', {
       instanceType: 'ml.m5.xlarge',
       roleArn: SageMakerNotebookinstanceRole.roleArn,
-    
+
       // the properties below are optional
       //acceleratorTypes: ['acceleratorTypes'],
       //additionalCodeRepositories: ['additionalCodeRepositories'],
-      defaultCodeRepository: 'https://github.com/beryh/contextual-rag.git',
+      defaultCodeRepository: 'https://github.com/bae12-jo/contextual-rag-with-amazon-bedrock.git',
       directInternetAccess: 'Enabled',
       //instanceMetadataServiceConfiguration: {
       //  minimumInstanceMetadataServiceVersion: 'minimumInstanceMetadataServiceVersion',
@@ -67,7 +67,7 @@ export class SagemakerNotebookStack extends cdk.Stack {
       //}],
       volumeSizeInGb: 10,
     });
-  
+
 
   }
 }
\ No newline at end of file
diff --git a/notebook/0_configuration.ipynb b/notebook/0_configuration.ipynb
index 3af4af7..6035341 100644
--- a/notebook/0_configuration.ipynb
+++ b/notebook/0_configuration.ipynb
@@ -4,52 +4,34 @@
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "# Configuration"
+    "# Configuration for Contextual RAG with Amazon Bedrock\n",
+    "\n",
+    "This notebook sets up the necessary AWS configurations for a Contextual RAG (Retrieval-Augmented Generation) system using Amazon Bedrock and OpenSearch. Follow the steps sequentially to configure your environment properly."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Import Required Libraries"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": null,
    "metadata": {
     "tags": [],
     "vscode": {
      "languageId": "shellscript"
     }
    },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Requirement already satisfied: ipywidgets in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (8.1.5)\n",
-      "Requirement already satisfied: comm>=0.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
-      "Requirement already satisfied: ipython>=6.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipywidgets) (8.31.0)\n",
-      "Requirement already satisfied: traitlets>=4.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
-      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
-      "Requirement already satisfied: jupyterlab_widgets~=3.0.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
-      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
-      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
-      "Requirement already satisfied: jedi>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
-      "Requirement already satisfied: matplotlib-inline in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
-      "Requirement already satisfied: pexpect>4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
-      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
-      "Requirement already satisfied: pygments>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
-      "Requirement already satisfied: stack_data in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
-      "Requirement already satisfied: typing_extensions>=4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
-      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
-      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
-      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
-      "Requirement already satisfied: executing>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
-      "Requirement already satisfied: asttokens>=2.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
-      "Requirement already satisfied: pure_eval in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
-      "Note: you may need to restart the kernel to use updated packages.\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "%load_ext autoreload\n",
     "%autoreload 2\n",
-    "%pip install ipywidgets"
+    "\n",
+    "# Install required packages\n",
+    "%pip install -r requirements.txt"
    ]
   },
   {
@@ -58,355 +40,73 @@
     "tags": []
    },
    "source": [
-    "## 1. AWS Configuration"
+    "## 1. AWS Configuration\n",
+    "This section sets up the AWS credentials and region for subsequent operations."
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": null,
    "metadata": {
     "tags": []
    },
-   "outputs": [
-    {
-     "data": {
-      "text/html": [
-       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/10/25 22:43:16] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/credentials.py#1075\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1075</span></a>\n",
-       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
-       "</pre>\n"
-      ],
-      "text/plain": [
-       "\u001b[2;36m[02/10/25 22:43:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=645973;file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=261935;file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/credentials.py#1075\u001b\\\u001b[2m1075\u001b[0m\u001b]8;;\u001b\\\n",
-       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
+   "outputs": [],
    "source": [
+    "import os\n",
     "import boto3\n",
+    "import json\n",
     "from sagemaker import get_execution_role\n",
+    "import sys\n",
+    "import requests\n",
+    "from pprint import pprint\n",
+    "\n",
+    "# Get AWS role and session\n",
     "role = get_execution_role()\n",
     "session = boto3.Session()"
    ]
   },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "os.environ[\"AWS_DEFAULT_REGION\"] = boto3.Session().region_name #us-west-2\n",
-    "os.environ[\"AWS_PROFILE\"] = session #role"
-   ]
-  },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "## 2. Bedrock Configuration"
+    "## 2. Bedrock Configuration\n",
+    "Configure Amazon Bedrock service with appropriate model IDs."
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": null,
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
-    "import os\n",
-    "import requests\n",
-    "from pprint import pprint\n",
-    "from utils.bedrock import BedrockClient"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 6,
-   "metadata": {
-    "tags": []
-   },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "\n",
-      "== All FM lists ==\n"
-     ]
-    },
-    {
-     "data": {
-      "text/html": [
-       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/10/25 22:43:59] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/credentials.py#1075\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1075</span></a>\n",
-       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
-       "</pre>\n"
-      ],
-      "text/plain": [
-       "\u001b[2;36m[02/10/25 22:43:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=816669;file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=765975;file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/credentials.py#1075\u001b\\\u001b[2m1075\u001b[0m\u001b]8;;\u001b\\\n",
-       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "['amazon.titan-tg1-large',\n",
-      " 'amazon.titan-embed-g1-text-02',\n",
-      " 'amazon.titan-text-lite-v1:0:4k',\n",
-      " 'amazon.titan-text-lite-v1',\n",
-      " 'amazon.titan-text-express-v1:0:8k',\n",
-      " 'amazon.titan-text-express-v1',\n",
-      " 'amazon.nova-pro-v1:0',\n",
-      " 'amazon.nova-lite-v1:0',\n",
-      " 'amazon.nova-micro-v1:0',\n",
-      " 'amazon.titan-embed-text-v1:2:8k',\n",
-      " 'amazon.titan-embed-text-v1',\n",
-      " 'amazon.titan-embed-text-v2:0',\n",
-      " 'amazon.titan-embed-image-v1:0',\n",
-      " 'amazon.titan-embed-image-v1',\n",
-      " 'amazon.titan-image-generator-v1:0',\n",
-      " 'amazon.titan-image-generator-v1',\n",
-      " 'amazon.titan-image-generator-v2:0',\n",
-      " 'amazon.rerank-v1:0',\n",
-      " 'stability.stable-diffusion-xl-v1:0',\n",
-      " 'stability.stable-diffusion-xl-v1',\n",
-      " 'stability.sd3-large-v1:0',\n",
-      " 'stability.sd3-5-large-v1:0',\n",
-      " 'stability.stable-image-core-v1:0',\n",
-      " 'stability.stable-image-core-v1:1',\n",
-      " 'stability.stable-image-ultra-v1:0',\n",
-      " 'stability.stable-image-ultra-v1:1',\n",
-      " 'anthropic.claude-3-5-sonnet-20241022-v2:0:18k',\n",
-      " 'anthropic.claude-3-5-sonnet-20241022-v2:0:51k',\n",
-      " 'anthropic.claude-3-5-sonnet-20241022-v2:0:200k',\n",
-      " 'anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
-      " 'anthropic.claude-3-5-haiku-20241022-v1:0',\n",
-      " 'anthropic.claude-instant-v1:2:100k',\n",
-      " 'anthropic.claude-instant-v1',\n",
-      " 'anthropic.claude-v2:0:18k',\n",
-      " 'anthropic.claude-v2:0:100k',\n",
-      " 'anthropic.claude-v2:1:18k',\n",
-      " 'anthropic.claude-v2:1:200k',\n",
-      " 'anthropic.claude-v2:1',\n",
-      " 'anthropic.claude-v2',\n",
-      " 'anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
-      " 'anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
-      " 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
-      " 'anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
-      " 'anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
-      " 'anthropic.claude-3-haiku-20240307-v1:0',\n",
-      " 'anthropic.claude-3-opus-20240229-v1:0:12k',\n",
-      " 'anthropic.claude-3-opus-20240229-v1:0:28k',\n",
-      " 'anthropic.claude-3-opus-20240229-v1:0:200k',\n",
-      " 'anthropic.claude-3-opus-20240229-v1:0',\n",
-      " 'anthropic.claude-3-5-sonnet-20240620-v1:0:18k',\n",
-      " 'anthropic.claude-3-5-sonnet-20240620-v1:0:51k',\n",
-      " 'anthropic.claude-3-5-sonnet-20240620-v1:0:200k',\n",
-      " 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
-      " 'cohere.command-text-v14:7:4k',\n",
-      " 'cohere.command-text-v14',\n",
-      " 'cohere.command-r-v1:0',\n",
-      " 'cohere.command-r-plus-v1:0',\n",
-      " 'cohere.command-light-text-v14:7:4k',\n",
-      " 'cohere.command-light-text-v14',\n",
-      " 'cohere.embed-english-v3:0:512',\n",
-      " 'cohere.embed-english-v3',\n",
-      " 'cohere.embed-multilingual-v3:0:512',\n",
-      " 'cohere.embed-multilingual-v3',\n",
-      " 'cohere.rerank-v3-5:0',\n",
-      " 'meta.llama3-8b-instruct-v1:0',\n",
-      " 'meta.llama3-70b-instruct-v1:0',\n",
-      " 'meta.llama3-1-8b-instruct-v1:0:128k',\n",
-      " 'meta.llama3-1-8b-instruct-v1:0',\n",
-      " 'meta.llama3-1-70b-instruct-v1:0:128k',\n",
-      " 'meta.llama3-1-70b-instruct-v1:0',\n",
-      " 'meta.llama3-1-405b-instruct-v1:0',\n",
-      " 'meta.llama3-2-11b-instruct-v1:0',\n",
-      " 'meta.llama3-2-90b-instruct-v1:0',\n",
-      " 'meta.llama3-2-1b-instruct-v1:0',\n",
-      " 'meta.llama3-2-3b-instruct-v1:0',\n",
-      " 'meta.llama3-3-70b-instruct-v1:0',\n",
-      " 'mistral.mistral-7b-instruct-v0:2',\n",
-      " 'mistral.mixtral-8x7b-instruct-v0:1',\n",
-      " 'mistral.mistral-large-2402-v1:0',\n",
-      " 'mistral.mistral-large-2407-v1:0',\n",
-      " 'luma.ray-v2:0']\n"
-     ]
-    }
-   ],
-   "source": [
-    "# 결과 출력\n",
-    "print(\"\\n== All FM lists ==\")\n",
+    "from utils.bedrock import BedrockClient\n",
+    "\n",
+    "# List available foundation models\n",
+    "print(\"\\n== Available Foundation Models ==\")\n",
     "all_models = BedrockClient.get_list_fm_models()\n",
     "pprint(all_models)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 11,
+   "execution_count": null,
    "metadata": {
     "tags": []
    },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "\u001b[32m\n",
-      "== All FM lists ==\u001b[0m\n",
-      "['amazon.titan-tg1-large',\n",
-      " 'amazon.titan-embed-g1-text-02',\n",
-      " 'amazon.titan-text-lite-v1:0:4k',\n",
-      " 'amazon.titan-text-lite-v1',\n",
-      " 'amazon.titan-text-express-v1:0:8k',\n",
-      " 'amazon.titan-text-express-v1',\n",
-      " 'amazon.nova-pro-v1:0',\n",
-      " 'amazon.nova-lite-v1:0',\n",
-      " 'amazon.nova-micro-v1:0',\n",
-      " 'amazon.titan-embed-text-v1:2:8k',\n",
-      " 'amazon.titan-embed-text-v1',\n",
-      " 'amazon.titan-embed-text-v2:0',\n",
-      " 'amazon.titan-embed-image-v1:0',\n",
-      " 'amazon.titan-embed-image-v1',\n",
-      " 'amazon.titan-image-generator-v1:0',\n",
-      " 'amazon.titan-image-generator-v1',\n",
-      " 'amazon.titan-image-generator-v2:0',\n",
-      " 'amazon.rerank-v1:0',\n",
-      " 'stability.stable-diffusion-xl-v1:0',\n",
-      " 'stability.stable-diffusion-xl-v1',\n",
-      " 'stability.sd3-large-v1:0',\n",
-      " 'stability.sd3-5-large-v1:0',\n",
-      " 'stability.stable-image-core-v1:0',\n",
-      " 'stability.stable-image-core-v1:1',\n",
-      " 'stability.stable-image-ultra-v1:0',\n",
-      " 'stability.stable-image-ultra-v1:1',\n",
-      " 'anthropic.claude-3-5-sonnet-20241022-v2:0:18k',\n",
-      " 'anthropic.claude-3-5-sonnet-20241022-v2:0:51k',\n",
-      " 'anthropic.claude-3-5-sonnet-20241022-v2:0:200k',\n",
-      " 'anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
-      " 'anthropic.claude-3-5-haiku-20241022-v1:0',\n",
-      " 'anthropic.claude-instant-v1:2:100k',\n",
-      " 'anthropic.claude-instant-v1',\n",
-      " 'anthropic.claude-v2:0:18k',\n",
-      " 'anthropic.claude-v2:0:100k',\n",
-      " 'anthropic.claude-v2:1:18k',\n",
-      " 'anthropic.claude-v2:1:200k',\n",
-      " 'anthropic.claude-v2:1',\n",
-      " 'anthropic.claude-v2',\n",
-      " 'anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
-      " 'anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
-      " 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
-      " 'anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
-      " 'anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
-      " 'anthropic.claude-3-haiku-20240307-v1:0',\n",
-      " 'anthropic.claude-3-opus-20240229-v1:0:12k',\n",
-      " 'anthropic.claude-3-opus-20240229-v1:0:28k',\n",
-      " 'anthropic.claude-3-opus-20240229-v1:0:200k',\n",
-      " 'anthropic.claude-3-opus-20240229-v1:0',\n",
-      " 'anthropic.claude-3-5-sonnet-20240620-v1:0:18k',\n",
-      " 'anthropic.claude-3-5-sonnet-20240620-v1:0:51k',\n",
-      " 'anthropic.claude-3-5-sonnet-20240620-v1:0:200k',\n",
-      " 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
-      " 'cohere.command-text-v14:7:4k',\n",
-      " 'cohere.command-text-v14',\n",
-      " 'cohere.command-r-v1:0',\n",
-      " 'cohere.command-r-plus-v1:0',\n",
-      " 'cohere.command-light-text-v14:7:4k',\n",
-      " 'cohere.command-light-text-v14',\n",
-      " 'cohere.embed-english-v3:0:512',\n",
-      " 'cohere.embed-english-v3',\n",
-      " 'cohere.embed-multilingual-v3:0:512',\n",
-      " 'cohere.embed-multilingual-v3',\n",
-      " 'cohere.rerank-v3-5:0',\n",
-      " 'meta.llama3-8b-instruct-v1:0',\n",
-      " 'meta.llama3-70b-instruct-v1:0',\n",
-      " 'meta.llama3-1-8b-instruct-v1:0:128k',\n",
-      " 'meta.llama3-1-8b-instruct-v1:0',\n",
-      " 'meta.llama3-1-70b-instruct-v1:0:128k',\n",
-      " 'meta.llama3-1-70b-instruct-v1:0',\n",
-      " 'meta.llama3-1-405b-instruct-v1:0',\n",
-      " 'meta.llama3-2-11b-instruct-v1:0',\n",
-      " 'meta.llama3-2-90b-instruct-v1:0',\n",
-      " 'meta.llama3-2-1b-instruct-v1:0',\n",
-      " 'meta.llama3-2-3b-instruct-v1:0',\n",
-      " 'meta.llama3-3-70b-instruct-v1:0',\n",
-      " 'mistral.mistral-7b-instruct-v0:2',\n",
-      " 'mistral.mixtral-8x7b-instruct-v0:1',\n",
-      " 'mistral.mistral-large-2402-v1:0',\n",
-      " 'mistral.mistral-large-2407-v1:0',\n",
-      " 'luma.ray-v2:0']\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
-    "# # Bedrock 클라이언트 생성 함수\n",
-    "# def get_bedrock_client(region=None):\n",
-    "#     session = boto3.Session()\n",
-    "    \n",
-    "#     config = Config(\n",
-    "#         retries={\n",
-    "#             'max_attempts': 10,\n",
-    "#             'mode': 'standard'\n",
-    "#         }\n",
-    "#     )\n",
-    "    \n",
-    "#     return session.client(\n",
-    "#         service_name='bedrock',\n",
-    "#         region_name=region,\n",
-    "#         config=config\n",
-    "#     )\n",
-    "\n",
-    "# # Bedrock 클라이언트 생성\n",
-    "# boto3_bedrock = get_bedrock_client(\n",
-    "#     region=boto3.Session().region_name\n",
-    "# )\n",
-    "\n",
-    "# # Bedrock 모델 리스트 가져오기 함수\n",
-    "# def get_list_fm_models(verbose=False):\n",
-    "#     response = boto3_bedrock.list_foundation_models()\n",
-    "#     models = response['modelSummaries']\n",
-    "    \n",
-    "#     if verbose:\n",
-    "#         return models\n",
-    "#     else:\n",
-    "#         return [model['modelId'] for model in models]\n",
+    "# Set Bedrock model IDs\n",
+    "# Claude 3.5 Sonnet for text generation\n",
+    "os.environ[\"BEDROCK_MODEL_ID\"] = \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
+    "# Titan Embeddings for vector generation\n",
+    "os.environ[\"EMBED_MODEL_ID\"] = \"amazon.titan-embed-text-v2:0\"\n",
+    "os.environ[\"BEDROCK_RETRIES\"] = \"10\"\n",
     "\n",
-    "# # 결과 출력\n",
-    "# print(colored(\"\\n== All FM lists ==\", \"green\"))\n",
-    "# all_models = get_list_fm_models(verbose=False)\n",
-    "# pprint(all_models)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 3,
-   "metadata": {
-    "tags": []
-   },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Collecting termcolor\n",
-      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
-      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
-      "Installing collected packages: termcolor\n",
-      "Successfully installed termcolor-2.5.0\n",
-      "Note: you may need to restart the kernel to use updated packages.\n"
-     ]
-    }
-   ],
-   "source": [
-    "os.environ[\"BEDROCK_MODEL_ID\"] = \"\"\n",
-    "os.environ[\"BEDROCK_RETRIES\"] = \"\"\n",
-    "os.environ[\"EMBED_MODEL_ID\"] = \"\""
+    "print(f\"\\nBedrock Models configured:\")\n",
+    "print(f\"- Text Generation: {os.environ['BEDROCK_MODEL_ID']}\")\n",
+    "print(f\"- Embeddings: {os.environ['EMBED_MODEL_ID']}\")"
    ]
   },
   {
@@ -415,7 +115,8 @@
     "tags": []
    },
    "source": [
-    "## 3. Opensearch Configuration"
+    "## 3. Opensearch Configuration\n",
+    "Set up the OpenSearch service for vector storage and retrieval."
    ]
   },
   {
@@ -427,41 +128,29 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
+   "execution_count": null,
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "from utils.ssm import parameter_store\n",
-    "region=boto3.Session().region_name\n",
-    "pm = parameter_store(region)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 11,
-   "metadata": {
-    "tags": []
-   },
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Parameter stored successfully.\n"
-     ]
-    }
-   ],
-   "source": [
+    "\n",
+    "# Create parameter store for configurations\n",
+    "region = boto3.Session().region_name\n",
+    "pm = parameter_store(region)\n",
+    "\n",
+    "# Define index name for OpenSearch\n",
     "index_name = \"default_doc_index\" \n",
     "\n",
+    "# Store the index name in parameter store\n",
     "pm.put_params(\n",
     "    key=\"opensearch_index_name\",\n",
     "    value=f'{index_name}',\n",
     "    overwrite=True,\n",
     "    enc=False\n",
-    ")"
+    ")\n",
+    "print(f\"Index name '{index_name}' stored in parameter store\")"
    ]
   },
   {
@@ -470,7 +159,8 @@
     "tags": []
    },
    "source": [
-    "### 3-2. Define Index Schema"
+    "### 3-2. Define Index Schema\n",
+    "The schema includes support for Korean text analysis using Nori plugin and vector search capabilities."
    ]
   },
   {
@@ -479,17 +169,18 @@
    "metadata": {},
    "outputs": [],
    "source": [
+    "# Define vector dimension for embeddings\n",
+    "dimension = 1024\n",
+    "\n",
+    "# Define the index schema with Nori analyzer for Korean text\n",
     "index_body = {\n",
     "    'settings': {\n",
     "        'analysis': {\n",
     "            'analyzer': {\n",
     "                'my_analyzer': {\n",
-    "                         'char_filter':['html_strip'],\n",
+    "                    'char_filter': ['html_strip'],\n",
     "                    'tokenizer': 'nori',\n",
     "                    'filter': [\n",
-    "                        #'nori_number',\n",
-    "                        #'lowercase',\n",
-    "                        #'trim',\n",
     "                        'my_nori_part_of_speech'\n",
     "                    ],\n",
     "                    'type': 'custom'\n",
@@ -506,10 +197,10 @@
     "                \"my_nori_part_of_speech\": {\n",
     "                    \"type\": \"nori_part_of_speech\",\n",
     "                    \"stoptags\": [\n",
-    "                        \"J\", \"XSV\", \"E\", \"IC\",\"MAJ\",\"NNB\",\n",
+    "                        \"J\", \"XSV\", \"E\", \"IC\", \"MAJ\", \"NNB\",\n",
     "                        \"SP\", \"SSC\", \"SSO\",\n",
-    "                        \"SC\",\"SE\",\"XSN\",\"XSV\",\n",
-    "                        \"UNA\",\"NA\",\"VCP\",\"VSV\",\n",
+    "                        \"SC\", \"SE\", \"XSN\", \"XSV\",\n",
+    "                        \"UNA\", \"NA\", \"VCP\", \"VSV\",\n",
     "                        \"VX\"\n",
     "                    ]\n",
     "                }\n",
@@ -517,7 +208,7 @@
     "        },\n",
     "        'index': {\n",
     "            'knn': True,\n",
-    "            'knn.space_type': 'cosinesimil'  # Example space type\n",
+    "            'knn.space_type': 'cosinesimil'  # Cosine similarity for vector search\n",
     "        }\n",
     "    },\n",
     "    'mappings': {\n",
@@ -525,14 +216,14 @@
     "            'metadata': {\n",
     "                'properties': {\n",
     "                    'source': {'type': 'keyword'},\n",
-    "                    'page_number': {'type':'long'},\n",
-    "                    'category': {'type':'text'},\n",
-    "                    'file_directory': {'type':'text'},\n",
+    "                    'page_number': {'type': 'long'},\n",
+    "                    'category': {'type': 'text'},\n",
+    "                    'file_directory': {'type': 'text'},\n",
     "                    'last_modified': {'type': 'text'},\n",
     "                    'type': {'type': 'keyword'},\n",
-    "                    'image_base64': {'type':'text'},\n",
-    "                    'origin_image': {'type':'text'},\n",
-    "                    'origin_table': {'type':'text'},\n",
+    "                    'image_base64': {'type': 'text'},\n",
+    "                    'origin_image': {'type': 'text'},\n",
+    "                    'origin_table': {'type': 'text'},\n",
     "                }\n",
     "            },\n",
     "            'text': {\n",
@@ -542,35 +233,34 @@
     "            },\n",
     "            'vector_field': {\n",
     "                'type': 'knn_vector',\n",
-    "                'dimension': f\"{dimension}\" # Replace with your vector dimension\n",
+    "                'dimension': dimension\n",
     "            }\n",
     "        }\n",
     "    }\n",
-    "}\n"
+    "}"
    ]
   },
   {
-   "cell_type": "code",
-   "execution_count": null,
+   "cell_type": "markdown",
    "metadata": {},
-   "outputs": [],
    "source": [
     "### 3-3. Get Opensearch Domain Information"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 15,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
+    "# Retrieve OpenSearch domain endpoint from parameter store\n",
     "opensearch_domain_endpoint = pm.get_params(\n",
     "    key=\"opensearch_domain_endpoint\",\n",
     "    enc=False\n",
     ")\n",
     "\n",
+    "# Get authentication credentials from AWS Secrets Manager\n",
     "secrets_manager = boto3.client('secretsmanager')\n",
-    "\n",
     "response = secrets_manager.get_secret_value(\n",
     "    SecretId='opensearch_user_password'\n",
     ")\n",
@@ -583,21 +273,20 @@
     "\n",
     "http_auth = (opensearch_user_id, opensearch_user_password)\n",
     "\n",
+    "# Parse OpenSearch domain endpoint\n",
     "result = pm.parse_opensearch_endpoint(opensearch_domain_endpoint)\n",
-    "prefix, domain_name = result"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "os.environ[\"OPENSEARCH_PREFIX\"] = prefix\n",
-    "os.environ[\"OPENSEARCH_DOMAIN_NAME\"] = domain_name\n",
+    "prefix, domain_name = result\n",
+    "\n",
+    "# Set OpenSearch environment variables\n",
+    "os.environ[\"OPENSEARCH_PREFIX\"] = \"\"\n",
+    "os.environ[\"OPENSEARCH_DOMAIN_NAME\"] = \"contextual-rag-domain\"\n",
     "os.environ[\"OPENSEARCH_DOCUMENT_NAME\"] = \"\"\n",
     "os.environ[\"OPENSEARCH_USER\"] = opensearch_user_id\n",
-    "os.environ[\"OPENSEARCH_PASSWORD\"] = opensearch_user_password"
+    "os.environ[\"OPENSEARCH_PASSWORD\"] = opensearch_user_password\n",
+    "\n",
+    "print(f\"OpenSearch Domain: {domain_name}\")\n",
+    "print(f\"OpenSearch Prefix: {prefix}\")\n",
+    "print(f\"OpenSearch User: {opensearch_user_id}\")"
    ]
   },
   {
@@ -611,7 +300,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 16,
+   "execution_count": null,
    "metadata": {
     "tags": []
    },
@@ -619,20 +308,28 @@
    "source": [
     "from utils.opensearch import opensearch_utils\n",
     "\n",
-    "aws_region = os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
-    "\n",
+    "# Create OpenSearch client\n",
+    "aws_region = os.environ.get(\"AWS_DEFAULT_REGION\")\n",
     "os_client = opensearch_utils.create_aws_opensearch_client(\n",
     "    aws_region,\n",
     "    opensearch_domain_endpoint,\n",
     "    http_auth\n",
-    ")"
+    ")\n",
+    "\n",
+    "# Check if index exists\n",
+    "index_exists = opensearch_utils.check_if_index_exists(\n",
+    "    os_client,\n",
+    "    index_name\n",
+    ")\n",
+    "print(f\"Index '{index_name}' exists: {index_exists}\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "### 3-5. Create Index "
+    "### 3-5. Check OpenSearch Plugins\n",
+    "Before creating the index, let's verify that the necessary plugins (especially Nori for Korean text) are installed."
    ]
   },
   {
@@ -641,28 +338,75 @@
    "metadata": {},
    "outputs": [],
    "source": [
+    "# Check installed plugins in OpenSearch\n",
+    "plugins = os_client.cat.plugins(format=\"json\")\n",
+    "print(\"Installed Plugins:\")\n",
+    "for plugin in plugins:\n",
+    "    print(plugin['component'])\n",
+    "\n",
+    "# Check if Nori plugin exists\n",
+    "nori_installed = any(\"analysis-nori\" in plugin['component'] for plugin in plugins)\n",
+    "if not nori_installed:\n",
+    "    print(\"\\n⚠️ WARNING: Nori plugin for Korean text analysis is not installed!\")\n",
+    "    print(\"To install Nori plugin, follow the instructions at:\")\n",
+    "    print(\"https://aws.amazon.com/blogs/tech/amazon-opensearch-service-korean-nori-plugin-for-analysis/\")\n",
+    "    print(\"You'll need to use a compatible OpenSearch version and add the plugin through the console.\")\n",
+    "    print(\"Without this plugin, Korean text analysis will not work properly.\\n\")\n",
+    "else:\n",
+    "    print(\"\\n✅ Nori plugin is installed. Korean text analysis is supported.\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### 3-6. Create Index"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Check again if index exists\n",
     "index_exists = opensearch_utils.check_if_index_exists(\n",
     "    os_client,\n",
     "    index_name\n",
     ")\n",
     "\n",
+    "# Delete index if it already exists\n",
     "if index_exists:\n",
+    "    print(f\"Deleting existing index: {index_name}\")\n",
     "    opensearch_utils.delete_index(\n",
     "        os_client,\n",
     "        index_name\n",
     "    )\n",
     "\n",
-    "opensearch_utils.create_index(os_client, index_name, index_body)\n",
-    "index_info = os_client.indices.get(index=index_name)\n",
-    "print(\"Index is created\")\n",
-    "pprint(index_info)"
+    "# Create new index\n",
+    "try:\n",
+    "    print(f\"Creating index: {index_name}\")\n",
+    "    opensearch_utils.create_index(os_client, index_name, index_body)\n",
+    "    index_info = os_client.indices.get(index=index_name)\n",
+    "    print(\"Index created successfully!\")\n",
+    "    print(\"\\nIndex Configuration:\")\n",
+    "    pprint(index_info)\n",
+    "except Exception as e:\n",
+    "    print(f\"Error creating index: {str(e)}\")\n",
+    "    \n",
+    "    # If error is related to Nori plugin, provide additional guidance\n",
+    "    if \"nori_tokenizer\" in str(e):\n",
+    "        print(\"\\nError appears to be related to the Nori tokenizer.\")\n",
+    "        print(\"Please ensure the Nori plugin is installed in your OpenSearch domain.\")\n",
+    "        print(\"Follow the instructions at: https://aws.amazon.com/blogs/tech/amazon-opensearch-service-korean-nori-plugin-for-analysis/\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "## 4. RAG Application Configuration"
+    "## 4. RAG Application Configuration\n",
+    "Additional configuration for the RAG application components."
    ]
   },
   {
@@ -672,19 +416,105 @@
    "outputs": [],
    "source": [
     "# Reranker Configuration\n",
-    "os.environ[\"RERANKER_AWS_REGION\"] = \"\"\n",
+    "os.environ[\"RERANKER_AWS_REGION\"] = \"us-west-2\"\n",
     "os.environ[\"RERANKER_AWS_PROFILE\"] = \"\"\n",
-    "os.environ[\"RERANKER_MODEL_ID\"] = \"\"\n",
+    "os.environ[\"RERANKER_MODEL_ID\"] = \"amazon.rerank-v1:0\"\n",
     "\n",
     "# Rank Fusion Configuration\n",
-    "os.environ[\"RERANK_TOP_K\"] = \"\"\n",
-    "os.environ[\"HYBRID_SCORE_FILTER\"] = \"\"\n",
-    "os.environ[\"FINAL_RERANKED_RESULTS\"] = \"\"\n",
-    "os.environ[\"KNN_WEIGHT\"] = \"\"\n",
+    "os.environ[\"RERANK_TOP_K\"] = \"20\"\n",
+    "os.environ[\"HYBRID_SCORE_FILTER\"] = \"40\"\n",
+    "os.environ[\"FINAL_RERANKED_RESULTS\"] = \"20\"\n",
+    "os.environ[\"KNN_WEIGHT\"] = \"0.6\"\n",
     "\n",
     "# Application Configuration\n",
     "os.environ[\"RATE_LIMIT_DELAY\"] = \"60\"  # API 요청 간 지연 시간(초) (기본값: 60)"
    ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## 5. Save Configuration to .env File\n",
+    "Create a .env file to easily share configurations between notebooks."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Create .env file with all configurations\n",
+    "env_vars = {\n",
+    "    # AWS Configuration\n",
+    "    \"AWS_REGION\": os.environ.get(\"AWS_DEFAULT_REGION\", \"us-west-2\"),\n",
+    "    \"AWS_PROFILE\": os.environ.get(\"AWS_PROFILE\", \"default\"),\n",
+    "    \n",
+    "    # Bedrock Configuration\n",
+    "    \"BEDROCK_MODEL_ID\": os.environ.get(\"BEDROCK_MODEL_ID\", \"\"),\n",
+    "    \"EMBED_MODEL_ID\": os.environ.get(\"EMBED_MODEL_ID\", \"\"),\n",
+    "    \"BEDROCK_RETRIES\": os.environ.get(\"BEDROCK_RETRIES\", \"10\"),\n",
+    "    \n",
+    "    # Model Configuration\n",
+    "    \"MAX_TOKENS\": os.environ.get(\"MAX_TOKENS\", \"4096\"),\n",
+    "    \"TEMPERATURE\": os.environ.get(\"TEMPERATURE\", \"0\"),\n",
+    "    \"TOP_P\": os.environ.get(\"TOP_P\", \"0.7\"),\n",
+    "    \n",
+    "    # OpenSearch Configuration\n",
+    "    \"OPENSEARCH_PREFIX\": os.environ.get(\"OPENSEARCH_PREFIX\", \"\"),\n",
+    "    \"OPENSEARCH_DOMAIN_NAME\": os.environ.get(\"OPENSEARCH_DOMAIN_NAME\", \"\"),\n",
+    "    \"OPENSEARCH_DOCUMENT_NAME\": os.environ.get(\"OPENSEARCH_DOCUMENT_NAME\", \"\"),\n",
+    "    \"OPENSEARCH_USER\": os.environ.get(\"OPENSEARCH_USER\", \"\"),\n",
+    "    \"OPENSEARCH_PASSWORD\": os.environ.get(\"OPENSEARCH_PASSWORD\", \"\"),\n",
+    "    \n",
+    "    # Reranker Configuration\n",
+    "    \"RERANKER_MODEL_ID\": os.environ.get(\"RERANKER_MODEL_ID\", \"amazon.rerank-v1:0\"),\n",
+    "    \"RERANKER_AWS_REGION\": os.environ.get(\"RERANKER_AWS_REGION\", \"us-west-2\"),\n",
+    "    \"RERANKER_AWS_PROFILE\": os.environ.get(\"RERANKER_AWS_PROFILE\", \"\"),\n",
+    "    \n",
+    "    # Rank Fusion Configuration\n",
+    "    \"RERANK_TOP_K\": os.environ.get(\"RERANK_TOP_K\", \"20\"),\n",
+    "    \"HYBRID_SCORE_FILTER\": os.environ.get(\"HYBRID_SCORE_FILTER\", \"40\"),\n",
+    "    \"FINAL_RERANKED_RESULTS\": os.environ.get(\"FINAL_RERANKED_RESULTS\", \"20\"),\n",
+    "    \"KNN_WEIGHT\": os.environ.get(\"KNN_WEIGHT\", \"0.6\"),\n",
+    "    \n",
+    "    # Application Configuration\n",
+    "    \"CHUNK_SIZE\": os.environ.get(\"CHUNK_SIZE\", \"1000\"),\n",
+    "    \"RATE_LIMIT_DELAY\": os.environ.get(\"RATE_LIMIT_DELAY\", \"60\")\n",
+    "}\n",
+    "\n",
+    "# Write .env file\n",
+    "with open('.env', 'w') as f:\n",
+    "    for key, value in env_vars.items():\n",
+    "        if value is not None and value != \"\":\n",
+    "            f.write(f\"{key}={value}\\n\")\n",
+    "\n",
+    "print(\".env file created successfully!\")\n",
+    "print(\"This file will be used by other notebooks to load the configuration.\")\n",
+    "\n",
+    "# Load configuration into config object\n",
+    "try:\n",
+    "    from config import Config\n",
+    "    config = Config.load()\n",
+    "    \n",
+    "    # Environment variables will be automatically loaded by the Config class\n",
+    "    # when instantiated via the load() method, so no need to manually update values\n",
+    "    \n",
+    "    print(\"\\nConfiguration loaded successfully!\")\n",
+    "    print(\"Current configuration:\")\n",
+    "    print(f\"AWS Config: {config.aws}\")\n",
+    "    print(f\"Bedrock Config: {config.bedrock}\")\n",
+    "    print(f\"Model Config: {config.model}\")\n",
+    "    print(f\"OpenSearch Config: {config.opensearch}\")\n",
+    "    print(f\"Reranker Config: {config.reranker}\")\n",
+    "    print(f\"Rank Fusion Config: {config.rank_fusion}\")\n",
+    "    print(f\"App Config: {config.app}\")\n",
+    "    \n",
+    "except ImportError:\n",
+    "    print(\"\\nWarning: Couldn't import Config. Configuration will be loaded from .env in subsequent notebooks.\")\n",
+    "\n",
+    "print(\"\\n✅ Configuration complete! You can now proceed to the next notebook.\")"
+   ]
   }
  ],
  "metadata": {
diff --git a/notebook/1_file_processor.ipynb b/notebook/1_file_processor.ipynb
index 8202942..b31b199 100644
--- a/notebook/1_file_processor.ipynb
+++ b/notebook/1_file_processor.ipynb
@@ -1,469 +1,648 @@
 {
- "cells": [
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "# File Processor\n",
-    "1. pdf 파일을 읽어서 chunking 작업을 수행\n",
-    "2. 저장된 chunk들에 대해 embedding 작업을 수행\n",
-    "3. embedding 된 내용을 vector store (opensearch)에 저장\n"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## 0. Prerequisites"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "plaintext"
-    }
-   },
-   "outputs": [],
-   "source": [
-    "%load_ext autoreload\n",
-    "%autoreload 2\n",
-    "%pip install ipywidgets"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {
-    "vscode": {
-     "languageId": "plaintext"
-    }
-   },
-   "source": [
-    "### 1. PDF 처리 후 Chunking"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "plaintext"
-    }
-   },
-   "outputs": [],
-   "source": [
-    "from pathlib import Path\n",
-    "\n",
-    "input_file = 'data/bedrock-ug.pdf'\n",
-    "chunk_size = 1000\n",
-    "start_page = 15\n",
-    "\n",
-    "# Additional Parameters for Contextual Retrieval\n",
-    "add_contextual = True\n",
-    "document_size = 20000\n",
-    "\n",
-    "document_name = Path(input_file).resolve().stem\n",
-    "document_name"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "#### Split Document into chunked format"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "plaintext"
-    }
-   },
-   "outputs": [],
-   "source": [
-    "from libs.document_parser import DocumentParser\n",
-    "\n",
-    "chunked_document = DocumentParser.split(full_text=DocumentParser.load_pdf(input_file, start_page=start_page), chunk_size=chunk_size, max_document_length=document_size if add_contextual else -1)\n",
-    "\n",
-    "# save result into json file\n",
-    "output_file = f\"output/{document_name}_{chunk_size}{\"_situated\" if add_contextual else \"\"}_chunks.json\"\n",
-    "\n",
-    "import json\n",
-    "with open(output_file, 'w', encoding='utf-8') as f:\n",
-    "    json.dump(chunked_document, f, ensure_ascii=False, indent=2)\n",
-    "    print(f\"Chunks saved to {output_file}\")\n",
-    "\n"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### 2. Progress Embedding"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### 2-0. Load Requirement"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "plaintext"
-    }
-   },
-   "outputs": [],
-   "source": [
-    "from libs.bedrock_service import BedrockService\n",
-    "from libs.opensearch_service import OpensearchService\n",
-    "\n",
-    "from config import Config\n",
-    "config = Config.load()\n",
-    "config.__dict__\n"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "plaintext"
-    }
-   },
-   "outputs": [],
-   "source": [
-    "bedrock_service = BedrockService(config.aws.region, config.aws.profile, config.bedrock.retries, config.bedrock.embed_model_id, config.bedrock.model_id, config.model.max_tokens, config.model.temperature, config.model.top_p)\n",
-    "opensearch_service = OpensearchService(config.aws.region, config.aws.profile, config.opensearch.prefix, config.opensearch.domain_name, config.opensearch.document_name, config.opensearch.user, config.opensearch.password)\n"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### 2-1. Situate Document"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "plaintext"
-    }
-   },
-   "outputs": [],
-   "source": [
-    "temperature = 0.0\n",
-    "top_p = 0.5"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "plaintext"
-    }
-   },
-   "outputs": [],
-   "source": [
-    "import json\n",
-    "import time\n",
-    "from tqdm.notebook import tqdm\n",
-    "\n",
-    "if add_contextual:\n",
-    "    chunked_file = f\"output/{document_name}_{chunk_size}{\"_situated\" if add_contextual else \"\"}_chunks.json\"\n",
-    "\n",
-    "    with open(chunked_file, 'r', encoding='utf-8') as f:\n",
-    "        documents = json.load(f)\n",
-    "\n",
-    "    total_token_usage = {\"inputTokens\": 0, \"outputTokens\": 0, \"totalTokens\": 0}\n",
-    "    documents_token_usage = {}\n",
-    "\n",
-    "    sys_prompt = \"\"\"\n",
-    "    You're an expert at providing a succinct context, targeted for specific text chunks.\n",
-    "\n",
-    "    <instruction>\n",
-    "    - Offer 1-5 short sentences that explain what specific information this chunk provides within the document.\n",
-    "    - Focus on the unique content of this chunk, avoiding general statements about the overall document.\n",
-    "    - Clarify how this chunk's content relates to other parts of the document and its role in the document.\n",
-    "    - If there's essential information in the document that backs up this chunk's key points, mention the details.\n",
-    "    </instruction>\n",
-    "    \"\"\"\n",
-    "    fail_count = 0\n",
-    "\n",
-    "    for doc_index, document in tqdm(enumerate(documents), leave = False, total=len(documents)):\n",
-    "        if fail_count > 10:\n",
-    "            break\n",
-    "        doc_content = document['content']\n",
-    "\n",
-    "        if 'token_usage' in document:\n",
-    "            doc_token_usage = document['token_usage']\n",
-    "        else:\n",
-    "            document['token_usage'] = {\"inputTokens\": 0, \"outputTokens\": 0, \"totalTokens\": 0}\n",
-    "        \n",
-    "        for chunk in tqdm(document['chunks']):\n",
-    "            if 'simulated' in chunk:\n",
-    "                continue\n",
-    "            document_context_prompt = f\"\"\"\n",
-    "            <document>\n",
-    "            {doc_content}\n",
-    "            </document>\n",
-    "            \"\"\"\n",
-    "\n",
-    "            chunk_content = chunk['content']\n",
-    "            chunk_context_prompt = f\"\"\"\n",
-    "            Here is the chunk we want to situate within the whole document:\n",
-    "\n",
-    "            <chunk>\n",
-    "            {chunk_content}\n",
-    "            </chunk>\n",
-    "\n",
-    "            Skip the preamble and only provide the consise context.\n",
-    "            \"\"\"\n",
-    "            usr_prompt = [{\n",
-    "                    \"role\": \"user\", \n",
-    "                    \"content\": [\n",
-    "                        {\"text\": document_context_prompt},\n",
-    "                        {\"text\": chunk_context_prompt}\n",
-    "                    ]\n",
-    "                }]\n",
-    "            \n",
-    "            try:\n",
-    "                response = bedrock_service.converse(\n",
-    "                    messages=usr_prompt, \n",
-    "                    system_prompt=sys_prompt,\n",
-    "                    temperature=temperature,\n",
-    "                    top_p=top_p,\n",
-    "                    max_tokens=4096\n",
-    "                )\n",
-    "                situated_context = response['output']['message']['content'][0]['text'].strip()\n",
-    "                chunk['content'] = f\"Context:\\n{situated_context}\\n\\nChunk:\\n{chunk['content']}\"\n",
-    "                chunk['simulated'] = True\n",
-    "\n",
-    "                if 'usage' in response:\n",
-    "                    usage = response['usage']\n",
-    "                    for key in ['inputTokens', 'outputTokens', 'totalTokens']:\n",
-    "                        document['token_usage'][key] += usage.get(key, 0)\n",
-    "                print(f\"completed generating context for chunk [{doc_index}_{chunk['chunk_id']}]\")\n",
-    "\n",
-    "            except Exception as e:\n",
-    "                print(f\"Error generating context for chunk [{doc_index}_{chunk['chunk_id']}]: {e}\")\n",
-    "                fail_count += 1\n",
-    "            time.sleep(5)\n",
-    "\n",
-    "    with open(chunked_file, \"w\", encoding='utf-8') as f:\n",
-    "        json.dump(documents, f, indent=4)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "plaintext"
-    }
-   },
-   "outputs": [],
-   "source": [
-    "documents[-1]"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### 2-2. Create Index"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "plaintext"
-    }
-   },
-   "outputs": [],
-   "source": [
-    "# Configure Index\n",
-    "index_prefix = \"aws_\"\n",
-    "index_name = (f\"{index_prefix}contextual_{document_name}\" if add_contextual and not document_name.startswith(\"contextual_\") else document_name) + f\"_{chunk_size}\"\n",
-    "\n",
-    "overwrite_index = True\n",
-    "\n",
-    "opensearch_index_configuration = {\n",
-    "    \"settings\": {\n",
-    "        \"index.knn\": True,\n",
-    "        \"index.knn.algo_param.ef_search\": 512\n",
-    "    },\n",
-    "    \"mappings\": {\n",
-    "        \"properties\": {\n",
-    "            \"metadata\": {\n",
-    "                \"properties\": {\n",
-    "                    \"source\": {\n",
-    "                        \"type\": \"keyword\"\n",
-    "                    },\n",
-    "                    \"doc_id\": {\n",
-    "                        \"type\": \"keyword\"\n",
-    "                    },\n",
-    "                    \"timestamp\": {\n",
-    "                        \"type\": \"date\"\n",
-    "                    }\n",
-    "                }\n",
-    "            },\n",
-    "            \"content\": {\n",
-    "                \"type\": \"text\",\n",
-    "                \"analyzer\": \"standard\"\n",
-    "            },\n",
-    "            \"content_embedding\": {\n",
-    "                \"type\": \"knn_vector\",\n",
-    "                \"dimension\": 1024,\n",
-    "                \"method\": {\n",
-    "                    \"engine\": \"faiss\",\n",
-    "                    \"name\": \"hnsw\",\n",
-    "                    \"parameters\": {\n",
-    "                        \"ef_construction\": 512,\n",
-    "                        \"m\": 16\n",
-    "                    },\n",
-    "                    \"space_type\": \"l2\"\n",
-    "                }\n",
-    "            }\n",
-    "        }\n",
-    "    }\n",
-    "}\n",
-    "\n",
-    "index_name"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "plaintext"
-    }
-   },
-   "outputs": [],
-   "source": [
-    "if overwrite_index:\n",
-    "    if opensearch_service.opensearch_client.indices.exists(index=index_name):\n",
-    "        opensearch_service.opensearch_client.indices.delete(index=index_name)\n",
-    "    \n",
-    "    opensearch_service.opensearch_client.indices.create(index=index_name, body=opensearch_index_configuration)\n",
-    "else:\n",
-    "    if not opensearch_service.opensearch_client.indices.exists(index=index_name):\n",
-    "        opensearch_service.opensearch_client.indices.create(index=index_name, body=opensearch_index_configuration)\n",
-    "\n",
-    "index_pattern = f\"{index_prefix}*\" if index_prefix else \"*\"\n",
-    "indices = opensearch_service.opensearch_client.cat.indices(index=index_pattern, format=\"json\")\n",
-    "\n",
-    "indices_name = [item['index'] for item in indices]\n",
-    "indices_name\n"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### 2-3. Embed Document"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "plaintext"
-    }
-   },
-   "outputs": [],
-   "source": [
-    "import json\n",
-    "from tqdm.notebook import tqdm\n",
-    "from libs.bedrock_service import BedrockService\n",
-    "from datetime import datetime\n",
-    "\n",
-    "bedrock_service = BedrockService(config.aws.region, config.aws.profile, config.bedrock.retries, config.bedrock.embed_model_id, config.bedrock.model_id, config.model.max_tokens, config.model.temperature, config.model.top_p)\n",
-    "\n",
-    "with open(chunked_file, 'r', encoding='utf-8') as f:\n",
-    "    documents = json.load(f)\n",
-    "\n",
-    "embedded_documents = []\n",
-    "\n",
-    "for document in tqdm(documents):\n",
-    "    doc_id = document['doc_id']\n",
-    "    embedded_chunks = []\n",
-    "\n",
-    "    for chunk in tqdm(document['chunks']):\n",
-    "        context = chunk['content']\n",
-    "        chunk_embedding = bedrock_service.embedding(text=context)\n",
-    "        if chunk_embedding:\n",
-    "            chunk_id = chunk['chunk_id']\n",
-    "            _id = f\"{doc_id}_{chunk_id}\"\n",
-    "            embedded_chunk = {\n",
-    "                \"metadata\": {\n",
-    "                    \"source\": document_name, \n",
-    "                    \"doc_id\": doc_id,\n",
-    "                    \"chunk_id\": chunk_id,\n",
-    "                    \"timestamp\": datetime.now().isoformat()\n",
-    "                },\n",
-    "                \"content\": chunk['content'],\n",
-    "                \"content_embedding\": chunk_embedding\n",
-    "            }\n",
-    "            embedded_chunks.append(embedded_chunk)\n",
-    "\n",
-    "            opensearch_service.opensearch_client.index(\n",
-    "                index=index_name,\n",
-    "                body=embedded_chunk\n",
-    "            )\n",
-    "            \n",
-    "        embedded_documents.append({\n",
-    "            \"_id\": _id,\n",
-    "            \"embedded_chunks\": embedded_chunks\n",
-    "        })\n",
-    "        \n",
-    "print(f\"Successfully embedded and stored documents in index '{index_name}'\")\n"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### 2-4. Test Query"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "vscode": {
-     "languageId": "plaintext"
-    }
-   },
-   "outputs": [],
-   "source": [
-    "question = \"What is Bedrock?\"\n",
-    "\n",
-    "question_embedding = bedrock_service.embedding(text=question)\n",
-    "knn = opensearch_service.search_by_knn(question_embedding, 'contextual_bedrock-ug_1000')\n",
-    "knn"
-   ]
-  }
- ],
- "metadata": {
-  "language_info": {
-   "name": "python"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 2
+    "cells": [
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "# File Processor for Contextual RAG with Amazon Bedrock\n",
+                "\n",
+                "This notebook processes PDF documents and prepares them for use with a Contextual RAG system:\n",
+                "1. Reads and chunks PDF documents\n",
+                "2. Enhances chunks with contextual information\n",
+                "3. Creates embeddings for each chunk\n",
+                "4. Stores the chunks and embeddings in OpenSearch"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## 0. Prerequisites"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Load extensions and install required packages\n",
+                "%load_ext autoreload\n",
+                "%autoreload 2\n",
+                "\n",
+                "# Install required packages\n",
+                "%pip install ipywidgets pdfplumber python-dotenv tqdm\n",
+                "# Uncomment and run if you have requirements.txt\n",
+                "# %pip install -r ../requirements.txt\n",
+                "\n",
+                "# Import basic dependencies\n",
+                "import os\n",
+                "import json\n",
+                "import sys\n",
+                "import time\n",
+                "from pathlib import Path\n",
+                "from datetime import datetime\n",
+                "from tqdm.notebook import tqdm\n",
+                "\n",
+                "# Create output directory\n",
+                "os.makedirs(\"output\", exist_ok=True)\n",
+                "\n",
+                "# Load environment variables from .env file\n",
+                "try:\n",
+                "    from dotenv import load_dotenv\n",
+                "    load_dotenv('.env')\n",
+                "    print(\"Environment variables loaded from .env file\")\n",
+                "except ImportError:\n",
+                "    print(\"python-dotenv not installed, skipping .env loading\")\n",
+                "    print(\"Run '%pip install python-dotenv' if needed\")"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## 1. PDF Processing and Chunking\n",
+                "\n",
+                "Define parameters for document processing:"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Input file and chunking parameters\n",
+                "input_file = 'data/bedrock-ug.pdf'\n",
+                "chunk_size = 1000\n",
+                "start_page = 15\n",
+                "\n",
+                "# Additional Parameters for Contextual Retrieval\n",
+                "add_contextual = True  # Set to True to enable contextual chunking\n",
+                "document_size = 20000  # Maximum document size for context\n",
+                "\n",
+                "# Extract document name from file path\n",
+                "document_name = Path(input_file).resolve().stem\n",
+                "print(f\"Processing document: {document_name}\")"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### Split Document into Chunks"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Import DocumentParser from local library\n",
+                "try:\n",
+                "    from libs.document_parser import DocumentParser\n",
+                "except ImportError:\n",
+                "    print(\"Error importing DocumentParser. Make sure the libs directory is available.\")\n",
+                "    print(\"You might need to add the parent directory to Python path:\")\n",
+                "    sys.path.append('..')\n",
+                "    from libs.document_parser import DocumentParser\n",
+                "\n",
+                "# Create output directory if it doesn't exist\n",
+                "os.makedirs(\"output\", exist_ok=True)\n",
+                "\n",
+                "try:\n",
+                "    # Load PDF and split into chunks\n",
+                "    print(f\"Loading PDF from {input_file} starting at page {start_page}...\")\n",
+                "    chunked_document = DocumentParser.split(\n",
+                "        full_text=DocumentParser.load_pdf(input_file, start_page=start_page), \n",
+                "        chunk_size=chunk_size, \n",
+                "        max_document_length=document_size if add_contextual else -1\n",
+                "    )\n",
+                "    \n",
+                "    # Define output file path with proper f-string syntax\n",
+                "    output_file = f\"output/{document_name}_{chunk_size}_{'situated' if add_contextual else ''}_chunks.json\"\n",
+                "    \n",
+                "    # Save chunks to JSON file\n",
+                "    with open(output_file, 'w', encoding='utf-8') as f:\n",
+                "        json.dump(chunked_document, f, ensure_ascii=False, indent=2)\n",
+                "        print(f\"✅ Chunks saved to {output_file}\")\n",
+                "        \n",
+                "    # Print summary statistics\n",
+                "    total_chunks = sum(len(doc.get('chunks', [])) for doc in chunked_document)\n",
+                "    print(f\"Total documents: {len(chunked_document)}\")\n",
+                "    print(f\"Total chunks: {total_chunks}\")\n",
+                "    \n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error processing document: {str(e)}\")"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## 2. Process Embeddings\n",
+                "\n",
+                "### 2-0. Load Requirements and Configuration"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "try:\n",
+                "    # Import required services\n",
+                "    from libs.bedrock_service import BedrockService\n",
+                "    from libs.opensearch_service import OpensearchService\n",
+                "    from config import Config\n",
+                "    \n",
+                "    # Load configuration\n",
+                "    config = Config.load()\n",
+                "    \n",
+                "    # Update config with environment variables if available\n",
+                "    config.aws.region = os.environ.get(\"AWS_DEFAULT_REGION\", config.aws.region)\n",
+                "    config.aws.profile = os.environ.get(\"AWS_PROFILE\", config.aws.profile)\n",
+                "    config.bedrock.model_id = os.environ.get(\"BEDROCK_MODEL_ID\", config.bedrock.model_id)\n",
+                "    config.bedrock.embed_model_id = os.environ.get(\"EMBED_MODEL_ID\", config.bedrock.embed_model_id)\n",
+                "    config.opensearch.prefix = os.environ.get(\"OPENSEARCH_PREFIX\", config.opensearch.prefix)\n",
+                "    config.opensearch.domain_name = os.environ.get(\"OPENSEARCH_DOMAIN_NAME\", config.opensearch.domain_name)\n",
+                "    config.opensearch.user = os.environ.get(\"OPENSEARCH_USER\", config.opensearch.user)\n",
+                "    config.opensearch.password = os.environ.get(\"OPENSEARCH_PASSWORD\", config.opensearch.password)\n",
+                "    \n",
+                "    print(\"Configuration loaded successfully\")\n",
+                "    print(f\"AWS Region: {config.aws.region}\")\n",
+                "    print(f\"Bedrock Model ID: {config.bedrock.model_id}\")\n",
+                "    print(f\"Embedding Model ID: {config.bedrock.embed_model_id}\")\n",
+                "    print(f\"OpenSearch Domain: {config.opensearch.domain_name}\")\n",
+                "    \n",
+                "except ImportError as e:\n",
+                "    print(f\"❌ Error importing required modules: {str(e)}\")\n",
+                "    print(\"Make sure all dependencies are installed and the paths are correct\")\n",
+                "    raise"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Initialize services\n",
+                "try:\n",
+                "    # Initialize Bedrock and OpenSearch services\n",
+                "    bedrock_service = BedrockService(\n",
+                "        config.aws.region, \n",
+                "        config.aws.profile, \n",
+                "        config.bedrock.retries, \n",
+                "        config.bedrock.embed_model_id, \n",
+                "        config.bedrock.model_id, \n",
+                "        config.model.max_tokens, \n",
+                "        config.model.temperature, \n",
+                "        config.model.top_p\n",
+                "    )\n",
+                "    \n",
+                "    opensearch_service = OpensearchService(\n",
+                "        config.aws.region, \n",
+                "        config.aws.profile, \n",
+                "        config.opensearch.prefix, \n",
+                "        config.opensearch.domain_name, \n",
+                "        config.opensearch.document_name, \n",
+                "        config.opensearch.user, \n",
+                "        config.opensearch.password\n",
+                "    )\n",
+                "    \n",
+                "    print(\"✅ Services initialized successfully\")\n",
+                "    \n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error initializing services: {str(e)}\")\n",
+                "    raise"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 2-1. Add Contextual Information to Chunks"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Set parameters for context generation\n",
+                "temperature = 0.0  # Lower temperature for more deterministic output\n",
+                "top_p = 0.5        # Nucleus sampling parameter"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Add contextual information to chunks if enabled\n",
+                "if add_contextual:\n",
+                "    # Define input and output file paths\n",
+                "    chunked_file = f\"output/{document_name}_{chunk_size}_{'situated' if add_contextual else ''}_chunks.json\"\n",
+                "    \n",
+                "    print(f\"Loading chunks from {chunked_file}...\")\n",
+                "    try:\n",
+                "        # Load chunked documents\n",
+                "        with open(chunked_file, 'r', encoding='utf-8') as f:\n",
+                "            documents = json.load(f)\n",
+                "        \n",
+                "        # Initialize token usage tracking\n",
+                "        total_token_usage = {\"inputTokens\": 0, \"outputTokens\": 0, \"totalTokens\": 0}\n",
+                "        documents_token_usage = {}\n",
+                "        \n",
+                "        # Define system prompt for context generation\n",
+                "        sys_prompt = \"\"\"\n",
+                "        You're an expert at providing a succinct context, targeted for specific text chunks.\n",
+                "\n",
+                "        <instruction>\n",
+                "        - Offer 1-5 short sentences that explain what specific information this chunk provides within the document.\n",
+                "        - Focus on the unique content of this chunk, avoiding general statements about the overall document.\n",
+                "        - Clarify how this chunk's content relates to other parts of the document and its role in the document.\n",
+                "        - If there's essential information in the document that backs up this chunk's key points, mention the details.\n",
+                "        </instruction>\n",
+                "        \"\"\"\n",
+                "        \n",
+                "        # Track failures to prevent infinite loops\n",
+                "        fail_count = 0\n",
+                "        \n",
+                "        print(\"Generating contextual information for each chunk...\")\n",
+                "        \n",
+                "        # Process each document\n",
+                "        for doc_index, document in tqdm(enumerate(documents), leave=False, total=len(documents)):\n",
+                "            # Break if too many failures\n",
+                "            if fail_count > 10:\n",
+                "                print(\"Too many failures, stopping context generation\")\n",
+                "                break\n",
+                "                \n",
+                "            # Get document content\n",
+                "            doc_content = document['content']\n",
+                "            \n",
+                "            # Initialize token usage tracking for this document\n",
+                "            if 'token_usage' in document:\n",
+                "                doc_token_usage = document['token_usage']\n",
+                "            else:\n",
+                "                document['token_usage'] = {\"inputTokens\": 0, \"outputTokens\": 0, \"totalTokens\": 0}\n",
+                "            \n",
+                "            # Process each chunk\n",
+                "            for chunk in tqdm(document['chunks'], leave=False):\n",
+                "                # Skip if already processed\n",
+                "                if 'simulated' in chunk:\n",
+                "                    continue\n",
+                "                    \n",
+                "                # Prepare document context prompt\n",
+                "                document_context_prompt = f\"\"\"\n",
+                "                <document>\n",
+                "                {doc_content}\n",
+                "                </document>\n",
+                "                \"\"\"\n",
+                "                \n",
+                "                # Prepare chunk context prompt\n",
+                "                chunk_content = chunk['content']\n",
+                "                chunk_context_prompt = f\"\"\"\n",
+                "                Here is the chunk we want to situate within the whole document:\n",
+                "\n",
+                "                <chunk>\n",
+                "                {chunk_content}\n",
+                "                </chunk>\n",
+                "\n",
+                "                Skip the preamble and only provide the concise context.\n",
+                "                \"\"\"\n",
+                "                \n",
+                "                # Create the user prompt\n",
+                "                usr_prompt = [{\n",
+                "                    \"role\": \"user\", \n",
+                "                    \"content\": [\n",
+                "                        {\"text\": document_context_prompt},\n",
+                "                        {\"text\": chunk_context_prompt}\n",
+                "                    ]\n",
+                "                }]\n",
+                "                \n",
+                "                try:\n",
+                "                    # Call Bedrock to generate context\n",
+                "                    response = bedrock_service.converse(\n",
+                "                        messages=usr_prompt, \n",
+                "                        system_prompt=sys_prompt,\n",
+                "                        temperature=temperature,\n",
+                "                        top_p=top_p,\n",
+                "                        max_tokens=4096\n",
+                "                    )\n",
+                "                    \n",
+                "                    # Extract and format the context\n",
+                "                    situated_context = response['output']['message']['content'][0]['text'].strip()\n",
+                "                    chunk['content'] = f\"Context:\\n{situated_context}\\n\\nChunk:\\n{chunk_content}\"\n",
+                "                    chunk['simulated'] = True\n",
+                "                    \n",
+                "                    # Track token usage\n",
+                "                    if 'usage' in response:\n",
+                "                        usage = response['usage']\n",
+                "                        for key in ['inputTokens', 'outputTokens', 'totalTokens']:\n",
+                "                            document['token_usage'][key] += usage.get(key, 0)\n",
+                "                            \n",
+                "                    print(f\"✅ Context generated for chunk [{doc_index}_{chunk['chunk_id']}]\")\n",
+                "                \n",
+                "                except Exception as e:\n",
+                "                    print(f\"❌ Error generating context for chunk [{doc_index}_{chunk['chunk_id']}]: {str(e)}\")\n",
+                "                    fail_count += 1\n",
+                "                    \n",
+                "                # Rate limiting to avoid API throttling\n",
+                "                time.sleep(5)\n",
+                "        \n",
+                "        # Save the updated documents with context\n",
+                "        print(f\"Saving documents with context to {chunked_file}...\")\n",
+                "        with open(chunked_file, \"w\", encoding='utf-8') as f:\n",
+                "            json.dump(documents, f, indent=4)\n",
+                "            \n",
+                "        print(\"✅ Context generation complete!\")\n",
+                "        \n",
+                "    except FileNotFoundError:\n",
+                "        print(f\"❌ File not found: {chunked_file}\")\n",
+                "        raise\n",
+                "    except Exception as e:\n",
+                "        print(f\"❌ Error during context generation: {str(e)}\")\n",
+                "        raise"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 2-2. Create OpenSearch Index"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Configure index name\n",
+                "index_prefix = \"aws_\"\n",
+                "index_name = (f\"{index_prefix}contextual_{document_name}\" if add_contextual and not document_name.startswith(\"contextual_\") else document_name) + f\"_{chunk_size}\"\n",
+                "\n",
+                "# Set to True to overwrite existing index, False to keep existing\n",
+                "overwrite_index = True\n",
+                "\n",
+                "# Define OpenSearch index configuration\n",
+                "opensearch_index_configuration = {\n",
+                "    \"settings\": {\n",
+                "        \"index.knn\": True,\n",
+                "        \"index.knn.algo_param.ef_search\": 512\n",
+                "    },\n",
+                "    \"mappings\": {\n",
+                "        \"properties\": {\n",
+                "            \"metadata\": {\n",
+                "                \"properties\": {\n",
+                "                    \"source\": {\n",
+                "                        \"type\": \"keyword\"\n",
+                "                    },\n",
+                "                    \"doc_id\": {\n",
+                "                        \"type\": \"keyword\"\n",
+                "                    },\n",
+                "                    \"timestamp\": {\n",
+                "                        \"type\": \"date\"\n",
+                "                    }\n",
+                "                }\n",
+                "            },\n",
+                "            \"content\": {\n",
+                "                \"type\": \"text\",\n",
+                "                \"analyzer\": \"standard\"\n",
+                "            },\n",
+                "            \"content_embedding\": {\n",
+                "                \"type\": \"knn_vector\",\n",
+                "                \"dimension\": 1024,  # Embedding dimension for Titan Embeddings\n",
+                "                \"method\": {\n",
+                "                    \"engine\": \"faiss\",\n",
+                "                    \"name\": \"hnsw\",\n",
+                "                    \"parameters\": {\n",
+                "                        \"ef_construction\": 512,\n",
+                "                        \"m\": 16\n",
+                "                    },\n",
+                "                    \"space_type\": \"l2\"\n",
+                "                }\n",
+                "            }\n",
+                "        }\n",
+                "    }\n",
+                "}\n",
+                "\n",
+                "print(f\"Index name: {index_name}\")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Create or update the OpenSearch index\n",
+                "try:\n",
+                "    # Check if index needs to be deleted and recreated\n",
+                "    if overwrite_index:\n",
+                "        if opensearch_service.opensearch_client.indices.exists(index=index_name):\n",
+                "            print(f\"Deleting existing index: {index_name}\")\n",
+                "            opensearch_service.opensearch_client.indices.delete(index=index_name)\n",
+                "        \n",
+                "        print(f\"Creating new index: {index_name}\")\n",
+                "        opensearch_service.opensearch_client.indices.create(index=index_name, body=opensearch_index_configuration)\n",
+                "    else:\n",
+                "        if not opensearch_service.opensearch_client.indices.exists(index=index_name):\n",
+                "            print(f\"Index doesn't exist. Creating: {index_name}\")\n",
+                "            opensearch_service.opensearch_client.indices.create(index=index_name, body=opensearch_index_configuration)\n",
+                "        else:\n",
+                "            print(f\"Index {index_name} already exists. Skipping creation.\")\n",
+                "\n",
+                "    # List all indices matching the prefix\n",
+                "    index_pattern = f\"{index_prefix}*\" if index_prefix else \"*\"\n",
+                "    indices = opensearch_service.opensearch_client.cat.indices(index=index_pattern, format=\"json\")\n",
+                "    \n",
+                "    # Extract and display index names\n",
+                "    indices_name = [item['index'] for item in indices]\n",
+                "    print(\"\\nAvailable indices:\")\n",
+                "    for idx in indices_name:\n",
+                "        print(f\" - {idx}\")\n",
+                "\n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error configuring OpenSearch index: {str(e)}\")\n",
+                "    raise"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 2-3. Embed Documents and Store in OpenSearch"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Load documents to embed\n",
+                "chunked_file = f\"output/{document_name}_{chunk_size}_{'situated' if add_contextual else ''}_chunks.json\"\n",
+                "\n",
+                "try:\n",
+                "    print(f\"Loading documents for embedding from {chunked_file}...\")\n",
+                "    with open(chunked_file, 'r', encoding='utf-8') as f:\n",
+                "        documents = json.load(f)\n",
+                "\n",
+                "    print(f\"Generating embeddings and storing in OpenSearch index: {index_name}\")\n",
+                "    \n",
+                "    # Track embedded documents\n",
+                "    embedded_documents = []\n",
+                "    total_chunks = sum(len(doc.get('chunks', [])) for doc in documents)\n",
+                "    \n",
+                "    # Process each document\n",
+                "    for document in tqdm(documents, desc=\"Documents\", total=len(documents)):\n",
+                "        doc_id = document['doc_id']\n",
+                "        \n",
+                "        # Process each chunk in the document\n",
+                "        for chunk in tqdm(document['chunks'], desc=\"Chunks\", leave=False):\n",
+                "            # Get chunk content\n",
+                "            context = chunk['content']\n",
+                "            \n",
+                "            # Generate embedding\n",
+                "            try:\n",
+                "                chunk_embedding = bedrock_service.embedding(text=context)\n",
+                "                \n",
+                "                if chunk_embedding:\n",
+                "                    # Create document ID for OpenSearch\n",
+                "                    chunk_id = chunk['chunk_id']\n",
+                "                    _id = f\"{doc_id}_{chunk_id}\"\n",
+                "                    \n",
+                "                    # Create document for OpenSearch\n",
+                "                    embedded_chunk = {\n",
+                "                        \"metadata\": {\n",
+                "                            \"source\": document_name, \n",
+                "                            \"doc_id\": doc_id,\n",
+                "                            \"chunk_id\": chunk_id,\n",
+                "                            \"timestamp\": datetime.now().isoformat()\n",
+                "                        },\n",
+                "                        \"content\": context,\n",
+                "                        \"content_embedding\": chunk_embedding\n",
+                "                    }\n",
+                "                    \n",
+                "                    # Store in tracking list\n",
+                "                    embedded_documents.append(_id)\n",
+                "                    \n",
+                "                    # Index in OpenSearch\n",
+                "                    opensearch_service.opensearch_client.index(\n",
+                "                        index=index_name,\n",
+                "                        id=_id,  # Explicitly set document ID\n",
+                "                        body=embedded_chunk\n",
+                "                    )\n",
+                "                else:\n",
+                "                    print(f\"⚠️ Warning: Empty embedding for chunk {doc_id}_{chunk['chunk_id']}\")\n",
+                "            \n",
+                "            except Exception as e:\n",
+                "                print(f\"❌ Error embedding chunk {doc_id}_{chunk['chunk_id']}: {str(e)}\")\n",
+                "                \n",
+                "            # Brief delay to prevent API throttling if needed\n",
+                "            time.sleep(0.1)\n",
+                "    \n",
+                "    # Force index refresh to make documents searchable immediately\n",
+                "    opensearch_service.opensearch_client.indices.refresh(index=index_name)\n",
+                "    \n",
+                "    print(f\"✅ Successfully embedded and stored {len(embedded_documents)} chunks in index '{index_name}'\")\n",
+                "    \n",
+                "except FileNotFoundError:\n",
+                "    print(f\"❌ File not found: {chunked_file}\")\n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error during embedding process: {str(e)}\")"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 2-4. Test Query"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Test a simple query to verify the setup\n",
+                "test_question = \"What is Amazon Bedrock?\"\n",
+                "\n",
+                "try:\n",
+                "    print(f\"Testing search with question: '{test_question}'\")\n",
+                "    \n",
+                "    # Generate embedding for the question\n",
+                "    question_embedding = bedrock_service.embedding(text=test_question)\n",
+                "    \n",
+                "    if not question_embedding:\n",
+                "        raise ValueError(\"Failed to generate embedding for the question\")\n",
+                "    \n",
+                "    # Search using KNN\n",
+                "    search_results = opensearch_service.search_by_knn(\n",
+                "        question_embedding,  # Query embedding vector\n",
+                "        index_name,          # Index to search\n",
+                "        top_n=3,             # Number of results to return\n",
+                "    )\n",
+                "    \n",
+                "    # Display results\n",
+                "    print(\"\\n=== Search Results ===\")\n",
+                "    if not search_results:\n",
+                "        print(\"No results found\")\n",
+                "    else:\n",
+                "        for i, result in enumerate(search_results, 1):\n",
+                "            print(f\"\\nResult {i} (Score: {result.get('score', 'N/A')}):\")\n",
+                "            # Based on _format_search_result likely returning content directly\n",
+                "            content = result.get('content', 'No content')\n",
+                "            \n",
+                "            # Truncate content if too long\n",
+                "            if len(content) > 300:\n",
+                "                content = content[:300] + \"...\"\n",
+                "                \n",
+                "            print(content)\n",
+                "    \n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error testing query: {str(e)}\")"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## Next Steps\n",
+                "\n",
+                "1. Proceed to the Question Generator notebook to generate test questions from your document\n",
+                "2. Or go directly to the RAG notebook to start querying your indexed documents\n",
+                "3. For any issues, check the configuration in the Configuration notebook"
+            ]
+        }
+    ],
+    "metadata": {
+        "kernelspec": {
+            "display_name": "conda_python3",
+            "language": "python",
+            "name": "conda_python3"
+        },
+        "language_info": {
+            "codemirror_mode": {
+                "name": "ipython",
+                "version": 3
+            },
+            "file_extension": ".py",
+            "mimetype": "text/x-python",
+            "name": "python",
+            "nbconvert_exporter": "python",
+            "pygments_lexer": "ipython3",
+            "version": "3.10.16"
+        }
+    },
+    "nbformat": 4,
+    "nbformat_minor": 2
 }
diff --git a/notebook/2_qustion_generator.ipynb b/notebook/2_qustion_generator.ipynb
index c99787c..8c15b73 100644
--- a/notebook/2_qustion_generator.ipynb
+++ b/notebook/2_qustion_generator.ipynb
@@ -1,415 +1,425 @@
 {
- "cells": [
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### 0. Prerequisites"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 1,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.12/site-packages (8.1.5)\n",
-      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
-      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (8.29.0)\n",
-      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
-      "Requirement already satisfied: widgetsnbextension~=4.0.12 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.13)\n",
-      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.13)\n",
-      "Requirement already satisfied: decorator in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
-      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
-      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
-      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
-      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
-      "Requirement already satisfied: stack-data in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
-      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
-      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
-      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
-      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
-      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
-      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
-      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
-      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
-      "\n",
-      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
-      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
-      "Note: you may need to restart the kernel to use updated packages.\n"
-     ]
-    }
-   ],
-   "source": [
-    "%load_ext autoreload\n",
-    "%autoreload 2\n",
-    "%pip install ipywidgets"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### 1. Setup File Information"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "'bedrock-ug'"
-      ]
-     },
-     "execution_count": 2,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "from pathlib import Path\n",
-    "\n",
-    "input_file = \"data/bedrock-ug.pdf\"\n",
-    "chunk_size = 1000\n",
-    "start_page = 0\n",
-    "end_page = -1\n",
-    "\n",
-    "document_name = Path(input_file).resolve().stem\n",
-    "document_name"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "us-west-2\n"
-     ]
-    }
-   ],
-   "source": [
-    "from libs.bedrock_service import BedrockService\n",
-    "from config import Config\n",
-    "config = Config.load()\n",
-    "\n",
-    "bedrock_service = BedrockService(config.aws.region, config.aws.profile, config.bedrock.retries, config.bedrock.embed_model_id, config.bedrock.model_id, config.model.max_tokens, config.model.temperature, config.model.top_p)"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### 2. Split Document into Chunks"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "100%|██████████| 1671/1671 [01:19<00:00, 20.99it/s]\n"
-     ]
+    "cells": [
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "# Question Generator for Contextual RAG\n",
+                "\n",
+                "This notebook generates realistic sample questions and answers based on your document content, which can be used to evaluate your RAG system."
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 0. Prerequisites"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "%load_ext autoreload\n",
+                "%autoreload 2\n",
+                "\n",
+                "# Install required packages\n",
+                "%pip install ipywidgets python-dotenv tqdm\n",
+                "\n",
+                "# Import basic dependencies\n",
+                "import os\n",
+                "import sys\n",
+                "import json\n",
+                "import uuid\n",
+                "import random\n",
+                "from pathlib import Path\n",
+                "from tqdm.notebook import tqdm\n",
+                "\n",
+                "# Create output directory\n",
+                "os.makedirs(\"output\", exist_ok=True)\n",
+                "\n",
+                "# Load environment variables from .env file\n",
+                "try:\n",
+                "    from dotenv import load_dotenv\n",
+                "    load_dotenv('.env')\n",
+                "    print(\"Environment variables loaded from .env file\")\n",
+                "except ImportError:\n",
+                "    print(\"python-dotenv not installed, skipping .env loading\")"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 1. Setup File Information"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Define input file and chunking parameters\n",
+                "input_file = \"data/bedrock-ug.pdf\"\n",
+                "chunk_size = 1000\n",
+                "start_page = 0\n",
+                "end_page = -1  # -1 means process all pages\n",
+                "\n",
+                "# Extract document name from file path\n",
+                "document_name = Path(input_file).resolve().stem\n",
+                "print(f\"Document name: {document_name}\")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "try:\n",
+                "    # Import required services and configuration\n",
+                "    from libs.bedrock_service import BedrockService\n",
+                "    from config import Config\n",
+                "    \n",
+                "    # Load configuration\n",
+                "    config = Config.load()\n",
+                "    \n",
+                "    # Update config with environment variables if available\n",
+                "    config.aws.region = os.environ.get(\"AWS_DEFAULT_REGION\", config.aws.region)\n",
+                "    config.aws.profile = os.environ.get(\"AWS_PROFILE\", config.aws.profile)\n",
+                "    config.bedrock.model_id = os.environ.get(\"BEDROCK_MODEL_ID\", config.bedrock.model_id)\n",
+                "    config.bedrock.embed_model_id = os.environ.get(\"EMBED_MODEL_ID\", config.bedrock.embed_model_id)\n",
+                "    \n",
+                "    # Initialize Bedrock service\n",
+                "    bedrock_service = BedrockService(\n",
+                "        config.aws.region, \n",
+                "        config.aws.profile, \n",
+                "        config.bedrock.retries, \n",
+                "        config.bedrock.embed_model_id, \n",
+                "        config.bedrock.model_id, \n",
+                "        config.model.max_tokens, \n",
+                "        config.model.temperature, \n",
+                "        config.model.top_p\n",
+                "    )\n",
+                "    \n",
+                "    print(\"✅ Bedrock service initialized successfully\")\n",
+                "    print(f\"Model ID: {config.bedrock.model_id}\")\n",
+                "    \n",
+                "except ImportError as e:\n",
+                "    print(f\"❌ Error importing required modules: {str(e)}\")\n",
+                "    print(\"Make sure all dependencies are installed and the paths are correct\")\n",
+                "    sys.path.append('..')\n",
+                "    print(\"Added parent directory to Python path. Try running the cell again.\")\n",
+                "    raise\n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error initializing Bedrock service: {str(e)}\")\n",
+                "    raise"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 2. Split Document into Chunks"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "try:\n",
+                "    # Import DocumentParser from local library\n",
+                "    from libs.document_parser import DocumentParser\n",
+                "    \n",
+                "    print(f\"Loading PDF from {input_file}...\")\n",
+                "    print(f\"Pages: {start_page} to {'end' if end_page == -1 else end_page}\")\n",
+                "    \n",
+                "    # Load and split document\n",
+                "    full_text = DocumentParser.load_pdf(input_file, start_page, end_page)\n",
+                "    chunked_document = DocumentParser.split(full_text, chunk_size, -1)\n",
+                "    chunks = chunked_document[0]['chunks']\n",
+                "    \n",
+                "    print(f\"✅ Document loaded and split into {len(chunks)} chunks\")\n",
+                "    \n",
+                "except ImportError:\n",
+                "    print(\"Error importing DocumentParser. Make sure the libs directory is available.\")\n",
+                "    print(\"You might need to add the parent directory to Python path:\")\n",
+                "    sys.path.append('..')\n",
+                "    # Try again with updated path\n",
+                "    from libs.document_parser import DocumentParser\n",
+                "    full_text = DocumentParser.load_pdf(input_file, start_page, end_page)\n",
+                "    chunked_document = DocumentParser.split(full_text, chunk_size, -1)\n",
+                "    chunks = chunked_document[0]['chunks']\n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error loading or chunking document: {str(e)}\")\n",
+                "    raise"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 3. Build Prompt and Tool Config\n",
+                "\n",
+                "Define the system prompts for different types of question generation and the tool configuration."
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# System prompts for different question types\n",
+                "sys_template = {\n",
+                "    \"complex\": \"\"\"\n",
+                "        You are an expert at generating practical questions based on given documentation.\n",
+                "        Your task is to generate complex, reasoning questions and answers.\n",
+                "\n",
+                "        Follow these rules:\n",
+                "        1. Generate questions that reflect real user information needs related to the document's subject matter (e.g., technical docs : feature availability, implementation details)\n",
+                "        2. Ensure questions are relevant, concise, preferably under 25 words, and fully answerable with the provided information\n",
+                "        3. Focus on extracting key information that users are likely to seek, while avoiding narrow or less important questions.\n",
+                "        4. When provided with code blocks, focus on understanding the overall functionality rather than the specific syntax or variables. Feel free to request examples of how to use key APIs or features.\n",
+                "        5. Do not use phrases like 'based on the provided context' or 'according to the context'.\n",
+                "    \"\"\",\n",
+                "    \"simple\": \"\"\"\n",
+                "        You are an expert at generating practical questions based on given documentation.\n",
+                "        Your task is to create simple, directly answerable questions from the given context.\n",
+                "\n",
+                "        Follow these rules:\n",
+                "        1. Generate questions that reflect real user information needs related to the document's subject matter (e.g., technical docs : feature availability, implementation details)\n",
+                "        2. Ensure questions are relevant, concise, preferably under 10 words, and fully answerable with the provided information\n",
+                "        3. Focus on extracting key information that users are likely to seek, while avoiding narrow or less important questions.\n",
+                "        4. When provided with code blocks, focus on understanding the overall functionality rather than the specific syntax or variables. Feel free to request examples of how to use key APIs or features.\n",
+                "        5. Do not use phrases like 'based on the provided context' or 'according to the context'.\n",
+                "    \"\"\"\n",
+                "}\n",
+                "\n",
+                "print(\"System prompts defined for simple and complex questions\")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Tool configuration for question-answer generation\n",
+                "tool_config = {\n",
+                "    \"tools\": [\n",
+                "        {\n",
+                "            \"toolSpec\": {\n",
+                "                \"name\": \"QuestionAnswerGenerator\",\n",
+                "                \"description\": \"Generates questions and answers based on the given context.\",\n",
+                "                \"inputSchema\": {\n",
+                "                    \"json\": {\n",
+                "                        \"type\": \"object\",\n",
+                "                        \"properties\": {\n",
+                "                            \"question\": {\n",
+                "                                \"type\": \"string\",\n",
+                "                                \"description\": \"The generated question\"\n",
+                "                            },\n",
+                "                            \"answer\": {\n",
+                "                                \"type\": \"string\",\n",
+                "                                \"description\": \"The answer to the generated question\"\n",
+                "                            }\n",
+                "                        },\n",
+                "                        \"required\": [\"question\", \"answer\"]\n",
+                "                    }\n",
+                "                }\n",
+                "            }\n",
+                "        }\n",
+                "    ]\n",
+                "}\n",
+                "\n",
+                "print(\"Tool configuration defined for QuestionAnswerGenerator\")"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 4. Generate Questions"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Set the number of question-answer pairs to generate\n",
+                "num_pairs = 5  # Will generate 5 simple and 5 complex questions (total 10)\n",
+                "\n",
+                "# Define output file path\n",
+                "output_file = f\"output/{document_name}_sample_questions.jsonl\"\n",
+                "print(f\"Questions will be saved to: {output_file}\")\n",
+                "\n",
+                "# Delete existing output file if it exists\n",
+                "if os.path.exists(output_file):\n",
+                "    os.remove(output_file)\n",
+                "    print(f\"Removed existing file: {output_file}\")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Initialize dataset to store generated questions\n",
+                "total_chunks = len(chunks)\n",
+                "dataset = []\n",
+                "\n",
+                "# Track questions by type\n",
+                "generated_question = {\"simple\": [], \"complex\": []}\n",
+                "\n",
+                "# Generation parameters\n",
+                "temperature = 0.0\n",
+                "top_p = 0.5\n",
+                "\n",
+                "# Validate that we have enough chunks\n",
+                "if total_chunks < 3:\n",
+                "    raise ValueError(f\"Not enough chunks to generate questions. Found {total_chunks}, need at least 3.\")\n",
+                "\n",
+                "print(f\"Generating {num_pairs*2} questions ({num_pairs} simple + {num_pairs} complex)...\")\n",
+                "\n",
+                "# Generate questions\n",
+                "for i in tqdm(range(num_pairs * 2)):\n",
+                "    try:\n",
+                "        # Select random starting chunk position\n",
+                "        start_id = random.randint(0, total_chunks - 3)\n",
+                "        \n",
+                "        # Get three consecutive chunks for context\n",
+                "        context_chunks = [\n",
+                "            chunks[start_id]['content'],\n",
+                "            chunks[start_id + 1]['content'],\n",
+                "            chunks[start_id + 2]['content']\n",
+                "        ]\n",
+                "        \n",
+                "        # Combine chunks into context\n",
+                "        context = \" \".join(context_chunks)\n",
+                "        \n",
+                "        # Alternate between complex and simple questions\n",
+                "        if i % 2 == 0:\n",
+                "            question_type = \"complex\"\n",
+                "        else:\n",
+                "            question_type = \"simple\"\n",
+                "\n",
+                "        # Create user prompt\n",
+                "        user_template = f\"\"\"\n",
+                "        Generate a {question_type} question and its answer based on the following context:\n",
+                "\n",
+                "        Context: {context}\n",
+                "\n",
+                "        Use the QuestionAnswerGenerator tool to provide the output.\n",
+                "        \"\"\"\n",
+                "\n",
+                "        # Prepare prompt and inference config\n",
+                "        sys_prompt = sys_template[question_type]\n",
+                "        user_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template}]}]\n",
+                "\n",
+                "        # Call Bedrock with tool configuration\n",
+                "        response = bedrock_service.converse_with_tools(\n",
+                "            messages=user_prompt,\n",
+                "            system_prompt=sys_prompt,\n",
+                "            tools=tool_config,\n",
+                "            temperature=temperature,\n",
+                "            top_p=top_p,\n",
+                "            max_tokens=4096\n",
+                "        )\n",
+                "\n",
+                "        stop_reason = response['stopReason']\n",
+                "\n",
+                "        # Process the tool response\n",
+                "        if stop_reason == 'tool_use':\n",
+                "            tool_requests = response['output']['message']['content']\n",
+                "\n",
+                "            for tool_request in [x for x in tool_requests if 'toolUse' in x]:\n",
+                "                if tool_request['toolUse']['name'] == 'QuestionAnswerGenerator':\n",
+                "                    # Extract question and answer\n",
+                "                    question = tool_request['toolUse']['input']['question']\n",
+                "                    answer = tool_request['toolUse']['input']['answer']\n",
+                "                    \n",
+                "                    # Create QA item\n",
+                "                    qa_item = {\n",
+                "                        \"question\": question,\n",
+                "                        \"ground_truth\": answer,\n",
+                "                        \"question_type\": question_type,\n",
+                "                        \"context\": context\n",
+                "                    }\n",
+                "\n",
+                "                    # Save to JSONL file\n",
+                "                    with open(output_file, 'a') as f:\n",
+                "                        json.dump(qa_item, f)\n",
+                "                        f.write('\\n')\n",
+                "                    \n",
+                "                    # Add to dataset\n",
+                "                    dataset.append(qa_item)\n",
+                "                    generated_question[question_type].append(question)\n",
+                "                    \n",
+                "                    print(f\"Question {i+1}/{num_pairs*2} ({question_type}): {question[:50]}...\")\n",
+                "        else:\n",
+                "            print(f\"⚠️ Warning: Question generation stopped with reason '{stop_reason}' instead of 'tool_use'\")\n",
+                "            \n",
+                "    except Exception as e:\n",
+                "        print(f\"❌ Error generating question {i+1}: {str(e)}\")\n",
+                "\n",
+                "print(f\"\\n✅ Generated {len(dataset)} questions ({len(generated_question['simple'])} simple + {len(generated_question['complex'])} complex)\")\n",
+                "print(f\"Questions saved to {output_file}\")"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### 5. Display Sample Questions"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Display a sample of generated questions\n",
+                "print(\"\\n=== Sample Simple Questions ===\")\n",
+                "for i, question in enumerate(generated_question[\"simple\"][:3], 1):\n",
+                "    print(f\"{i}. {question}\")\n",
+                "\n",
+                "print(\"\\n=== Sample Complex Questions ===\")\n",
+                "for i, question in enumerate(generated_question[\"complex\"][:3], 1):\n",
+                "    print(f\"{i}. {question}\")\n",
+                "\n",
+                "print(f\"\\n✅ Complete dataset saved to {output_file}\")"
+            ]
+        }
+    ],
+    "metadata": {
+        "kernelspec": {
+            "display_name": "conda_python3",
+            "language": "python",
+            "name": "conda_python3"
+        },
+        "language_info": {
+            "codemirror_mode": {
+                "name": "ipython",
+                "version": 3
+            },
+            "file_extension": ".py",
+            "mimetype": "text/x-python",
+            "name": "python",
+            "nbconvert_exporter": "python",
+            "pygments_lexer": "ipython3",
+            "version": "3.10.16"
+        }
     },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "PDF Load: data/bedrock-ug.pdf\n"
-     ]
-    },
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "100%|██████████| 1/1 [00:00<00:00, 93.94it/s]\n"
-     ]
-    }
-   ],
-   "source": [
-    "from libs.document_parser import DocumentParser\n",
-    "chunked_document = DocumentParser.split(DocumentParser.load_pdf(input_file, start_page, end_page), chunk_size, -1)\n",
-    "chunks = chunked_document[0]['chunks']\n"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### 3. Build Prompt and Tool Config"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 7,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "sys_template = {\n",
-    "    \"complex\": \"\"\"\n",
-    "        You are an expert at generating practical questions based on given documentation.\n",
-    "        Your task is to generate complex, reasoning questions and answers.\n",
-    "\n",
-    "        Follow these rules:\n",
-    "        1. Generate questions that reflect real user information needs related to the document's subject matter (e.g., technical docs : feature availability, implementation details)\n",
-    "        2. Ensure questions are relevant, concise, preferably under 25 words, and fully answerable with the provided information\n",
-    "        3. Focus on extracting key information that users are likely to seek, while avoiding narrow or less important questions.\n",
-    "        4. When provided with code blocks, focus on understanding the overall functionality rather than the specific syntax or variables. Feel free to request examples of how to use key APIs or features.\n",
-    "        5. Do not use phrases like 'based on the provided context' or 'according to the context'.\n",
-    "    \"\"\",\n",
-    "    \"simple\": \"\"\"\n",
-    "        You are an expert at generating practical questions based on given documentation.\n",
-    "        Your task is to create simple, directly answerable questions from the given context.\n",
-    "\n",
-    "        Follow these rules:\n",
-    "        1. Generate questions that reflect real user information needs related to the document's subject matter (e.g., technical docs : feature availability, implementation details)\n",
-    "        2. Ensure questions are relevant, concise, preferably under 10 words, and fully answerable with the provided information\n",
-    "        3. Focus on extracting key information that users are likely to seek, while avoiding narrow or less important questions.\n",
-    "        4. When provided with code blocks, focus on understanding the overall functionality rather than the specific syntax or variables. Feel free to request examples of how to use key APIs or features.\n",
-    "        5. Do not use phrases like 'based on the provided context' or 'according to the context'.\n",
-    "    \"\"\"\n",
-    "}"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 6,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "tool_config = {\n",
-    "    \"tools\": [\n",
-    "        {\n",
-    "            \"toolSpec\": {\n",
-    "                \"name\": \"QuestionAnswerGenerator\",\n",
-    "                \"description\": \"Generates questions and answers based on the given context.\",\n",
-    "                \"inputSchema\": {\n",
-    "                    \"json\": {\n",
-    "                        \"type\": \"object\",\n",
-    "                        \"properties\": {\n",
-    "                            \"question\": {\n",
-    "                                \"type\": \"string\",\n",
-    "                                \"description\": \"The generated question\"\n",
-    "                            },\n",
-    "                            \"answer\": {\n",
-    "                                \"type\": \"string\",\n",
-    "                                \"description\": \"The answer to the generated question\"\n",
-    "                            }\n",
-    "                        },\n",
-    "                        \"required\": [\"question\", \"answer\"]\n",
-    "                    }\n",
-    "                }\n",
-    "            }\n",
-    "        }\n",
-    "    ]\n",
-    "}"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### 4. Generate Question"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 8,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "num_pairs = 5\n",
-    "\n",
-    "output_file = f\"output/{document_name}_sample_questions.jsonl\"\n",
-    "output_file"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "3f91ef2033a340a487519a41637f362d",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "  0%|          | 0/10 [00:00<?, ?it/s]"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/plain": [
-       "[{'question_id': '154444d4-a694-4dea-bb47-fac2b39ba251',\n",
-       "  'question': \"How do the S3 storage and S3 retrieval nodes in Amazon Bedrock's prompt flow differ in their functionality, inputs, and outputs, and what potential use case might combine these two node types?\",\n",
-       "  'ground_truth': \"The S3 storage and S3 retrieval nodes in Amazon Bedrock's prompt flow have distinct but complementary functions:\\n\\n1. S3 storage node:\\n   - Function: Stores data in an Amazon S3 location\\n   - Inputs: Content to store and the object key\\n   - Output: URI of the S3 location\\n   - Configuration: Specifies the S3 bucket for data storage\\n\\n2. S3 retrieval node:\\n   - Function: Retrieves data from an Amazon S3 location\\n   - Input: Object key\\n   - Output: Content from the S3 location (currently limited to UTF-8 encoded strings)\\n   - Configuration: Specifies the S3 bucket for data retrieval\\n\\nA potential use case combining these nodes could be a multi-step data processing workflow:\\n1. Use the S3 storage node to save intermediate results or large datasets generated during the flow.\\n2. Later in the flow or in a separate flow, use the S3 retrieval node to fetch the stored data for further processing or analysis.\\n\\nThis combination allows for efficient handling of large datasets, enables data persistence between flow executions, and facilitates modular design of complex data processing pipelines in Amazon Bedrock's prompt flow system.\",\n",
-       "  'question_type': 'complex',\n",
-       "  'context': 'el ID to use if you want to generate a response based on the retrieved results. To return the retrieved results as an array, omit the model ID. The input into the node is the query to the knowledge base. The output is either the model response, as a string, or an array of the retrieved results. Node types in prompt flow 912 Amazon Bedrock User Guide The following shows the general structure of a knowledge base FlowNode object: { \"name\": \"string\", \"type\": \"KnowledgeBase\", \"inputs\": [ { \"name\": \"retrievalQuery\", \"type\": \"String\", \"expression\": \"string\" } ], \"outputs\": [ { \"name\": \"retrievalResults\", \"type\": \"Array | String\" } ], \"configuration\": { \"knowledgeBase\": { \"knowledgeBaseId\": \"string\", \"modelId\": \"string\" } } } S3 storage node An S3 storage node lets you store data in the flow to an Amazon S3 location. In the configuration, you specify the S3 bucket to use for data storage. The inputs into the node are the content to store and the object key. The node returns the URI of the S3 location as its output. and the object key. The node returns the URI of the S3 location as its output. The following shows the general structure of an S3 storage FlowNode object: { \"name\": \"string\", \"type\": \"Storage\", \"inputs\": [ { \"name\": \"content\", \"type\": \"String | Number | Boolean | Object | Array\", \"expression\": \"string\" }, Node types in prompt flow 913 Amazon Bedrock User Guide { \"name\": \"objectKey\", \"type\": \"String\", \"expression\": \"string\" } ], \"outputs\": [ { \"name\": \"s3Uri\", \"type\": \"String\" } ], \"configuration\": { \"retrieval\": { \"serviceConfiguration\": { \"s3\": { \"bucketName\": \"string\" } } } } } S3 retrieval node An S3 retrieval node lets you retrieve data from an Amazon S3 location to introduce to the flow. In the configuration, you specify the S3 bucket from which to retrieve data. The input into the node is the object key. The node returns the content in the S3 location as the output. Note Currently, the data in the S3 location must be a UTF-8 encoded string. The following shows the general structure of an S3 retrieval FlowNode object: { \"name\": \"string\", \"type\": \"Retrieval\", \"inputs\": [ { \"name\": \"objectKey\", Node types in prompt flow 914 Amazon Bedrock User Guide \"type\": \"String\", \"expression\": \"string\" } ], \"outputs\": [ { \"name\": \"s3Content\", \"type\": \"String\" } ], \"configuration\": { \"retrieval\": { \"serviceConfiguration\": { \"s3\": { \"bucketName\": \"string\" } } } } } Lambda function node A Lambda function node lets you call a Lambda function in which you can define code to carry out business logic. all a Lambda function in which you can define code to carry out business logic. When you include a Lambda node in a prompt flow, Amazon Bedrock sends an input event to the Lambda function that you specify. In the configuration, specify the Amazon Resource Name (ARN) of the Lambda function. Define inputs to send in the Lambda input event. You can write code based on these inputs and define what the function returns. The function response is returned in the output. The following shows the general structure of a Λ function FlowNode object: { \"name\": \"string\", \"type\": \"LambdaFunction\", \"inputs\": [ { \"name\": \"string\", \"type\": \"String | Number | Boolean | Object | Array\", \"expression\": \"string\" }, ... Node types in prompt flow 915 Amazon Bedrock User Guide ], \"outputs\": [ { \"name\": \"functionResponse\", \"type\": \"String | Number | Boolean | Object | Array\" } ], \"configuration\": { \"lambdaFunction\": { \"lambdaArn\": \"string\" } } } Lambda input event for a prompt flow The input event sent to a Lambda function in a Lambda node is of the following format: { \"messageVersion\": \"1.0\", \"flow\": { \"flowArn\": \"string\", \"flowAliasArn\": \"string\" }, \"node\": { \"name\": \"string\", \"nodeInputs\": [ { \"name\": \"string\", \"type\": \"String | Number | Boolean | Object | Array\", \"expression\": \"string\", \"value\": ...'},\n",
-       " {'question_id': '10ce6181-3123-4af7-9ed0-b493b027cc13',\n",
-       "  'question': 'What does the woods symbolize in Robert Frost\\'s poem \"Stopping by Woods on a Snowy Evening\"?',\n",
-       "  'ground_truth': 'The woods symbolize death and the afterlife, representing both comfort and the unknown.',\n",
-       "  'question_type': 'simple',\n",
-       "  'context': 'ategory name 3}} which can be used to build 56 different predefined structures. Assistant:\"\"\" Enhance creativity, different structures can be connected (Source: Written by AWS) to form new structures, encouraging out-of-the box thinking. Enhance child-parent bonding, parents can play with their children together to foster social skills. </text> Categories are: (1) Toys (2) Beauty and Health (3) Electronics Assistant: Output: <output>Toys</output> (Source of prompt: AWS, model used: Anthropic Claude) Text classification 354 Amazon Bedrock User Guide Question-answer, without context In a question-answer prompt without context, the model must answer the question with its internal knowledge without using any context or document. Prompt Template to Titan User prompt: \"\"\"{{Question}}\"\"\" What is Robert Frost\\'s \"Stopping by the woods on a snowy evening\" about metaphorically? Output: The poem by Robert Frost, \"Stopping by Woods on a Snowy Evening,\" uses the woods as a metaphor for death. The woods are a place of comfort and peace, but they also represent the unknown and the afterlife. ce of comfort and peace, but they also represent the unknown and the afterlife. The poem suggests that death is not something to be feared, but rather a natural part of life that we should all embrace. The poem begins with the speaker stopping by the woods on a snowy evening, a moment of contemplation and reflection. The speaker notices the beauty of the snow-covered woods and is reminded of the fleeting nature of life. The poem continues as the speaker considers the temptation of continuin g on to his destination, but instead chooses to pause and take in the beauty of the woods. The woods are used as a metaphor for the unknown and the afterlife. Question-answer, without context 355 Amazon Bedrock User Guide The speaker wonders what lies beyond the trees, and whether there is anything after death. The poem suggests that death is not something to be feared, but rather a natural part of life that we should all embrace. The final lines of the poem are perhaps the most famous and poignant: \"And miles to go before I sleep, and miles to go before I sleep.\" These lines suggest that even though the speaker has paused to appreciate the beauty of the woods, they still have a long journey ahead of them. preciate the beauty of the woods, they still have a long journey ahead of them. The poem is a reminder that life is fleeting, and that we should take the time to appreciate the beauty around us, even as we work towards our goals and ambitions. Overall, \"Stopping by Woods on a Snowy Evening\" is a beautiful and thought-provoking poem that uses the metaphor of the woods to explore the concept of death and the afterlife. (Source of prompt: AWS, model used: Amazon Titan Text) Model encouragement can also help in question-answer tasks. Prompt Template to Titan User prompt: \"\"\"{{Model encouragement}} You are excellent at answering {{Question}}\"\"\" questions, and it makes you happy when you provide the correct answer. Question-answer, without context 356 Amazon Bedrock User Guide What moon in the solar system is most likely to host life? Output: The moon of Titan, which is a moon of the planet Saturn, is the most likely to support life in the solar system. Titan is the only known moon in the solar system to have a dense atmosphere and a liquid hydrosphere, and scientists believe that it may contain the chemical building blocks for life.'},\n",
-       " {'question_id': '281ff923-00c1-43d0-bf17-35667fe6bf91',\n",
-       "  'question': 'How does Amazon Bedrock handle vector embeddings and storage configurations when creating a knowledge base, and what options are available for different database types?',\n",
-       "  'ground_truth': \"When creating a knowledge base in Amazon Bedrock, vector embeddings and storage configurations are handled through several options:\\n\\n1. Vector embeddings: Users must specify an embedding model in the 'embeddingModelArn' field of the 'knowledgeBaseConfiguration' object. This model converts data into vector embeddings.\\n\\n2. Storage configurations: Amazon Bedrock offers multiple options for vector storage:\\n   a. Amazon OpenSearch Service (managed by Amazon Bedrock)\\n   b. User's own vector store, configured in the 'storageConfiguration' object\\n\\n3. Database options for user-managed vector stores:\\n   - Amazon OpenSearch Service: Use 'opensearchServerlessConfiguration'\\n   - Pinecone: Use 'pineconeConfiguration'\\n   - Redis Enterprise Cloud: Use 'redisEnterpriseCloudConfiguration'\\n   - Amazon Aurora: Use 'rdsConfiguration'\\n   - MongoDB Atlas: Use 'mongodbConfiguration'\\n\\n4. Data sources: After creating the knowledge base, users must create a data source using the 'CreateDataSource' request, specifying connection information, chunking configuration, and data deletion policy.\\n\\nThis flexible approach allows users to choose the most suitable vector embedding model and storage solution for their specific use case while integrating with various database types.\",\n",
-       "  'question_type': 'complex',\n",
-       "  'context': \"ils of your knowledge base. Choose Edit in any section that you need to modify. When you are satisfied, select Create knowledge base. Create a knowledge base 583 Amazon Bedrock User Guide 10. The time it takes to create the knowledge base depends on your specific configurations. When the knowledge base creation has completed, the status of the knowledge base changes to either state it is ready or available. API To create a knowledge base, send a CreateKnowledgeBase request with a Agents for Amazon Bedrock build-time endpoint and provide the name, description, instructions for what it should do, and the foundation model for it to orchestrate with. Note If you prefer to let Amazon Bedrock create and manage a vector store for you in Amazon OpenSearch Service, use the console. For more information, see Create an Amazon Bedrock knowledge base. • Provide the ARN with permissions to create a knowledge base in the roleArn field. • Provide the vector embeddings model to use in the embeddingModelArn field in the knowledgeBaseConfiguration object. to use in the embeddingModelArn field in the knowledgeBaseConfiguration object. See supported models for knowledge bases. You must enable model access to use a model that's supported for knowledge bases. Take note of your model Amazon Resource Name (ARN) that's required for converting your data into vector embeddings. Copy the model ID for your chosen model for knowledge bases and construct the model ARN using the model (resource) ID, following the provided ARN examples for your model resource type. • Provide the configuration for your vector store in the storageConfiguration object. For more information, see Prerequisites for your own vector store for a knowledge base • For an Amazon OpenSearch Service database, use the opensearchServerlessConfiguration object. • For a Pinecone database, use the pineconeConfiguration object. • For a Redis Enterprise Cloud database, use the redisEnterpriseCloudConfiguration object. • For an Amazon Aurora database, use the rdsConfiguration object. • For an MongoDB Atlas database, use the mongodbConfiguration object. n object. • For an MongoDB Atlas database, use the mongodbConfiguration object. Create a knowledge base 584 Amazon Bedrock User Guide After you create a knowledge base, create a data source containing the documents or content for your knowledge base. To create the data source send a CreateDataSource request. See Supported data sources to select your data source and follow the API connection configuration example. • Provide the connection information for the data source files in the dataSourceConfiguration field. • Specify how to chunk the data sources in the vectorIngestionConfiguration field. Note You can't change the chunking configuration after you create the data source. • Provide the dataDeletionPolicy for your data source. You can DELETE all data from your data source that’s converted into vector embeddings upon deletion of a knowledge base or data source resource. This flag is ignored if an AWS account is deleted. You can RETAIN all data from your data source that’s converted into vector embeddings upon deletion of a knowledge base or data source resource.\"},\n",
-       " {'question_id': 'a7544e00-a5e2-441d-97e3-0c55b9b71ae5',\n",
-       "  'question': 'How can you increase model inference rates when creating an agent alias?',\n",
-       "  'ground_truth': 'You can increase model inference rates by selecting Provisioned Throughput (PT) and choosing a previously purchased provisioned model when creating the agent alias.',\n",
-       "  'question_type': 'simple',\n",
-       "  'context': \"hoose Create. 4. Enter a unique Alias name and provide an optional Description. 5. Under Associate a version, choose one of the following options: • To create a new version, choose Create a new version and to associate it to this alias. • To use an existing version, choose Use an existing version to associate this alias. From the dropdown menu, choose the version that you want to associate the alias to. 6. Under Select throughput, select one of the following options: • To let your agent run model inference at the rates set for your account, select On- demand (ODT). For more information, see Quotas for Amazon Bedrock. • To let your agent run model inference at an increased rate using a Provisioned Throughput that you previously purchased for the model, select Provisioned Throughput (PT) and then select a provisioned model. For more information, see Increase model invocation capacity with Provisioned Throughput in Amazon Bedrock. 7. Select Create alias. Deploy and integrate agent into your application 888 Amazon Bedrock User Guide API To create an alias for an agent, send a CreateAgentAlias request (see link for request and response formats and field details) with an Agents for Amazon Bedrock build-time endpoint. rmats and field details) with an Agents for Amazon Bedrock build-time endpoint. The following fields are required: Field Use case agentId To specify the ID of the agent for which to create an alias. agentName To specify a name for the alias. The following fields are optional: Field Use case description To provide a description of the alias. routingConfiguration To specify a version to associate the alias with (leave blank to create a new version) and a Provisioned Throughput to associate with the alias. clientToken Identifier to ensure the API request completes only once. tags To associate tags with the alias. See code examples 2. Deploy your agent by setting up your application to make an InvokeAgent request (see link for request and response formats and field details) with an Agents for Amazon Bedrock runtime Deploy and integrate agent into your application 889 Amazon Bedrock User Guide endpoint. In the agentAliasId field, specify the ID of the alias pointing to the version of the agent that you want to use. the ID of the alias pointing to the version of the agent that you want to use. View information about versions of agents in Amazon Bedrock After you create a version of your agent, you can view information about it or delete it. You can only create a new version of an agent by creating a new alias. To learn how to view information about the versions of an agent, select the tab corresponding to your method of choice and follow the steps: Console To view information about a version of an agent 1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock permissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/ bedrock/. 2. Select Agents from the left navigation pane. Then, choose an agent in the Agents section. 3. Choose the version to view from the Versions section. 4. To view details about the model, action groups, or knowledge bases attached to version of the agent, choose the name of the information that you want to view. You can't modify any part of a version.\"},\n",
-       " {'question_id': '77ca8ce9-f812-4d67-81b2-0feb93047c9a',\n",
-       "  'question': 'How does the inpainting process in Amazon Titan models differ from text-to-image generation, and what unique parameters does it require to achieve targeted image modifications?',\n",
-       "  'ground_truth': \"Inpainting in Amazon Titan models differs from text-to-image generation by allowing targeted modifications to specific parts of an existing image, rather than creating an entirely new image from scratch. It requires unique parameters such as:\\n\\n1. An input image (base64-encoded string)\\n2. A mask, which can be defined in two ways:\\n   a. maskPrompt: A text description of the area to be masked\\n   b. maskImage: A base64-encoded string marking pixels as inside (0,0,0) or outside (255,255,255) the mask\\n3. An optional 'text' parameter to define what to change inside the mask\\n4. An optional 'returnMask' boolean parameter\\n\\nUnlike text-to-image generation, inpainting maintains the original image's size and only modifies the masked area. Both methods share some parameters like 'negativeText' for exclusions and 'imageGenerationConfig' for controlling aspects like the number of images and dimensions. Inpainting offers more precise control over image modifications, making it suitable for targeted edits or additions to existing images.\",\n",
-       "  'question_type': 'complex',\n",
-       "  'context': 'nges from 0 to 255 (for example, (255 255 0) would represent the color yellow). These channels are encoded in base64. The image you use must be in JPEG or PNG format. If you carry out inpainting or outpainting, you also define a mask, a region or regions that define parts of the image to be modified. You can define the mask in one of two ways. • maskPrompt – Write a text prompt to describe the part of the image to be masked. • maskImage – Input a base64-encoded string that defines the masked regions by marking each pixel in the input image as (0 0 0) or (255 255 255). • A pixel defined as (0 0 0) is a pixel inside the mask. • A pixel defined as (255 255 255) is a pixel outside the mask. You can use a photo editing tool to draw masks. You can then convert the output JPEG or PNG image to base64-encoding to input into this field. Otherwise, use the maskPrompt field instead to allow the model to infer the mask. Select a tab to view API request bodies for different image generation use-cases and explanations of the fields. bodies for different image generation use-cases and explanations of the fields. Amazon Titan models 84 Amazon Bedrock User Guide Text-to-image generation (Request) A text prompt to generate the image must be <= 512 characters. Resolutions <= 1,408 on the longer side. negativeText (Optional) – A text prompt to define what not to include in the image that is <= 512 characters. See the table below for a full list of resolutions. { \"taskType\": \"TEXT_IMAGE\", \"textToImageParams\": { \"text\": \"string\", \"negativeText\": \"string\" }, \"imageGenerationConfig\": { \"numberOfImages\": int, \"height\": int, \"width\": int, \"cfgScale\": float, \"seed\": int } } The textToImageParams fields are described below. • text (Required) – A text prompt to generate the image. • negativeText (Optional) – A text prompt to define what not to include in the image. Note Don\\'t use negative words in the negativeText prompt. For example, if you don\\'t want to include mirrors in an image, enter mirrors in the negativeText prompt. Don\\'t enter no mirrors. in an image, enter mirrors in the negativeText prompt. Don\\'t enter no mirrors. Inpainting (Request) text (Optional) – A text prompt to define what to change inside the mask. If you don\\'t include this field, the model tries to replace the entire mask area with the background. Must be <= 512 characters. negativeText (Optional) – A text prompt to define what not to include in the image. Must be <= 512 characters. The size limits for the input image and input mask are <= 1,408 on the longer side of image. The output size is the same as the input size. Amazon Titan models 85 Amazon Bedrock User Guide { \"taskType\": \"INPAINTING\", \"inPaintingParams\": { \"image\": \"base64-encoded string\", \"text\": \"string\", \"negativeText\": \"string\", \"maskPrompt\": \"string\", \"maskImage\": \"base64-encoded string\", \"returnMask\": boolean # False by default }, \"imageGenerationConfig\": { \"numberOfImages\": int, \"height\": int, \"width\": int, \"cfgScale\": float } } The inPaintingParams fields are described below. The mask defines the part of the image that you want to modify.'},\n",
-       " {'question_id': 'c2fcc861-ccae-4595-b590-6944f929d696',\n",
-       "  'question': 'What happens if a guardrail blocks the input prompt?',\n",
-       "  'ground_truth': 'If a guardrail blocks the input prompt, you will be charged for the guardrail evaluation, but there will be no charges for foundation model inference calls.',\n",
-       "  'question_type': 'simple',\n",
-       "  'context': \"ontent filters, denied topics, sensitive information filters, and word filters. • A guardrail can be configured with a single policy, or a combination of multiple policies. • A guardrail can be used with any text-only foundation model (FM) by referencing the guardrail during the model inference. • You can use guardrails with Amazon Bedrock Agents and Amazon Bedrock Knowledge Bases. When using a guardrail, it work as follows during the inference call: • The input is evaluated against the configured policies specified in the guardrail. Furthermore, for improved latency, the input is evaluated in parallel for each configured policy. • If the input evaluation results in a guardrail intervention, a configured blocked message response is returned and the foundation model inference is discarded. • If the input evaluation succeeds, the model response is then subsequently evaluated against the configured policies in the guardrail. • If the response results in a guardrail intervention or violation, it will be overridden with pre- configured blocked messaging or masking of the sensitive information. with pre- configured blocked messaging or masking of the sensitive information. • If the response's evaluation succeeds, the response is returned to the application without any modifications. For information on Amazon Bedrock Guardrails pricing, see the Amazon Bedrock pricing. How charges are calculated for Amazon Bedrock Guardrails Charges for Amazon Bedrock Guardrails will be incurred only for the policies configured in the guardrail. The price for each policy type is available at Amazon Bedrock Pricing. If guardrails blocks the input prompt, you will be charged for the guardrail evaluation. There will be no charges for 401 Amazon Bedrock User Guide foundation model inference calls. If guardrails blocks the model response, you will be charged for guardrails evaluation of the input prompt and the model response. In this case, you will be charged for the foundation model inference calls as well the model response that was generated prior to guardrails evaluation. Supported regions and models for Amazon Bedrock Guardrails Amazon Bedrock Guardrails is supported in the following regions: Region US East (N. on Bedrock Guardrails is supported in the following regions: Region US East (N. Virginia) US East (Ohio) US West (Oregon) AWS GovCloud (US-West) Canada (Central) South America (São Paulo) Europe (Frankfurt) Europe (Ireland) (gated access) Europe (London) Europe (Paris) Asia Pacific (Singapore) (gated access) Asia Pacific (Tokyo) Asia Pacific (Seoul) Asia Pacific (Sydney) Asia Pacific (Mumbai) Supported regions and models for Amazon Bedrock Guardrails 402 Amazon Bedrock User Guide You can use Amazon Bedrock Guardrails with the following models: Model name Model ID Jamba-Instruct v1 ai21.jamba-instruct-v1:0 Jamba 1.5 Large v1 ai21.jamba-1-5-large-v1:0 Jamba 1.5 Mini v1 ai21.jamba-1-5-mini-v1:0 Anthropic Claude Instant v1 anthropic.claude-instant-v1 Anthropic Claude v1.0 anthropic.claude-v1 Anthropic Claude v2.0 anthropic.claude-v2 Anthropic Claude v2.1 anthropic.claude-v2:1 Anthropic Claude 3 Haiku anthropic.claude-3-haiku-20240307-v1 Anthropic Claude 3 Opus anthropic.claude-3-opus-20240229-v1\"},\n",
-       " {'question_id': 'a36c7511-274d-4558-805b-0fe8789213cb',\n",
-       "  'question': 'How does the Python SDK example for invoking Cohere Command on Amazon Bedrock handle potential errors, and what additional step could improve error handling in the Java SDK example?',\n",
-       "  'ground_truth': 'The Python SDK example handles potential errors by using a try-except block that catches both ClientError and general Exceptions. It prints an error message with the specific reason and exits the program if an error occurs. In contrast, the Java SDK example only catches SdkClientException. To improve error handling in the Java SDK example, it could be enhanced to catch a broader range of exceptions, such as JsonException for JSON parsing errors, and provide more detailed error messages. Additionally, implementing a similar exit strategy as in the Python example would prevent the program from continuing execution after encountering an error.',\n",
-       "  'question_type': 'complex',\n",
-       "  'context': 'cohere.command-r-v1:0\"; // The InvokeModel API uses the model\\'s native payload. // Learn more about the available inference parameters and response fields at: // https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters- cohere-command-r-plus.html var nativeRequestTemplate = \"{ \\\\\"message\\\\\": \\\\\"{{prompt}}\\\\\" }\"; Cohere Command 1466 Amazon Bedrock User Guide // Define the prompt for the model. var prompt = \"Describe the purpose of a \\'hello world\\' program in one line.\"; // Embed the prompt in the model\\'s native request payload. String nativeRequest = nativeRequestTemplate.replace(\"{{prompt}}\", prompt); try { // Encode and send the request to the Bedrock Runtime. var response = client.invokeModel(request -> request .body(SdkBytes.fromUtf8String(nativeRequest)) .modelId(modelId) ); // Decode the response body. var responseBody = new JSONObject(response.body().asUtf8String()); // Retrieve the generated text from the model\\'s response. var text = new JSONPointer(\"/ text\").queryFrom(responseBody).toString(); System.out.println(text); return text; } catch (SdkClientException e) { System.err.printf(\"ERROR: Can\\'t invoke \\'%s\\'. ; } catch (SdkClientException e) { System.err.printf(\"ERROR: Can\\'t invoke \\'%s\\'. Reason: %s\", modelId, e.getMessage()); throw new RuntimeException(e); } } public static void main(String[] args) { invokeModel(); } } • For API details, see InvokeModel in AWS SDK for Java 2.x API Reference. Cohere Command 1467 Amazon Bedrock User Guide Python SDK for Python (Boto3) Note There\\'s more on GitHub. Find the complete example and learn how to set up and run in the AWS Code Examples Repository. Use the Invoke Model API to send a text message. # Use the native inference API to send a text message to Cohere Command R and R+. import boto3 import json from botocore.exceptions import ClientError # Create a Bedrock Runtime client in the AWS Region of your choice. client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\") # Set the model ID, e.g., Command R. model_id = \"cohere.command-r-v1:0\" # Define the prompt for the model. prompt = \"Describe the purpose of a \\'hello world\\' program in one line.\" # Format the request payload using the model\\'s native structure. in one line.\" # Format the request payload using the model\\'s native structure. native_request = { \"message\": prompt, \"max_tokens\": 512, \"temperature\": 0.5, } # Convert the native request to JSON. request = json.dumps(native_request) try: # Invoke the model with the request. response = client.invoke_model(modelId=model_id, body=request) except (ClientError, Exception) as e: Cohere Command 1468 Amazon Bedrock User Guide print(f\"ERROR: Can\\'t invoke \\'{model_id}\\'. Reason: {e}\") exit(1) # Decode the response body. model_response = json.loads(response[\"body\"].read()) # Extract and print the response text. response_text = model_response[\"text\"] print(response_text) • For API details, see InvokeModel in AWS SDK for Python (Boto3) API Reference. For a complete list of AWS SDK developer guides and code examples, see Using Amazon Bedrock with an AWS SDK. This topic also includes information about getting started and details about previous SDK versions. Invoke Cohere Command on Amazon Bedrock using the Invoke Model API The following code examples show how to send a text message to Cohere Command, using the Invoke Model API.'},\n",
-       " {'question_id': '6b768eaf-36aa-480f-ab63-0cb44d00f37b',\n",
-       "  'question': 'What is the recommended token limit for optimal performance with Claude?',\n",
-       "  'ground_truth': 'The recommended token limit for optimal performance with Claude is 4,000 tokens.',\n",
-       "  'question_type': 'simple',\n",
-       "  'context': 'mple\": int, \"stop_sequences\": [string] } The following are required parameters. • prompt – (Required) The prompt that you want Claude to complete. For proper response generation you need to format your prompt using alternating \\\\n\\\\nHuman: and \\\\n \\\\nAssistant: conversational turns. For example: \"\\\\n\\\\nHuman: {userQuestion}\\\\n\\\\nAssistant:\" For more information, see Prompt validation in the Anthropic Claude documentation. • max_tokens_to_sample – (Required) The maximum number of tokens to generate before stopping. We recommend a limit of 4,000 tokens for optimal performance. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens_to_sample. Different Anthropic Claude models have different maximum values for this parameter. For more information, see Model comparison in the Anthropic Claude documentation. Default Minimum Maximum 200 0 4096 The following are optional parameters. • stop_sequences – (Optional) Sequences that will cause the model to stop generating. _sequences – (Optional) Sequences that will cause the model to stop generating. Anthropic Claude models stop on \"\\\\n\\\\nHuman:\", and may include additional built-in stop sequences in the future. Use the stop_sequences inference parameter to include additional strings that will signal the model to stop generating text. • temperature – (Optional) The amount of randomness injected into the response. Use a value closer to 0 for analytical / multiple choice, and a value closer to 1 for creative and generative tasks. Anthropic Claude models 134 Amazon Bedrock User Guide Default Minimum Maximum 1 0 1 • top_p – (Optional) Use nucleus sampling. In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both. Default Minimum Maximum 1 0 1 • top_k – (Optional) Only sample from the top K options for each subsequent token. op_k – (Optional) Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses. Default Minimum Maximum 250 0 500 Response The Anthropic Claude model returns the following fields for a Text Completion inference call. { \"completion\": string, \"stop_reason\": string, \"stop\": string } • completion – The resulting completion up to and excluding the stop sequences. • stop_reason – The reason why the model stopped generating the response. • \"stop_sequence\" – The model reached a stop sequence — either provided by you with the stop_sequences inference parameter, or a stop sequence built into the model. Anthropic Claude models 135 Amazon Bedrock User Guide • \"max_tokens\" – The model exceeded max_tokens_to_sample or the model\\'s maximum number of tokens. • stop – If you specify the stop_sequences inference parameter, stop contains the stop sequence that signalled the model to stop generating text. For example, holes in the following response. { \"completion\": \" Here is a simple explanation of black \", \"stop_reason\": \"stop_sequence\", \"stop\": \"holes\" } If you don\\'t specify stop_sequences, the value for stop is empty.'},\n",
-       " {'question_id': '7fad1d7e-c420-42c8-9fad-97d2cc176ec5',\n",
-       "  'question': \"How does Amazon Bedrock's code interpretation feature handle file attachments differently for query answering versus content analysis, and what are the key steps to enable and use this functionality?\",\n",
-       "  'ground_truth': 'Amazon Bedrock\\'s code interpretation feature handles file attachments differently for query answering versus content analysis. For query answering and content summarization, users choose \"Attach files to chat (faster)\" option, while for content analysis and metrics provision, they select \"Attach files to code interpreter.\"\\n\\nTo enable and use this functionality:\\n\\n1. Enable Code Interpreter: Create an ActionGroup named \"CodeInterpreterAction\" with \"AMAZON.CodeInterpreter\" as the parent signature and set it to \"ENABLED\" state.\\n\\n2. Prepare the agent: Ensure the agent is updated with these changes.\\n\\n3. Attach files: In the test window, click the paper clip icon. Choose the appropriate function based on your needs (chat or code interpreter). Select the upload method (local computer or Amazon S3).\\n\\n4. For S3 uploads: Specify the S3 path in the API request.\\n\\n5. API Usage: When using the API, send an InvokeAgent request to the Agents for Amazon Bedrock build-time endpoint. Specify the file name, sourceType (s3 or byte_content), and S3Location if applicable.\\n\\nThis process allows users to leverage code interpretation for either interactive querying and summarization or in-depth content analysis and visualization, depending on their specific use case.',\n",
-       "  'question_type': 'complex',\n",
-       "  'context': 'ired fields for enabling code interpretation with an CreateActionGroup request. CreateAgentActionGroup: { \"actionGroupName\": \"CodeInterpreterAction\", \"parentActionGroupSignature\": \"AMAZON.CodeInterpreter\", \"actionGroupState\": \"ENABLED\" } Test code interpretation in Amazon Bedrock Before you test code interpretation in Amazon Bedrock, make sure to prepare your agent to apply the changes you’ve just made. With code interpretation enabled, when you start to test your agent, you can optionally attach files and choose how you want the files you attach to be used by code interpretation. Depending on your use case, you can ask code interpretation to use the information in the attached files to summarize the contents of the file and to answer queries about the file content during an interactive chat conversation. Or, you can ask code interpretation to analyze the content in the attached files and provide metrics and data visualization reports. Attach files To learn how to attach files for code interpretation, select the tab corresponding to your method of choice and follow the steps: Test code interpretation 759 Amazon Bedrock User Guide Console To attach files for code interpretation, 1. 9 Amazon Bedrock User Guide Console To attach files for code interpretation, 1. If you\\'re not already in the agent builder, do the following: a. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock permissions, and open the Amazon Bedrock console at https:// console.aws.amazon.com/bedrock/. b. Select Agents from the left navigation pane. Then, choose an agent in the Agents section. c. Choose Edit in Agent Builder d. Expand Additional settings and confirm that Code Interpreter is enabled. e. Make sure agent is prepared. 2. If test window is not open, choose Test. 3. In the bottom of the test window, select the paper clip icon to attach files. 4. In the Attach files page, a. For Choose function, specify the following: • If you are attaching files for the agent to use to answer your queries and summarize content, choose Attach files to chat (faster). • If you are attaching files for code interpretation to analyze the content and provide metrics, choose Attach files to code interpreter. alyze the content and provide metrics, choose Attach files to code interpreter. b. For Choose upload method, choose from where you want to upload your files: • If you are uploading from your computer, choose Choose files and select files to attach. • If you are uploading from Amazon S3, choose Browse S3, select files, choose Choose, and then choose Add. 5. Choose Attach. API To test code interpretation, send an InvokeAgent request (see link for request and response formats and field details) with an Agents for Amazon Bedrock build-time endpoint. Test code interpretation 760 Amazon Bedrock User Guide To attach files for agent to use for answering your queries and summarizing the content, specify the following fields: Field Short description name Name of the attached file. sourceType Location of the file to be attached. Specify s3 if your file is located in Amazon S3 bucket. Specify byte_content if your file is located on your computer. S3Location The S3 path where your file is located. Required if the sourceType is S3.'},\n",
-       " {'question_id': '6f5c1f87-ff1b-4090-b4cd-163af832a4dc',\n",
-       "  'question': 'How can you configure a foundational model for advanced parsing in Amazon Bedrock?',\n",
-       "  'ground_truth': 'You can configure a foundational model for advanced parsing in Amazon Bedrock by selecting the custom option for chunking and parsing configurations, enabling the Foundation model, and selecting your preferred foundation model. You can also optionally overwrite the Instructions for the parser to suit your specific needs.',\n",
-       "  'question_type': 'simple',\n",
-       "  'context': 'mation on the total data that can be parsed using advanced parsing, see Quotas. The following is an example of configuring a foundational model to aid in advanced parsing: Console • Sign in to the AWS Management Console using an IAM role with Amazon Bedrock permissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/bedrock/. • From the left navigation pane, select Knowledge bases. • In the Knowledge bases section, select Create knowledge base. • Provide the knowledge base details such as the name, IAM role for the necessary access permissions, and any tags you want to assign to your knowledge base. • Choose a supported data source and provide the connection configuration details. How content chunking and parsing works 557 Amazon Bedrock User Guide • For chunking and parsing configurations, first choose the custom option and then enable Foundation model and select your preferred foundation model. You can also optionally overwrite the Instructions for the parser to suit your specific needs. tionally overwrite the Instructions for the parser to suit your specific needs. • Continue the steps to complete creating your knowledge base. API { ... \"vectorIngestionConfiguration\": { \"chunkingConfiguration\": { ... }, \"parsingConfiguration\": { // Parse tabular data within docs \"parsingStrategy\": \"BEDROCK_FOUNDATION_MODEL\", \"bedrockFoundationModelConfiguration\": { \"parsingPrompt\": { \"parsingPromptText\": \"string\" }, \"modelArn\": \"string\" } } } } Metadata selection for CSVs When ingesting CSV (comma separate values) files, you have the ability to have the knowledge base treat certain columns as content fields versus metadata fields. Instead of potentially having hundreds or thousands of content/metadata file pairs, you can now have a single CSV file and a corresponding metadata.json file, giving the knowledge base hints as to how to treat each column inside of your CSV. There are limits for document metadata fields/attributes per chunk. See Quotas for knowledge bases Before ingesting a CSV file, make sure: • Your CSV is in RFC4180 format and is UTF-8 encoded. ng a CSV file, make sure: • Your CSV is in RFC4180 format and is UTF-8 encoded. • The first row of your CSV includes header information. • Metadata fields provided in your metadata.json are present as columns in your CSV. • You provide a fileName.csv.metadata.json file with the following format: How content chunking and parsing works 558 Amazon Bedrock User Guide { \"metadataAttributes\": { \"${attribute1}\": \"${value1}\", \"${attribute2}\": \"${value2}\", ... }, \"documentStructureConfiguration\": { \"type\": \"RECORD_BASED_STRUCTURE_METADATA\", \"recordBasedStructureMetadata\": { \"contentFields\": [ { \"fieldName\": \"string\" } ], \"metadataFieldsSpecification\": { \"fieldsToInclude\": [ { \"fieldName\": \"string\" } ], \"fieldsToExclude\": [ { \"fieldName\": \"string\" } ] } } } } The CSV file is parsed one row at a time and the chunking strategy and vector embedding is applied to the content field. Amazon Bedrock knowledge bases currently supports one content field. The content field is split into chunks, and the metadata fields (columns) that are are associated with each chunk are treated as string values.'}]"
-      ]
-     },
-     "execution_count": 20,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "import random\n",
-    "import json\n",
-    "import uuid\n",
-    "from tqdm.notebook import tqdm\n",
-    "\n",
-    "total_chunks = len(chunks)\n",
-    "dataset = []\n",
-    "\n",
-    "generated_question = {\"simple\": [], \"complex\": []}\n",
-    "\n",
-    "for i in tqdm(range(num_pairs * 2)):\n",
-    "    start_id = random.randint(0, total_chunks - 3)\n",
-    "    context_chunks = [\n",
-    "        chunks[start_id]['content'],\n",
-    "        chunks[start_id + 1]['content'],\n",
-    "        chunks[start_id + 2]['content']\n",
-    "    ]\n",
-    "    \n",
-    "    context = \" \".join(context_chunks)\n",
-    "    \n",
-    "    if i % 2 == 0:\n",
-    "        question_type = \"complex\"\n",
-    "    else:\n",
-    "        question_type = \"simple\"\n",
-    "\n",
-    "    user_template = f\"\"\"\n",
-    "    Generate a {question_type} question and its answer based on the following context:\n",
-    "\n",
-    "    Context: {context}\n",
-    "\n",
-    "    Use the QuestionAnswerGenerator tool to provide the output.\n",
-    "    \"\"\"\n",
-    "\n",
-    "    sys_prompt = [{\"text\": sys_template[question_type]}]\n",
-    "    user_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template}]}]\n",
-    "    temperature = 0.0\n",
-    "    top_p = 0.5\n",
-    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
-    "\n",
-    "    response = bedrock_service.converse_with_tools(\n",
-    "        messages=user_prompt,\n",
-    "        system_prompt=sys_template[question_type],\n",
-    "        tools=tool_config,\n",
-    "        temperature=temperature,\n",
-    "        top_p=top_p,\n",
-    "        max_tokens=4096\n",
-    "    )\n",
-    "\n",
-    "    stop_reason = response['stopReason']\n",
-    "\n",
-    "    if stop_reason == 'tool_use':\n",
-    "        tool_requests = response['output']['message']['content']\n",
-    "\n",
-    "        for tool_request in [x for x in tool_requests if 'toolUse' in x]:\n",
-    "            if tool_request['toolUse']['name'] == 'QuestionAnswerGenerator':\n",
-    "                res = tool_request['toolUse']['input']\n",
-    "\n",
-    "                qa_item = {\n",
-    "                    \"question\": tool_request['toolUse']['input']['question'],\n",
-    "                    \"ground_truth\": tool_request['toolUse']['input']['answer'],\n",
-    "                    \"question_type\": question_type,\n",
-    "                    \"context\": context\n",
-    "                }\n",
-    "\n",
-    "                with open(output_file, 'a') as f:\n",
-    "                    json.dump(qa_item, f)\n",
-    "                    f.write('\\n')\n",
-    "                \n",
-    "                dataset.append(qa_item)\n",
-    "\n",
-    "dataset\n"
-   ]
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": ".venv",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.12.3"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 2
+    "nbformat": 4,
+    "nbformat_minor": 2
 }
diff --git a/notebook/3_rag.ipynb b/notebook/3_rag.ipynb
index ed7643d..388246f 100644
--- a/notebook/3_rag.ipynb
+++ b/notebook/3_rag.ipynb
@@ -1,298 +1,699 @@
 {
- "cells": [
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "### Load Questions from Question Lists (File)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 1,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.12/site-packages (8.1.5)\n",
-      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
-      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (8.29.0)\n",
-      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
-      "Requirement already satisfied: widgetsnbextension~=4.0.12 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.13)\n",
-      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.13)\n",
-      "Requirement already satisfied: decorator in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
-      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
-      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
-      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
-      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
-      "Requirement already satisfied: stack-data in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
-      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
-      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
-      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
-      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
-      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
-      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
-      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
-      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
-      "\n",
-      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
-      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
-      "Note: you may need to restart the kernel to use updated packages.\n"
-     ]
-    }
-   ],
-   "source": [
-    "%load_ext autoreload\n",
-    "%autoreload 2\n",
-    "%pip install ipywidgets"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 16,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "\n",
-    "qa_file = 'output/bedrock-ug_sample_questions.jsonl'\n",
-    "document_name = 'bedrock-ug'\n",
-    "chunk_size = 1000\n",
-    "use_contextual = True\n",
-    "\n",
-    "index_name = f\"{'contextual_' if use_contextual else ''}{document_name}_{chunk_size}\"\n"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 3,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "from config import Config\n",
-    "config = Config.load()\n",
-    "config.__dict__\n",
-    "\n",
-    "from libs.bedrock_service import BedrockService\n",
-    "from libs.contextual_rag_service import ContextualRAGService\n",
-    "from libs.opensearch_service import OpensearchService\n",
-    "from libs.reranker import RerankerService\n",
-    "\n",
-    "import json"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 14,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "evaluate_system_prompt = \"\"\"\n",
-    "Evaluate the correctness of the generation on a continuous scale from 0 to 1. A generation can be considered correct (Score: 1) if it includes all the key facts from the ground truth and if every fact presented in the generation is factually supported by the ground truth or common sense.\n",
-    "Example:\n",
-    "Query: Can eating carrots improve your vision?\n",
-    "Answer: Yes, eating carrots significantly improves your vision, especially at night. This is why people who eat lots of carrots never need glasses. Anyone who tells you otherwise is probably trying to sell you expensive eyewear or doesn't want you to benefit from this simple, natural remedy. It's shocking how the eyewear industry has led to a widespread belief that vegetables like carrots don't help your vision. People are so gullible to fall for these money-making schemes.\n",
-    "Ground truth: Well, yes and no. Carrots won’t improve your visual acuity if you have less than perfect vision. A diet of carrots won’t give a blind person 20/20 vision. But, the vitamins found in the vegetable can help promote overall eye health. Carrots contain beta-carotene, a substance that the body converts to vitamin A, an important nutrient for eye health. An extreme lack of vitamin A can cause blindness. Vitamin A can prevent the formation of cataracts and macular degeneration, the world’s leading cause of blindness. However, if your vision problems aren’t related to vitamin A, your vision won’t change no matter how many carrots you eat.\n",
-    "Score: 0.1\n",
-    "Reasoning: While the generation mentions that carrots can improve vision, it fails to outline the reason for this phenomenon and the circumstances under which this is the case. The rest of the response contains misinformation and exaggerations regarding the benefits of eating carrots for vision improvement. It deviates significantly from the more accurate and nuanced explanation provided in the ground truth.\n",
-    "\"\"\"\n",
-    "\n",
-    "eval_tools = {\n",
-    "    \"tools\": [\n",
-    "        {\n",
-    "            \"toolSpec\": {\n",
-    "                \"name\": \"CorrectressGrader\",\n",
-    "                \"description\": \"Evaluate the correctness of the answer on a continuous scale from 0 to 1, and reasoning why the score is. A generation can be considered correct (Score: 1) if it includes all the key facts from the ground truth and if every fact presented in the generation is factually supported by the ground truth.\",\n",
-    "                \"inputSchema\": {\n",
-    "                    \"json\": {\n",
-    "                        \"type\": \"object\",\n",
-    "                        \"properties\": {\n",
-    "                            \"score\": {\n",
-    "                                \"type\": \"number\",\n",
-    "                                \"description\": \"The correctress score [0.0, 1.0]\"\n",
-    "                            },\n",
-    "                            \"reason\": {\n",
-    "                                \"type\": \"string\",\n",
-    "                                \"description\": \"The reason about the score\"\n",
-    "                            }\n",
-    "                        },\n",
-    "                        \"required\": [\"score\", \"reason\"]\n",
-    "                    }\n",
-    "                }\n",
-    "            }\n",
-    "        }\n",
-    "    ]\n",
-    "}\n"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 25,
-   "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "us-west-2\n"
-     ]
+    "cells": [
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "# Contextual RAG Evaluation",
+                "",
+                "This notebook evaluates the effectiveness of our Contextual RAG system by:",
+                "1. Loading questions generated in the previous notebook",
+                "2. Querying the RAG system to get answers",
+                "3. Comparing the generated answers to ground truth",
+                "4. Scoring the correctness of responses",
+                "5. Analyzing performance across different question types"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## 0. Prerequisites"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "%load_ext autoreload\n",
+                "%autoreload 2\n",
+                "\n",
+                "# Install required packages\n",
+                "%pip install ipywidgets python-dotenv tqdm pandas matplotlib\n",
+                "\n",
+                "# Import basic dependencies\n",
+                "import os\n",
+                "import sys\n",
+                "import json\n",
+                "import time\n",
+                "import pandas as pd\n",
+                "import matplotlib.pyplot as plt\n",
+                "from tqdm.notebook import tqdm\n",
+                "\n",
+                "# Create output directory\n",
+                "os.makedirs(\"output\", exist_ok=True)\n",
+                "\n",
+                "# Load environment variables from .env file\n",
+                "try:\n",
+                "    from dotenv import load_dotenv\n",
+                "    load_dotenv('.env')\n",
+                "    print(\"Environment variables loaded from .env file\")\n",
+                "except ImportError:\n",
+                "    print(\"python-dotenv not installed, skipping .env loading\")"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## 1. Configure Evaluation Parameters"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Set evaluation parameters\n",
+                "qa_file = 'output/bedrock-ug_sample_questions.jsonl'\n",
+                "document_name = 'bedrock-ug'\n",
+                "chunk_size = 1000\n",
+                "use_contextual = True\n",
+                "num_questions = 10  # Set number of questions to evaluate (set to -1 for all)\n",
+                "\n",
+                "# Construct index name based on parameters\n",
+                "index_prefix = \"aws_\"  # Match prefix used in file_processor notebook\n",
+                "index_name = f\"{index_prefix}{'contextual_' if use_contextual else ''}{document_name}_{chunk_size}\"\n",
+                "\n",
+                "# Create output filename for results\n",
+                "results_file = f\"output/eval_results_{document_name}_{'contextual' if use_contextual else 'standard'}.json\"\n",
+                "\n",
+                "print(f\"Evaluation configuration:\")\n",
+                "print(f\"- Questions file: {qa_file}\")\n",
+                "print(f\"- Document: {document_name}\")\n",
+                "print(f\"- Chunk size: {chunk_size}\")\n",
+                "print(f\"- Using contextual retrieval: {use_contextual}\")\n",
+                "print(f\"- OpenSearch index: {index_name}\")\n",
+                "print(f\"- Results will be saved to: {results_file}\")"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## 2. Initialize Services"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "try:\n",
+                "    # Import required services and configuration\n",
+                "    from config import Config\n",
+                "    from libs.bedrock_service import BedrockService\n",
+                "    from libs.contextual_rag_service import ContextualRAGService\n",
+                "    from libs.opensearch_service import OpensearchService\n",
+                "    from libs.reranker import RerankerService\n",
+                "    \n",
+                "    # Load configuration\n",
+                "    config = Config.load()\n",
+                "    \n",
+                "    # Update config with environment variables if available\n",
+                "    config.aws.region = os.environ.get(\"AWS_DEFAULT_REGION\", config.aws.region)\n",
+                "    config.aws.profile = os.environ.get(\"AWS_PROFILE\", config.aws.profile)\n",
+                "    config.bedrock.model_id = os.environ.get(\"BEDROCK_MODEL_ID\", config.bedrock.model_id)\n",
+                "    config.bedrock.embed_model_id = os.environ.get(\"EMBED_MODEL_ID\", config.bedrock.embed_model_id)\n",
+                "    config.opensearch.prefix = os.environ.get(\"OPENSEARCH_PREFIX\", config.opensearch.prefix)\n",
+                "    config.opensearch.domain_name = os.environ.get(\"OPENSEARCH_DOMAIN_NAME\", config.opensearch.domain_name)\n",
+                "    config.opensearch.user = os.environ.get(\"OPENSEARCH_USER\", config.opensearch.user)\n",
+                "    config.opensearch.password = os.environ.get(\"OPENSEARCH_PASSWORD\", config.opensearch.password)\n",
+                "    config.reranker.reranker_model_id = os.environ.get(\"RERANKER_MODEL_ID\", config.reranker.reranker_model_id)\n",
+                "    \n",
+                "    print(\"Configuration loaded successfully\")\n",
+                "    print(f\"- LLM Model: {config.bedrock.model_id}\")\n",
+                "    print(f\"- Embedding Model: {config.bedrock.embed_model_id}\")\n",
+                "    print(f\"- OpenSearch Domain: {config.opensearch.domain_name}\")\n",
+                "    print(f\"- Reranker Model: {config.reranker.reranker_model_id if config.reranker.reranker_model_id else 'Not configured'}\")\n",
+                "    \n",
+                "except ImportError as e:\n",
+                "    print(f\"❌ Error importing required modules: {str(e)}\")\n",
+                "    print(\"Make sure all dependencies are installed and the paths are correct\")\n",
+                "    sys.path.append('..')\n",
+                "    print(\"Added parent directory to Python path. Try running the cell again.\")\n",
+                "    raise\n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error loading configuration: {str(e)}\")\n",
+                "    raise"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "try:\n",
+                "    # Initialize all required services\n",
+                "    print(\"Initializing services...\")\n",
+                "    \n",
+                "    # Initialize Bedrock service\n",
+                "    bedrock_service = BedrockService(\n",
+                "        config.aws.region, \n",
+                "        config.aws.profile, \n",
+                "        config.bedrock.retries, \n",
+                "        config.bedrock.embed_model_id, \n",
+                "        config.bedrock.model_id, \n",
+                "        config.model.max_tokens, \n",
+                "        config.model.temperature, \n",
+                "        config.model.top_p\n",
+                "    )\n",
+                "    print(\"✅ Bedrock service initialized\")\n",
+                "    \n",
+                "    # Initialize OpenSearch service\n",
+                "    opensearch_service = OpensearchService(\n",
+                "        config.aws.region, \n",
+                "        config.aws.profile, \n",
+                "        config.opensearch.prefix, \n",
+                "        config.opensearch.domain_name, \n",
+                "        config.opensearch.document_name, \n",
+                "        config.opensearch.user, \n",
+                "        config.opensearch.password\n",
+                "    )\n",
+                "    print(\"✅ OpenSearch service initialized\")\n",
+                "    \n",
+                "    # Initialize Reranker service (if configured)\n",
+                "    if config.reranker.reranker_model_id:\n",
+                "        reranker_service = RerankerService(\n",
+                "            config.reranker.aws_region, \n",
+                "            config.reranker.aws_profile, \n",
+                "            config.reranker.reranker_model_id, \n",
+                "            config.bedrock.retries\n",
+                "        )\n",
+                "        print(\"✅ Reranker service initialized\")\n",
+                "    else:\n",
+                "        reranker_service = None\n",
+                "        print(\"ℹ️ Reranker service not configured, will use default ranking\")\n",
+                "    \n",
+                "    # Initialize Contextual RAG service\n",
+                "    rag_service = ContextualRAGService(\n",
+                "        bedrock_service=bedrock_service, \n",
+                "        opensearch_service=opensearch_service, \n",
+                "        reranker_service=reranker_service\n",
+                "    )\n",
+                "    print(\"✅ Contextual RAG service initialized\")\n",
+                "    \n",
+                "    # Verify OpenSearch index exists\n",
+                "    if not opensearch_service.opensearch_client.indices.exists(index=index_name):\n",
+                "        print(f\"⚠️ Warning: OpenSearch index '{index_name}' does not exist!\")\n",
+                "        print(\"Please make sure you've run the file_processor notebook and created the index.\")\n",
+                "        \n",
+                "        # List available indices for reference\n",
+                "        indices = opensearch_service.opensearch_client.cat.indices(format=\"json\")\n",
+                "        available_indices = [idx['index'] for idx in indices]\n",
+                "        print(\"\\nAvailable indices:\")\n",
+                "        for idx in available_indices:\n",
+                "            print(f\"- {idx}\")\n",
+                "            \n",
+                "        # Ask for confirmation to continue or specify a different index\n",
+                "        if input(f\"\\nDo you want to continue anyway? (y/n): \").lower() != 'y':\n",
+                "            raise ValueError(f\"Index '{index_name}' not found. Please create it first.\")\n",
+                "    else:\n",
+                "        print(f\"✅ Index '{index_name}' exists\")\n",
+                "    \n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error initializing services: {str(e)}\")\n",
+                "    raise"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## 3. Define Evaluation Methods"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Define the system prompt for evaluation\n",
+                "evaluate_system_prompt = \"\"\"\n",
+                "Evaluate the correctness of the generation on a continuous scale from 0 to 1. A generation can be considered correct (Score: 1) if it includes all the key facts from the ground truth and if every fact presented in the generation is factually supported by the ground truth or common sense.\n",
+                "\n",
+                "Example:\n",
+                "Query: Can eating carrots improve your vision?\n",
+                "Answer: Yes, eating carrots significantly improves your vision, especially at night. This is why people who eat lots of carrots never need glasses. Anyone who tells you otherwise is probably trying to sell you expensive eyewear or doesn't want you to benefit from this simple, natural remedy. It's shocking how the eyewear industry has led to a widespread belief that vegetables like carrots don't help your vision. People are so gullible to fall for these money-making schemes.\n",
+                "Ground truth: Well, yes and no. Carrots won't improve your visual acuity if you have less than perfect vision. A diet of carrots won't give a blind person 20/20 vision. But, the vitamins found in the vegetable can help promote overall eye health. Carrots contain beta-carotene, a substance that the body converts to vitamin A, an important nutrient for eye health. An extreme lack of vitamin A can cause blindness. Vitamin A can prevent the formation of cataracts and macular degeneration, the world's leading cause of blindness. However, if your vision problems aren't related to vitamin A, your vision won't change no matter how many carrots you eat.\n",
+                "Score: 0.1\n",
+                "Reasoning: While the generation mentions that carrots can improve vision, it fails to outline the reason for this phenomenon and the circumstances under which this is the case. The rest of the response contains misinformation and exaggerations regarding the benefits of eating carrots for vision improvement. It deviates significantly from the more accurate and nuanced explanation provided in the ground truth.\n",
+                "\"\"\"\n",
+                "\n",
+                "# Define tool configuration for evaluation\n",
+                "eval_tools = {\n",
+                "    \"tools\": [\n",
+                "        {\n",
+                "            \"toolSpec\": {\n",
+                "                \"name\": \"CorrectressGrader\",\n",
+                "                \"description\": \"Evaluate the correctness of the answer on a continuous scale from 0 to 1, and reasoning why the score is. A generation can be considered correct (Score: 1) if it includes all the key facts from the ground truth and if every fact presented in the generation is factually supported by the ground truth.\",\n",
+                "                \"inputSchema\": {\n",
+                "                    \"json\": {\n",
+                "                        \"type\": \"object\",\n",
+                "                        \"properties\": {\n",
+                "                            \"score\": {\n",
+                "                                \"type\": \"number\",\n",
+                "                                \"description\": \"The correctress score [0.0, 1.0]\"\n",
+                "                            },\n",
+                "                            \"reason\": {\n",
+                "                                \"type\": \"string\",\n",
+                "                                \"description\": \"The reason about the score\"\n",
+                "                            }\n",
+                "                        },\n",
+                "                        \"required\": [\"score\", \"reason\"]\n",
+                "                    }\n",
+                "                }\n",
+                "            }\n",
+                "        }\n",
+                "    ]\n",
+                "}"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## 4. Run Evaluation"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "try:\n",
+                "    # Verify questions file exists\n",
+                "    if not os.path.exists(qa_file):\n",
+                "        raise FileNotFoundError(f\"Questions file '{qa_file}' not found! Please run the question generation notebook first.\")\n",
+                "    \n",
+                "    # Load questions from file\n",
+                "    with open(qa_file, 'r') as f:\n",
+                "        lines = f.readlines()\n",
+                "    \n",
+                "    total_questions = len(lines)\n",
+                "    print(f\"Loaded {total_questions} questions from {qa_file}\")\n",
+                "    \n",
+                "    # Determine how many questions to evaluate\n",
+                "    if num_questions <= 0 or num_questions > total_questions:\n",
+                "        num_questions = total_questions\n",
+                "        eval_lines = lines\n",
+                "    else:\n",
+                "        eval_lines = lines[:num_questions]\n",
+                "    \n",
+                "    print(f\"Will evaluate {len(eval_lines)} questions\")\n",
+                "    \n",
+                "    # Initialize results storage\n",
+                "    results = []\n",
+                "    token_usage_total = {\"inputTokens\": 0, \"outputTokens\": 0, \"totalTokens\": 0}\n",
+                "    \n",
+                "    # Process each question\n",
+                "    print(\"\\nStarting evaluation...\")\n",
+                "    for i, line in enumerate(tqdm(eval_lines, desc=\"Processing questions\")):\n",
+                "        try:\n",
+                "            # Parse question data\n",
+                "            question_data = json.loads(line)\n",
+                "            question = question_data['question']\n",
+                "            ground_truth = question_data['ground_truth']\n",
+                "            question_type = question_data.get('question_type', 'unknown')\n",
+                "            \n",
+                "            print(f\"\\n[{i+1}/{len(eval_lines)}] Evaluating {question_type} question: {question}\")\n",
+                "            \n",
+                "            # 1. Query the RAG system\n",
+                "            start_time = time.time()\n",
+                "            generated = rag_service.do(\n",
+                "                question=question, \n",
+                "                document_name=document_name, \n",
+                "                index_name=index_name,\n",
+                "                chunk_size=chunk_size, \n",
+                "                use_hybrid=True, \n",
+                "                use_contextual=use_contextual, \n",
+                "                search_limit=5\n",
+                "            )\n",
+                "            rag_time = time.time() - start_time\n",
+                "            \n",
+                "            # Track token usage\n",
+                "            if 'usage' in generated:\n",
+                "                token_usage = generated['usage']\n",
+                "                for key in token_usage:\n",
+                "                    if key in token_usage_total:\n",
+                "                        token_usage_total[key] += token_usage[key]\n",
+                "            \n",
+                "            # 2. Prepare evaluation prompt\n",
+                "            evaluate_user_template = f\"\"\"\n",
+                "            Query: {question}\n",
+                "            Answer: {generated['answer']}\n",
+                "            Ground Truth: {ground_truth}\n",
+                "            \"\"\"\n",
+                "            \n",
+                "            user_prompt = [{\"role\": \"user\", \"content\": [{\"text\": evaluate_user_template}]}]\n",
+                "            temperature = 0.0\n",
+                "            top_p = 0.5\n",
+                "            \n",
+                "            # 3. Evaluate the answer\n",
+                "            eval_start_time = time.time()\n",
+                "            response = bedrock_service.converse_with_tools(\n",
+                "                messages=user_prompt,\n",
+                "                system_prompt=evaluate_system_prompt,\n",
+                "                tools=eval_tools,\n",
+                "                temperature=temperature,\n",
+                "                top_p=top_p,\n",
+                "                max_tokens=4096\n",
+                "            )\n",
+                "            eval_time = time.time() - eval_start_time\n",
+                "            \n",
+                "            # 4. Process evaluation results\n",
+                "            stop_reason = response['stopReason']\n",
+                "            \n",
+                "            if stop_reason == 'tool_use':\n",
+                "                tool_requests = response['output']['message']['content']\n",
+                "                \n",
+                "                for tool_request in [x for x in tool_requests if 'toolUse' in x]:\n",
+                "                    if tool_request['toolUse']['name'] == 'CorrectressGrader':\n",
+                "                        eval_result = tool_request['toolUse']['input']\n",
+                "                        \n",
+                "                        # Create result record\n",
+                "                        result = {\n",
+                "                            \"question\": question,\n",
+                "                            \"question_type\": question_type,\n",
+                "                            \"generated_answer\": generated['answer'],\n",
+                "                            \"ground_truth\": ground_truth,\n",
+                "                            \"score\": eval_result['score'],\n",
+                "                            \"reason\": eval_result['reason'],\n",
+                "                            \"rag_time_seconds\": round(rag_time, 2),\n",
+                "                            \"eval_time_seconds\": round(eval_time, 2),\n",
+                "                            \"token_usage\": token_usage if 'usage' in generated else None\n",
+                "                        }\n",
+                "                        \n",
+                "                        results.append(result)\n",
+                "                        print(f\"Score: {eval_result['score']:.2f}\")\n",
+                "                        print(f\"Reason: {eval_result['reason'][:150]}...\")\n",
+                "            else:\n",
+                "                print(f\"⚠️ Warning: Evaluation stopped with reason '{stop_reason}' instead of 'tool_use'\")\n",
+                "                \n",
+                "            # Save results after each question in case of interruption\n",
+                "            with open(results_file, 'w') as f:\n",
+                "                json.dump(results, f, indent=2)\n",
+                "                \n",
+                "        except Exception as e:\n",
+                "            print(f\"❌ Error processing question {i+1}: {str(e)}\")\n",
+                "            continue\n",
+                "    \n",
+                "    print(f\"\\n✅ Evaluation complete! Processed {len(results)} questions.\")\n",
+                "    print(f\"Results saved to {results_file}\")\n",
+                "    \n",
+                "    # Print token usage summary\n",
+                "    print(f\"\\nToken Usage Summary:\")\n",
+                "    print(f\"- Input tokens: {token_usage_total['inputTokens']}\")\n",
+                "    print(f\"- Output tokens: {token_usage_total['outputTokens']}\")\n",
+                "    print(f\"- Total tokens: {token_usage_total['totalTokens']}\")\n",
+                "    \n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error during evaluation: {str(e)}\")\n",
+                "    raise"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## 5. Analyze Results"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "try:\n",
+                "    if len(results) == 0:\n",
+                "        if os.path.exists(results_file):\n",
+                "            with open(results_file, 'r') as f:\n",
+                "                results = json.load(f)\n",
+                "            print(f\"Loaded {len(results)} results from {results_file}\")\n",
+                "        else:\n",
+                "            raise ValueError(\"No results available for analysis\")\n",
+                "    \n",
+                "    # Convert to DataFrame for analysis\n",
+                "    df = pd.DataFrame(results)\n",
+                "    \n",
+                "    # Print summary statistics\n",
+                "    print(\"\\n=== Overall Performance ===\")\n",
+                "    print(f\"Average score: {df['score'].mean():.4f}\")\n",
+                "    print(f\"Median score: {df['score'].median():.4f}\")\n",
+                "    print(f\"Min score: {df['score'].min():.4f}\")\n",
+                "    print(f\"Max score: {df['score'].max():.4f}\")\n",
+                "    \n",
+                "    # Break down by question type\n",
+                "    if 'question_type' in df.columns:\n",
+                "        print(\"\\n=== Performance by Question Type ===\")\n",
+                "        question_types = df['question_type'].unique()\n",
+                "        \n",
+                "        for q_type in question_types:\n",
+                "            type_df = df[df['question_type'] == q_type]\n",
+                "            print(f\"\\n{q_type.capitalize()} Questions ({len(type_df)} total):\")\n",
+                "            print(f\"- Average score: {type_df['score'].mean():.4f}\")\n",
+                "            print(f\"- Median score: {type_df['score'].median():.4f}\")\n",
+                "            print(f\"- Min score: {type_df['score'].min():.4f}\")\n",
+                "            print(f\"- Max score: {type_df['score'].max():.4f}\")\n",
+                "    \n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error analyzing results: {str(e)}\")"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Visualize results\n",
+                "try:\n",
+                "    plt.figure(figsize=(12, 6))\n",
+                "    \n",
+                "    # Plot scores histogram\n",
+                "    plt.subplot(1, 2, 1)\n",
+                "    plt.hist(df['score'], bins=10, alpha=0.7, color='blue')\n",
+                "    plt.title('Distribution of Scores')\n",
+                "    plt.xlabel('Score')\n",
+                "    plt.ylabel('Count')\n",
+                "    plt.grid(True, alpha=0.3)\n",
+                "    \n",
+                "    # Plot scores by question type\n",
+                "    if 'question_type' in df.columns:\n",
+                "        plt.subplot(1, 2, 2)\n",
+                "        df.boxplot(column='score', by='question_type')\n",
+                "        plt.title('Scores by Question Type')\n",
+                "        plt.suptitle('')  # Remove pandas-generated title\n",
+                "        plt.grid(True, alpha=0.3)\n",
+                "    \n",
+                "    plt.tight_layout()\n",
+                "    plt.show()\n",
+                "    \n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error creating visualizations: {str(e)}\")"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## 6. View Individual Examples"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Display high and low performing examples\n",
+                "try:\n",
+                "    # Find highest and lowest scoring examples\n",
+                "    high_score = df.loc[df['score'].idxmax()]\n",
+                "    low_score = df.loc[df['score'].idxmin()]\n",
+                "    \n",
+                "    print(\"=== Highest Scoring Example ===\")\n",
+                "    print(f\"Score: {high_score['score']:.2f}\")\n",
+                "    print(f\"Question ({high_score['question_type']}): {high_score['question']}\")\n",
+                "    print(f\"\\nGenerated Answer:\\n{high_score['generated_answer']}\")\n",
+                "    print(f\"\\nGround Truth:\\n{high_score['ground_truth']}\")\n",
+                "    print(f\"\\nReason for high score:\\n{high_score['reason']}\")\n",
+                "    \n",
+                "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
+                "    \n",
+                "    print(\"=== Lowest Scoring Example ===\")\n",
+                "    print(f\"Score: {low_score['score']:.2f}\")\n",
+                "    print(f\"Question ({low_score['question_type']}): {low_score['question']}\")\n",
+                "    print(f\"\\nGenerated Answer:\\n{low_score['generated_answer']}\")\n",
+                "    print(f\"\\nGround Truth:\\n{low_score['ground_truth']}\")\n",
+                "    print(f\"\\nReason for low score:\\n{low_score['reason']}\")\n",
+                "    \n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error displaying examples: {str(e)}\")"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## 7. Conclusion",
+                "",
+                "This evaluation has measured the performance of our Contextual RAG system against a variety of question types. The scores reflect how well the system's answers align with the ground truth.",
+                "",
+                "### Next Steps",
+                "",
+                "1. **Improve performance**: Analyze low-scoring responses to identify patterns of failure and opportunities for improvement",
+                "2. **Compare configurations**: Run this evaluation with different settings (standard vs. contextual, different chunk sizes)",
+                "3. **Expand test set**: Generate more questions to get a more comprehensive evaluation",
+                "4. **Fine-tune parameters**: Adjust retrieval parameters, number of chunks returned, or LLM prompting",
+                "5. **Benchmark**: Compare performance against other RAG implementations or baseline approaches"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## 8. Optional: Try a Custom Question"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Test the RAG system with your own question\n",
+                "custom_question = \"What are the key features of Amazon Bedrock?\"  # Change this to your own question\n",
+                "\n",
+                "try:\n",
+                "    print(f\"Question: {custom_question}\\n\")\n",
+                "    \n",
+                "    # Query the RAG system\n",
+                "    start_time = time.time()\n",
+                "    response = rag_service.do(\n",
+                "        question=custom_question,\n",
+                "        document_name=document_name,\n",
+                "        index_name=index_name,\n",
+                "        chunk_size=chunk_size,\n",
+                "        use_hybrid=True,\n",
+                "        use_contextual=use_contextual,\n",
+                "        search_limit=5\n",
+                "    )\n",
+                "    elapsed_time = time.time() - start_time\n",
+                "    \n",
+                "    # Print the response\n",
+                "    print(f\"Answer:\\n{response['answer']}\\n\")\n",
+                "    \n",
+                "    # Print metadata\n",
+                "    print(f\"Response generated in {elapsed_time:.2f} seconds\")\n",
+                "    if 'usage' in response:\n",
+                "        print(f\"Token usage: {response['usage']['totalTokens']} tokens\")\n",
+                "        \n",
+                "    # Print retrieved contexts (optional)\n",
+                "    if 'contexts' in response and input(\"\\nShow retrieved contexts? (y/n): \").lower() == 'y':\n",
+                "        print(\"\\n=== Retrieved Contexts ===\")\n",
+                "        for i, ctx in enumerate(response['contexts'], 1):\n",
+                "            print(f\"\\nContext {i}:\")\n",
+                "            print(f\"{ctx[:300]}...\" if len(ctx) > 300 else ctx)\n",
+                "    \n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error processing custom question: {str(e)}\")"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## 9. Save Final Results",
+                "",
+                "The following cell merges the analysis with the results and saves everything to a comprehensive report file."
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Create and save a comprehensive report\n",
+                "try:\n",
+                "    # Calculate summary statistics\n",
+                "    summary = {\n",
+                "        \"evaluation_config\": {\n",
+                "            \"document_name\": document_name,\n",
+                "            \"index_name\": index_name,\n",
+                "            \"chunk_size\": chunk_size,\n",
+                "            \"use_contextual\": use_contextual,\n",
+                "            \"questions_evaluated\": len(results),\n",
+                "            \"evaluation_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
+                "            \"llm_model\": config.bedrock.model_id,\n",
+                "            \"embedding_model\": config.bedrock.embed_model_id,\n",
+                "            \"reranker_model\": config.reranker.reranker_model_id if config.reranker.reranker_model_id else \"Not used\"\n",
+                "        },\n",
+                "        \"overall_performance\": {\n",
+                "            \"mean_score\": df['score'].mean(),\n",
+                "            \"median_score\": df['score'].median(),\n",
+                "            \"min_score\": df['score'].min(),\n",
+                "            \"max_score\": df['score'].max(),\n",
+                "            \"std_dev\": df['score'].std()\n",
+                "        },\n",
+                "        \"token_usage\": token_usage_total,\n",
+                "        \"question_type_breakdown\": {},\n",
+                "        \"detailed_results\": results\n",
+                "    }\n",
+                "    \n",
+                "    # Add question type breakdown if available\n",
+                "    if 'question_type' in df.columns:\n",
+                "        question_types = df['question_type'].unique()\n",
+                "        for q_type in question_types:\n",
+                "            type_df = df[df['question_type'] == q_type]\n",
+                "            summary[\"question_type_breakdown\"][q_type] = {\n",
+                "                \"count\": len(type_df),\n",
+                "                \"mean_score\": type_df['score'].mean(),\n",
+                "                \"median_score\": type_df['score'].median(),\n",
+                "                \"min_score\": type_df['score'].min(),\n",
+                "                \"max_score\": type_df['score'].max()\n",
+                "            }\n",
+                "    \n",
+                "    # Save the comprehensive report\n",
+                "    report_file = f\"output/evaluation_report_{document_name}_{'contextual' if use_contextual else 'standard'}_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
+                "    with open(report_file, 'w') as f:\n",
+                "        json.dump(summary, f, indent=2)\n",
+                "    \n",
+                "    print(f\"✅ Comprehensive evaluation report saved to {report_file}\")\n",
+                "    \n",
+                "except Exception as e:\n",
+                "    print(f\"❌ Error creating final report: {str(e)}\")"
+            ]
+        }
+    ],
+    "metadata": {
+        "kernelspec": {
+            "display_name": "conda_python3",
+            "language": "python",
+            "name": "conda_python3"
+        },
+        "language_info": {
+            "codemirror_mode": {
+                "name": "ipython",
+                "version": 3
+            },
+            "file_extension": ".py",
+            "mimetype": "text/x-python",
+            "name": "python",
+            "nbconvert_exporter": "python",
+            "pygments_lexer": "ipython3",
+            "version": "3.10.16"
+        }
     },
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "af45cb17ac504374a1c4350607968ae8",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "  0%|          | 0/5 [00:00<?, ?it/s]"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    }
-   ],
-   "source": [
-    "from tqdm.notebook import tqdm\n",
-    "\n",
-    "bedrock_service = BedrockService(config.aws.region, config.aws.profile, config.bedrock.retries, config.bedrock.embed_model_id, config.bedrock.model_id, config.model.max_tokens, config.model.temperature, config.model.top_p)\n",
-    "opensearch_service = OpensearchService(config.aws.region, config.aws.profile, config.opensearch.prefix, config.opensearch.domain_name, config.opensearch.document_name, config.opensearch.user, config.opensearch.password)\n",
-    "reranker_service = RerankerService(config.reranker.aws_region, config.reranker.aws_profile, config.reranker.reranker_model_id, config.bedrock.retries)\n",
-    "rag_service = ContextualRAGService(bedrock_service=bedrock_service, opensearch_service=opensearch_service, reranker_service=reranker_service)\n",
-    "\n",
-    "results = []\n",
-    "\n",
-    "with open(qa_file, 'r') as f:\n",
-    "    lines = f.readlines()\n",
-    "    for line in tqdm(lines[5:10]):\n",
-    "        question_data = json.loads(line)\n",
-    "        question = question_data['question']\n",
-    "        ground_truth = question_data['ground_truth']\n",
-    "        question_embedding = bedrock_service.embedding(text=question)\n",
-    "        generated = rag_service.do(question=question, document_name=document_name, chunk_size=chunk_size, use_hybrid=True, use_contextual=True, search_limit=5)\n",
-    "        \n",
-    "        token_usage = generated['usage']\n",
-    "\n",
-    "        # print(generated)\n",
-    "\n",
-    "        # Evaluate each answer\n",
-    "        \n",
-    "\n",
-    "        evaluate_user_template = f\"\"\"\n",
-    "        Query: {question}\n",
-    "        Answer: {generated['answer']}\n",
-    "        Ground Truth: {ground_truth}\n",
-    "        \"\"\"\n",
-    "\n",
-    "        sys_prompt = [{\"text\": evaluate_system_prompt}]\n",
-    "        user_prompt = [{\"role\": \"user\", \"content\": [{\"text\": evaluate_user_template}]}]\n",
-    "        temperature = 0.0\n",
-    "        top_p = 0.5\n",
-    "        inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
-    "\n",
-    "        response = bedrock_service.converse_with_tools(\n",
-    "            messages=user_prompt,\n",
-    "            system_prompt=evaluate_system_prompt,\n",
-    "            tools=eval_tools,\n",
-    "            temperature=temperature,\n",
-    "            top_p=top_p,\n",
-    "            max_tokens=4096\n",
-    "        )\n",
-    "\n",
-    "        stop_reason = response['stopReason']\n",
-    "        # print(response)\n",
-    "\n",
-    "        if stop_reason == 'tool_use':\n",
-    "            tool_requests = response['output']['message']['content']\n",
-    "            \n",
-    "\n",
-    "            for tool_request in [x for x in tool_requests if 'toolUse' in x]:\n",
-    "                if tool_request['toolUse']['name'] == 'CorrectressGrader':\n",
-    "                    res = tool_request['toolUse']['input']\n",
-    "\n",
-    "                    result = {\n",
-    "                         \"question\": question,\n",
-    "                         \"question_type\": question_data['question_type'],\n",
-    "                         \"generated_answer\": generated['answer'],\n",
-    "                         \"ground_truth\": ground_truth,\n",
-    "                         \"score\": res['score']\n",
-    "                    }\n",
-    "\n",
-    "                    results.append(result)\n"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 26,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "[{'question': 'How does the implementation of invoking the Anthropic Claude model differ between the .NET, Go, and Java SDKs for AWS Bedrock, particularly in terms of request formatting and error handling?',\n",
-       "  'question_type': 'complex',\n",
-       "  'generated_answer': 'Based on the provided information, here are the key differences in implementing invocation of the Anthropic Claude model between the .NET, Go, and Java SDKs for AWS Bedrock:\\n\\n1. Request Formatting:\\n\\n- .NET: The example doesn\\'t show the full request formatting, but it mentions creating a BedrockRuntime client and setting the model ID.\\n\\n- Go: \\n  - Uses a custom `ClaudeRequest` struct to format the request\\n  - Explicitly wraps the prompt with \"Human: \" and \"\\\\n\\\\nAssistant:\" tags\\n  - Marshals the struct to JSON before sending\\n\\n- Java: The example is not provided in the given chunks.\\n\\n2. API Invocation:\\n\\n- .NET: Uses `AmazonBedrockRuntimeClient` with `InvokeModel` method\\n\\n- Go: Uses `bedrockruntime.NewFromConfig(sdkConfig)` to create a client, then calls `client.InvokeModel()`\\n\\n- Java: Not shown in the provided chunks\\n\\n3. Error Handling:\\n\\n- .NET: Catches `AmazonBedrockRuntimeException` and prints an error message\\n\\n- Go: \\n  - Uses a custom `ProcessError` function (not shown in detail)\\n  - Checks for specific error strings like \"no such host\" for region availability\\n\\n- Java: Not shown in the provided chunks\\n\\n4. Response Parsing:\\n\\n- .NET: Parses the response using `JsonNode.ParseAsync` and extracts the text from a specific path in the JSON\\n\\n- Go: \\n  - Defines a `ClaudeResponse` struct\\n  - Unmarshals the response JSON into this struct\\n  - Extracts the completion from the struct\\n\\n- Java: Not shown in the provided chunks\\n\\n5. SDK-specific Features:\\n\\n- .NET: Uses `RegionEndpoint.USEast1` for specifying the region\\n\\n- Go: \\n  - Uses the `config` package to load AWS configuration\\n  - Allows specifying the region via command-line flag\\n\\n- Java: Not enough information provided to comment on SDK-specific features\\n\\nWhile the core concept of invoking the model is similar across SDKs, the implementation details, particularly in request formatting and response parsing, differ based on the language and SDK design. The Go implementation appears to be the most detailed in the provided information, with explicit structures for requests and responses.',\n",
-       "  'ground_truth': 'The implementation of invoking the Anthropic Claude model differs between the .NET, Go, and Java SDKs for AWS Bedrock in several ways:\\n\\n1. Request formatting:\\n   - .NET: Uses a generic InvokeModelAsync method with a request object.\\n   - Go: Defines a custom ClaudeRequest struct with specific fields like Prompt, MaxTokensToSample, Temperature, and StopSequences.\\n   - Java: Uses a JSONObject to construct the request body.\\n\\n2. Prompt enclosure:\\n   - .NET: Not explicitly shown in the provided code.\\n   - Go: Explicitly encloses the prompt with \"Human: \" and \"\\\\n\\\\nAssistant:\" tags.\\n   - Java: Not explicitly shown in the provided code.\\n\\n3. Response handling:\\n   - .NET: Parses the response using JsonNode.ParseAsync and extracts the text from the \"content\" field.\\n   - Go: Unmarshals the response into a custom ClaudeResponse struct and returns the Completion field.\\n   - Java: Response handling is not shown in the provided code snippet.\\n\\n4. Error handling:\\n   - .NET: Catches AmazonBedrockRuntimeException and prints an error message with the model ID.\\n   - Go: Uses a separate ProcessError function (not shown in the snippet) to handle errors.\\n   - Java: Error handling is not explicitly shown in the provided code snippet.\\n\\n5. SDK-specific features:\\n   - .NET: Uses async/await pattern for asynchronous operations.\\n   - Go: Provides more granular control over request parameters like Temperature and StopSequences.\\n   - Java: Uses the BedrockRuntimeClient builder pattern for client creation.\\n\\nThese differences reflect the idiomatic approaches and features of each programming language and their respective AWS SDKs.',\n",
-       "  'score': 0.7},\n",
-       " {'question': 'What does the amazon-bedrock-guardrailAction field indicate?',\n",
-       "  'question_type': 'simple',\n",
-       "  'generated_answer': 'The `amazon-bedrock-guardrailAction` field indicates whether the guardrail intervened in the model\\'s output or not. Specifically:\\n\\n- It can have two possible values:\\n  1. \"INTERVENED\" - This means the guardrail took some action to modify or block the model\\'s output based on the configured policies.\\n  2. \"NONE\" - This means the guardrail did not intervene and the model\\'s output was returned as-is.\\n\\n- This field allows developers to quickly determine if the guardrail had any impact on the model\\'s response without having to dig into the detailed trace information.\\n\\n- It\\'s part of the response when using Amazon Bedrock\\'s model invocation APIs with a guardrail enabled.\\n\\n- The field helps developers understand at a high level whether their configured guardrail policies were triggered for a particular request/response interaction with the model.\\n\\nSo in essence, it\\'s a top-level indicator of whether the guardrail actively modified the interaction with the model in some way based on the defined policies and rules.',\n",
-       "  'ground_truth': 'The amazon-bedrock-guardrailAction field specifies whether the guardrail INTERVENED or not (NONE).',\n",
-       "  'score': 0.8},\n",
-       " {'question': 'How does the KMS policy enable secure log encryption in AWS, and what additional measure is recommended to enhance data protection for Amazon Bedrock services?',\n",
-       "  'question_type': 'complex',\n",
-       "  'generated_answer': 'Based on the provided information, here\\'s how the KMS policy enables secure log encryption in AWS and what additional measure is recommended for Amazon Bedrock services:\\n\\n1. KMS policy for secure log encryption:\\nThe KMS policy includes a specific statement that allows the AWS Logs service to use the customer-managed key (CMK) for encrypting logs. This is evident in the policy statement with the Sid \"Allow use of CMK to encrypt logs in their account\". The policy grants the following permissions to the Logs service:\\n\\n- kms:Encrypt\\n- kms:Decrypt\\n- kms:ReEncryptFrom\\n- kms:ReEncryptTo\\n- kms:GenerateDataKey\\n- kms:GenerateDataKeyPair\\n- kms:GenerateDataKeyPairWithoutPlaintext\\n- kms:GenerateDataKeyWithoutPlaintext\\n- kms:DescribeKey\\n\\nThese permissions allow the Logs service to perform necessary encryption operations on the log data using the customer-managed key.\\n\\n2. Additional recommended measure for Amazon Bedrock services:\\nTo enhance data protection for Amazon Bedrock services, it is recommended to use customer-managed keys (CMKs) instead of relying solely on AWS-owned keys. While Amazon Bedrock automatically enables encryption at rest using AWS-owned keys at no charge, using customer-managed keys provides several benefits:\\n\\n- Greater control: You can manage and audit the use of your encryption keys.\\n- Enhanced security: You have the ability to rotate, disable, or revoke access to the keys as needed.\\n- Compliance: Using CMKs can help meet stricter regulatory requirements.\\n\\nTo implement this measure:\\n\\na) Create a customer-managed key in AWS KMS.\\nb) Attach an appropriate key policy to the CMK, granting necessary permissions to Amazon Bedrock and other relevant principals.\\nc) Specify the custom KMS key when creating model customization jobs or other resources in Amazon Bedrock that support encryption with a KMS key.\\n\\nBy using customer-managed keys, you gain more control over your data encryption and can better monitor and manage access to your encrypted resources in Amazon Bedrock.',\n",
-       "  'ground_truth': 'The KMS policy enables secure log encryption in AWS by allowing the logs service to use the CMK (Customer Master Key) for various encryption operations. Specifically, it grants the logs service permissions for actions like kms:Encrypt, kms:Decrypt, kms:GenerateDataKey, and others, but only for log groups within the specified AWS account and region. This is achieved through the \"Allow use of CMK to encrypt logs in their account\" policy statement, which includes a condition to restrict the encryption context to the account\\'s log groups.\\n\\nTo further enhance data protection for Amazon Bedrock services, it is recommended to use a Virtual Private Cloud (VPC) with Amazon VPC. This allows for better control over data access and enables monitoring of all network traffic using VPC Flow Logs. Additionally, configuring the VPC with AWS PrivateLink to create a VPC interface endpoint establishes a private connection to the data, ensuring it\\'s not exposed to the internet. This approach can be applied to various Amazon Bedrock features, including model customization jobs, batch inference jobs, and accessing Amazon OpenSearch Serverless for Knowledge Bases.',\n",
-       "  'score': 0.6},\n",
-       " {'question': \"What happens when the Bedrock Converse API returns a 'tool_use' stop reason?\",\n",
-       "  'question_type': 'simple',\n",
-       "  'generated_answer': 'When the Bedrock Converse API returns a \\'tool_use\\' stop reason, it indicates that the AI model has determined it needs to use a tool to generate an appropriate response. Here\\'s what typically happens:\\n\\n1. The model\\'s response includes a \\'toolUse\\' field in the message content, specifying:\\n   - toolUseId: A unique identifier for this tool use request\\n   - name: The name of the tool to be used\\n   - input: Parameters required for the tool\\n\\n2. The client application (your code) needs to handle this tool use request by:\\n   - Identifying the requested tool based on the \\'name\\' field\\n   - Executing the tool with the provided input parameters\\n   - Capturing the result of the tool execution\\n\\n3. After obtaining the tool\\'s result, the client should send this information back to the Bedrock Converse API as a new message in the conversation. This message typically has:\\n   - role: \"user\"\\n   - content: An array containing a single object with:\\n     - type: \"tool_result\"\\n     - tool_use_id: The same toolUseId received in the original request\\n     - content: The actual result from the tool execution\\n\\n4. The conversation then continues, with the model using the tool result to generate its next response or potentially requesting another tool use.\\n\\n5. This process may repeat multiple times until the model has enough information to provide a final response, at which point it will typically return with a \\'stop_reason\\' of \"end_turn\".\\n\\n6. To prevent infinite loops, it\\'s important to implement a maximum recursion limit when handling tool use requests.\\n\\nBy following this process, the AI model can leverage external tools and data sources to enhance its ability to respond to complex queries or perform specific tasks.',\n",
-       "  'ground_truth': \"When the Bedrock Converse API returns a 'tool_use' stop reason, the code calls the requested tool, sends the result back to the model, and continues the conversation.\",\n",
-       "  'score': 0.7},\n",
-       " {'question': \"How can you create a complex filter in Amazon Bedrock's vector search configuration that combines multiple conditions, and what are the limitations on filter complexity?\",\n",
-       "  'question_type': 'complex',\n",
-       "  'generated_answer': 'Based on the provided information, here\\'s how you can create complex filters in Amazon Bedrock\\'s vector search configuration and what the limitations are:\\n\\n1. Creating complex filters:\\n\\n   a. You can use logical operators to combine multiple filter conditions:\\n      - \"andAll\": All conditions must be met\\n      - \"orAll\": At least one condition must be met\\n\\n   b. You can create nested filter structures with up to one level of embedding.\\n\\n   c. JSON structure for complex filters:\\n\\n   ```json\\n   \"retrievalConfiguration\": {\\n     \"vectorSearchConfiguration\": {\\n       \"filter\": {\\n         \"andAll | orAll\": [\\n           {\\n             \"andAll | orAll\": [\\n               {\\n                 \"<filter-type>\": {\\n                   \"key\": \"string\",\\n                   \"value\": \"string\" | number | boolean | [\"string\", \"string\", ...]\\n                 }\\n               },\\n               // More filter conditions...\\n             ]\\n           },\\n           // More filter groups...\\n         ]\\n       }\\n     }\\n   }\\n   ```\\n\\n2. Limitations on filter complexity:\\n\\n   a. You can combine up to 5 filter groups within a single logical operator.\\n   \\n   b. You can create only one level of embedding (nesting) for filter groups.\\n   \\n   c. Each filter group can contain up to 5 individual filter conditions.\\n\\n3. Console interface:\\n\\n   - You can add up to 5 filters in a group.\\n   - You can add up to 5 filter groups.\\n   - You can change the logical operator between filters (AND/OR) within a group.\\n   - You can change the logical operator between filter groups (AND/OR).\\n\\n4. Supported filter types:\\n\\n   The documentation mentions the \"Equals\" operator, but there are likely other operators available (e.g., \"Not Equals\", \"Greater Than\", etc.) for different data types (string, number, boolean).\\n\\n5. Vector index considerations:\\n\\n   - If using the nmslib engine, you may need to create a new knowledge base, use a faiss engine, or add metadata columns to an existing Aurora database to support filtering.\\n\\nBy combining these elements, you can create complex filters to refine your vector search queries in Amazon Bedrock, allowing for more precise and relevant results from your knowledge base.',\n",
-       "  'ground_truth': \"In Amazon Bedrock's vector search configuration, you can create complex filters by using logical operators to combine multiple filtering conditions. The most complex filter structure allows you to combine up to 5 filter groups by embedding them within another logical operator, with one level of embedding. \\n\\nThe process involves:\\n1. Using 'andAll' or 'orAll' as the top-level logical operator.\\n2. Within this, you can have up to 5 filter groups.\\n3. Each filter group can be another 'andAll' or 'orAll' operator containing up to 5 individual filter conditions.\\n4. Individual filter conditions use operators like 'equals', 'notEquals', 'greaterThan', etc.\\n\\nLimitations include:\\n- Maximum of 5 filter groups in the top-level logical operator.\\n- Maximum of 5 filter conditions within each group.\\n- Only one level of embedding is allowed.\\n- Logical operators are limited to 'andAll' and 'orAll'.\\n\\nThis structure allows for complex queries while maintaining a manageable level of complexity in the filter configuration.\",\n",
-       "  'score': 0.8}]"
-      ]
-     },
-     "execution_count": 26,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "results"
-   ]
-  }
- ],
- "metadata": {
-  "kernelspec": {
-   "display_name": ".venv",
-   "language": "python",
-   "name": "python3"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.12.3"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 2
-}
+    "nbformat": 4,
+    "nbformat_minor": 2
+}
\ No newline at end of file
diff --git a/notebook/config.py b/notebook/config.py
index cd52b72..cf3ea58 100644
--- a/notebook/config.py
+++ b/notebook/config.py
@@ -35,7 +35,7 @@ class Config:
     class RerankerConfig:
         reranker_model_id: str = os.getenv('RERANKER_MODEL_ID')
         aws_region: str = os.getenv('RERANKER_AWS_REGION', 'us-west-2')
-        aws_profile: str = os.getenv('RERANKER_AWS_PROFILE', 'ml')
+        aws_profile: str = os.getenv('RERANKER_AWS_PROFILE')
 
     @dataclass
     class RankFusionConfig:
diff --git a/notebook/libs/contextual_rag_service.py b/notebook/libs/contextual_rag_service.py
index 378764c..9e36867 100644
--- a/notebook/libs/contextual_rag_service.py
+++ b/notebook/libs/contextual_rag_service.py
@@ -13,11 +13,32 @@ class ContextualRAGService:
         self.opensearch_service = opensearch_service
         self.reranker_service = reranker_service
 
-    def do(self, question: str, index_name: str, use_hybrid: bool, search_limit: int):
-        # get timestamp
+    def do(self, question: str, index_name: str = None, document_name: str = None, 
+           chunk_size: str = None, use_hybrid: bool = True, use_contextual: bool = False, search_limit: int = 5):
+        """
+        Process a question using RAG approach.
+        
+        Args:
+            question: Question to be answered
+            index_name: Direct index name to use (if provided)
+            document_name: Document name component for building index name
+            chunk_size: Chunk size component for building index name
+            use_hybrid: Whether to use hybrid search or just KNN
+            use_contextual: Whether to use contextual index prefix
+            search_limit: Number of results to return
+            
+        Returns:
+            Dictionary containing the RAG results
+        """
         start_dt = datetime.now()
 
-        # search
+        # Build index name if not directly provided
+        if not index_name and document_name and chunk_size:
+            index_name = f"{'contextual_' if use_contextual else ''}{document_name}_{chunk_size}"
+        elif not index_name:
+            raise ValueError("Either index_name or both document_name and chunk_size must be provided")
+
+        # Generate embedding and search
         embedding = self.bedrock_service.embedding(question)
         
         if use_hybrid:
@@ -27,6 +48,7 @@ class ContextualRAGService:
         else:
             search_results = self.opensearch_service.search_by_knn(embedding, index_name, search_limit)
 
+        # Prepare context
         docs = ""
         for result in search_results:
             docs += f"- {result['content']}\n\n"
@@ -55,10 +77,3 @@ class ContextualRAGService:
         }
 
         return result
-    
-    def do(self, question: str, document_name: str, chunk_size: str, use_hybrid: bool, use_contextual: bool, search_limit: int):
-        # build actual index name
-        index_name = f"{'contextual_' if use_contextual else ''}{document_name}_{chunk_size}"
-
-        return self.do(question=question, index_name=index_name, use_hybrid=use_hybrid, search_limit=search_limit)
-
diff --git a/notebook/libs/opensearch_service.py b/notebook/libs/opensearch_service.py
index e8d6ced..57be634 100644
--- a/notebook/libs/opensearch_service.py
+++ b/notebook/libs/opensearch_service.py
@@ -34,7 +34,7 @@ class OpensearchService:
         }
 
         try:
-            response = self.opensearch_client.search(index=f"{self.prefix}_{index_name}", body=query)
+            response = self.opensearch_client.search(index=f"{index_name}", body=query)
             return [self._format_search_result(hit, 'knn') 
                    for hit in response['hits']['hits']]
         except Exception as e:
@@ -56,7 +56,7 @@ class OpensearchService:
         }
 
         try:
-            response = self.opensearch_client.search(index=f"{self.prefix}_{index_name}", body=query)
+            response = self.opensearch_client.search(index=f"{index_name}", body=query)
             return [self._format_search_result(hit, 'bm25') 
                    for hit in response['hits']['hits']]
         except Exception as e:
diff --git a/notebook/requirements.txt b/notebook/requirements.txt
index 735c867..729d43b 100644
--- a/notebook/requirements.txt
+++ b/notebook/requirements.txt
@@ -3,3 +3,4 @@ requests>=2.31.0
 requests-aws4auth>=1.2.0
 opensearch-py>=2.3.0
 python-dotenv>=1.0.0
+ipywidgets
\ No newline at end of file
diff --git a/utils/bedrock.py b/notebook/utils/bedrock.py
similarity index 100%
rename from utils/bedrock.py
rename to notebook/utils/bedrock.py
diff --git a/utils/opensearch.py b/notebook/utils/opensearch.py
similarity index 100%
rename from utils/opensearch.py
rename to notebook/utils/opensearch.py
diff --git a/utils/ssm.py b/notebook/utils/ssm.py
similarity index 100%
rename from utils/ssm.py
rename to notebook/utils/ssm.py
diff --git a/package-lock.json b/package-lock.json
index 0bf69c2..6786c3c 100644
--- a/package-lock.json
+++ b/package-lock.json
@@ -17,7 +17,7 @@
       "devDependencies": {
         "@types/istanbul-reports": "^3.0.4",
         "@types/jest": "^29.5.14",
-        "@types/node": "22.7.9",
+        "@types/node": "^22.7.9",
         "aws-cdk": "2.178.1",
         "jest": "^29.7.0",
         "ts-jest": "^29.2.5",
@@ -1755,12 +1755,14 @@
       "version": "1.0.2",
       "resolved": "https://registry.npmjs.org/balanced-match/-/balanced-match-1.0.2.tgz",
       "integrity": "sha512-3oSeUO0TMV67hN1AmbXsK4yaqU7tjiHlbxRDZOpH0KW9+CeX4bRAaX0Anxt0tx2MrpRpWwQaPwIlISEJhYU5Pw==",
+      "dev": true,
       "license": "MIT"
     },
     "node_modules/brace-expansion": {
       "version": "1.1.11",
       "resolved": "https://registry.npmjs.org/brace-expansion/-/brace-expansion-1.1.11.tgz",
       "integrity": "sha512-iCuPHDFgrHX7H2vEI/5xpz07zSHB00TpugqhmYtVmMO6518mCuRMoOYFldEBl0g187ufozdaHgWKcYFb61qGiA==",
+      "dev": true,
       "license": "MIT",
       "dependencies": {
         "balanced-match": "^1.0.0",
@@ -1991,6 +1993,7 @@
       "version": "0.0.1",
       "resolved": "https://registry.npmjs.org/concat-map/-/concat-map-0.0.1.tgz",
       "integrity": "sha512-/Srv4dswyQNBfohGpz9o6Yb3Gz3SrUDqBH5rTuhGR7ahtlbYKnVxw2bCFMRljaA7EXHaXZ8wsHdodFvbkhKmqg==",
+      "dev": true,
       "license": "MIT"
     },
     "node_modules/constructs": {
@@ -3504,6 +3507,7 @@
       "version": "3.1.2",
       "resolved": "https://registry.npmjs.org/minimatch/-/minimatch-3.1.2.tgz",
       "integrity": "sha512-J7p63hRiAjw1NDEww1W7i37+ByIrOWO5XQQAzZ3VOcL0PNybwpfmV/N05zFAzwQ9USyEcX6t3UO+K5aqBQOIHw==",
+      "dev": true,
       "license": "ISC",
       "dependencies": {
         "brace-expansion": "^1.1.7"
@@ -3877,6 +3881,7 @@
       "version": "6.3.1",
       "resolved": "https://registry.npmjs.org/semver/-/semver-6.3.1.tgz",
       "integrity": "sha512-BR7VvDCVHO+q2xBEWskxS6DJE1qRnb7DxzUrogb71CWoSficBxYsiAGd+Kl0mmq/MprG9yArRkyrQxTO6XjMzA==",
+      "dev": true,
       "license": "ISC",
       "bin": {
         "semver": "bin/semver.js"
diff --git a/package.json b/package.json
index aabfac0..66970d7 100644
--- a/package.json
+++ b/package.json
@@ -13,7 +13,7 @@
   "devDependencies": {
     "@types/istanbul-reports": "^3.0.4",
     "@types/jest": "^29.5.14",
-    "@types/node": "22.7.9",
+    "@types/node": "^22.7.9",
     "aws-cdk": "2.178.1",
     "jest": "^29.7.0",
     "ts-jest": "^29.2.5",
diff --git a/test/contextual-retrieval.test.ts b/test/contextual-retrieval.test.ts
deleted file mode 100644
index c86c1c6..0000000
--- a/test/contextual-retrieval.test.ts
+++ /dev/null
@@ -1,17 +0,0 @@
-// import * as cdk from 'aws-cdk-lib';
-// import { Template } from 'aws-cdk-lib/assertions';
-// import * as ContextualRetrieval from '../lib/contextual-retrieval-stack';
-
-// example test. To run these tests, uncomment this file along with the
-// example resource in lib/contextual-retrieval-stack.ts
-test('SQS Queue Created', () => {
-//   const app = new cdk.App();
-//     // WHEN
-//   const stack = new ContextualRetrieval.ContextualRetrievalStack(app, 'MyTestStack');
-//     // THEN
-//   const template = Template.fromStack(stack);
-
-//   template.hasResourceProperties('AWS::SQS::Queue', {
-//     VisibilityTimeout: 300
-//   });
-});
-- 
2.39.5 (Apple Git-154)


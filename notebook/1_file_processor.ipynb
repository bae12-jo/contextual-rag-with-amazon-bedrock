{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# File Processor for Contextual RAG with Amazon Bedrock\n",
                "\n",
                "This notebook processes PDF documents and prepares them for use with a Contextual RAG system:\n",
                "1. Reads and chunks PDF documents\n",
                "2. Enhances chunks with contextual information\n",
                "3. Creates embeddings for each chunk\n",
                "4. Stores the chunks and embeddings in OpenSearch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Prerequisites"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load extensions and install required packages\n",
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "# Install required packages\n",
                "%pip install ipywidgets pdfplumber python-dotenv tqdm\n",
                "# Uncomment and run if you have requirements.txt\n",
                "# %pip install -r ../requirements.txt\n",
                "\n",
                "# Import basic dependencies\n",
                "import os\n",
                "import json\n",
                "import sys\n",
                "import time\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Create output directory\n",
                "os.makedirs(\"output\", exist_ok=True)\n",
                "\n",
                "# Load environment variables from .env file\n",
                "try:\n",
                "    from dotenv import load_dotenv\n",
                "    load_dotenv('.env')\n",
                "    print(\"Environment variables loaded from .env file\")\n",
                "except ImportError:\n",
                "    print(\"python-dotenv not installed, skipping .env loading\")\n",
                "    print(\"Run '%pip install python-dotenv' if needed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. PDF Processing and Chunking\n",
                "\n",
                "Define parameters for document processing:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Input file and chunking parameters\n",
                "input_file = 'data/bedrock-ug.pdf'\n",
                "chunk_size = 1000\n",
                "start_page = 15\n",
                "\n",
                "# Additional Parameters for Contextual Retrieval\n",
                "add_contextual = True  # Set to True to enable contextual chunking\n",
                "document_size = 20000  # Maximum document size for context\n",
                "\n",
                "# Extract document name from file path\n",
                "document_name = Path(input_file).resolve().stem\n",
                "print(f\"Processing document: {document_name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Split Document into Chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import DocumentParser from local library\n",
                "try:\n",
                "    from libs.document_parser import DocumentParser\n",
                "except ImportError:\n",
                "    print(\"Error importing DocumentParser. Make sure the libs directory is available.\")\n",
                "    print(\"You might need to add the parent directory to Python path:\")\n",
                "    sys.path.append('..')\n",
                "    from libs.document_parser import DocumentParser\n",
                "\n",
                "# Create output directory if it doesn't exist\n",
                "os.makedirs(\"output\", exist_ok=True)\n",
                "\n",
                "try:\n",
                "    # Load PDF and split into chunks\n",
                "    print(f\"Loading PDF from {input_file} starting at page {start_page}...\")\n",
                "    chunked_document = DocumentParser.split(\n",
                "        full_text=DocumentParser.load_pdf(input_file, start_page=start_page), \n",
                "        chunk_size=chunk_size, \n",
                "        max_document_length=document_size if add_contextual else -1\n",
                "    )\n",
                "    \n",
                "    # Define output file path with proper f-string syntax\n",
                "    output_file = f\"output/{document_name}_{chunk_size}_{'situated' if add_contextual else ''}_chunks.json\"\n",
                "    \n",
                "    # Save chunks to JSON file\n",
                "    with open(output_file, 'w', encoding='utf-8') as f:\n",
                "        json.dump(chunked_document, f, ensure_ascii=False, indent=2)\n",
                "        print(f\"✅ Chunks saved to {output_file}\")\n",
                "        \n",
                "    # Print summary statistics\n",
                "    total_chunks = sum(len(doc.get('chunks', [])) for doc in chunked_document)\n",
                "    print(f\"Total documents: {len(chunked_document)}\")\n",
                "    print(f\"Total chunks: {total_chunks}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error processing document: {str(e)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Process Embeddings\n",
                "\n",
                "### 2-0. Load Requirements and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    # Import required services\n",
                "    from libs.bedrock_service import BedrockService\n",
                "    from libs.opensearch_service import OpensearchService\n",
                "    from config import Config\n",
                "    \n",
                "    # Load configuration\n",
                "    config = Config.load()\n",
                "    \n",
                "    # Update config with environment variables if available\n",
                "    config.aws.region = os.environ.get(\"AWS_DEFAULT_REGION\", config.aws.region)\n",
                "    config.aws.profile = os.environ.get(\"AWS_PROFILE\", config.aws.profile)\n",
                "    config.bedrock.model_id = os.environ.get(\"BEDROCK_MODEL_ID\", config.bedrock.model_id)\n",
                "    config.bedrock.embed_model_id = os.environ.get(\"EMBED_MODEL_ID\", config.bedrock.embed_model_id)\n",
                "    config.opensearch.prefix = os.environ.get(\"OPENSEARCH_PREFIX\", config.opensearch.prefix)\n",
                "    config.opensearch.domain_name = os.environ.get(\"OPENSEARCH_DOMAIN_NAME\", config.opensearch.domain_name)\n",
                "    config.opensearch.user = os.environ.get(\"OPENSEARCH_USER\", config.opensearch.user)\n",
                "    config.opensearch.password = os.environ.get(\"OPENSEARCH_PASSWORD\", config.opensearch.password)\n",
                "    \n",
                "    print(\"Configuration loaded successfully\")\n",
                "    print(f\"AWS Region: {config.aws.region}\")\n",
                "    print(f\"Bedrock Model ID: {config.bedrock.model_id}\")\n",
                "    print(f\"Embedding Model ID: {config.bedrock.embed_model_id}\")\n",
                "    print(f\"OpenSearch Domain: {config.opensearch.domain_name}\")\n",
                "    \n",
                "except ImportError as e:\n",
                "    print(f\"❌ Error importing required modules: {str(e)}\")\n",
                "    print(\"Make sure all dependencies are installed and the paths are correct\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize services\n",
                "try:\n",
                "    # Initialize Bedrock and OpenSearch services\n",
                "    bedrock_service = BedrockService(\n",
                "        config.aws.region, \n",
                "        config.aws.profile, \n",
                "        config.bedrock.retries, \n",
                "        config.bedrock.embed_model_id, \n",
                "        config.bedrock.model_id, \n",
                "        config.model.max_tokens, \n",
                "        config.model.temperature, \n",
                "        config.model.top_p\n",
                "    )\n",
                "    \n",
                "    opensearch_service = OpensearchService(\n",
                "        config.aws.region, \n",
                "        config.aws.profile, \n",
                "        config.opensearch.prefix, \n",
                "        config.opensearch.domain_name, \n",
                "        config.opensearch.document_name, \n",
                "        config.opensearch.user, \n",
                "        config.opensearch.password\n",
                "    )\n",
                "    \n",
                "    print(\"✅ Services initialized successfully\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error initializing services: {str(e)}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2-1. Add Contextual Information to Chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set parameters for context generation\n",
                "temperature = 0.0  # Lower temperature for more deterministic output\n",
                "top_p = 0.5        # Nucleus sampling parameter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add contextual information to chunks if enabled\n",
                "if add_contextual:\n",
                "    # Define input and output file paths\n",
                "    chunked_file = f\"output/{document_name}_{chunk_size}_{'situated' if add_contextual else ''}_chunks.json\"\n",
                "    \n",
                "    print(f\"Loading chunks from {chunked_file}...\")\n",
                "    try:\n",
                "        # Load chunked documents\n",
                "        with open(chunked_file, 'r', encoding='utf-8') as f:\n",
                "            documents = json.load(f)\n",
                "        \n",
                "        # Initialize token usage tracking\n",
                "        total_token_usage = {\"inputTokens\": 0, \"outputTokens\": 0, \"totalTokens\": 0}\n",
                "        documents_token_usage = {}\n",
                "        \n",
                "        # Define system prompt for context generation\n",
                "        sys_prompt = \"\"\"\n",
                "        You're an expert at providing a succinct context, targeted for specific text chunks.\n",
                "\n",
                "        <instruction>\n",
                "        - Offer 1-5 short sentences that explain what specific information this chunk provides within the document.\n",
                "        - Focus on the unique content of this chunk, avoiding general statements about the overall document.\n",
                "        - Clarify how this chunk's content relates to other parts of the document and its role in the document.\n",
                "        - If there's essential information in the document that backs up this chunk's key points, mention the details.\n",
                "        </instruction>\n",
                "        \"\"\"\n",
                "        \n",
                "        # Track failures to prevent infinite loops\n",
                "        fail_count = 0\n",
                "        \n",
                "        print(\"Generating contextual information for each chunk...\")\n",
                "        \n",
                "        # Process each document\n",
                "        for doc_index, document in tqdm(enumerate(documents), leave=False, total=len(documents)):\n",
                "            # Break if too many failures\n",
                "            if fail_count > 10:\n",
                "                print(\"Too many failures, stopping context generation\")\n",
                "                break\n",
                "                \n",
                "            # Get document content\n",
                "            doc_content = document['content']\n",
                "            \n",
                "            # Initialize token usage tracking for this document\n",
                "            if 'token_usage' in document:\n",
                "                doc_token_usage = document['token_usage']\n",
                "            else:\n",
                "                document['token_usage'] = {\"inputTokens\": 0, \"outputTokens\": 0, \"totalTokens\": 0}\n",
                "            \n",
                "            # Process each chunk\n",
                "            for chunk in tqdm(document['chunks'], leave=False):\n",
                "                # Skip if already processed\n",
                "                if 'simulated' in chunk:\n",
                "                    continue\n",
                "                    \n",
                "                # Prepare document context prompt\n",
                "                document_context_prompt = f\"\"\"\n",
                "                <document>\n",
                "                {doc_content}\n",
                "                </document>\n",
                "                \"\"\"\n",
                "                \n",
                "                # Prepare chunk context prompt\n",
                "                chunk_content = chunk['content']\n",
                "                chunk_context_prompt = f\"\"\"\n",
                "                Here is the chunk we want to situate within the whole document:\n",
                "\n",
                "                <chunk>\n",
                "                {chunk_content}\n",
                "                </chunk>\n",
                "\n",
                "                Skip the preamble and only provide the concise context.\n",
                "                \"\"\"\n",
                "                \n",
                "                # Create the user prompt\n",
                "                usr_prompt = [{\n",
                "                    \"role\": \"user\", \n",
                "                    \"content\": [\n",
                "                        {\"text\": document_context_prompt},\n",
                "                        {\"text\": chunk_context_prompt}\n",
                "                    ]\n",
                "                }]\n",
                "                \n",
                "                try:\n",
                "                    # Call Bedrock to generate context\n",
                "                    response = bedrock_service.converse(\n",
                "                        messages=usr_prompt, \n",
                "                        system_prompt=sys_prompt,\n",
                "                        temperature=temperature,\n",
                "                        top_p=top_p,\n",
                "                        max_tokens=4096\n",
                "                    )\n",
                "                    \n",
                "                    # Extract and format the context\n",
                "                    situated_context = response['output']['message']['content'][0]['text'].strip()\n",
                "                    chunk['content'] = f\"Context:\\n{situated_context}\\n\\nChunk:\\n{chunk_content}\"\n",
                "                    chunk['simulated'] = True\n",
                "                    \n",
                "                    # Track token usage\n",
                "                    if 'usage' in response:\n",
                "                        usage = response['usage']\n",
                "                        for key in ['inputTokens', 'outputTokens', 'totalTokens']:\n",
                "                            document['token_usage'][key] += usage.get(key, 0)\n",
                "                            \n",
                "                    print(f\"✅ Context generated for chunk [{doc_index}_{chunk['chunk_id']}]\")\n",
                "                \n",
                "                except Exception as e:\n",
                "                    print(f\"❌ Error generating context for chunk [{doc_index}_{chunk['chunk_id']}]: {str(e)}\")\n",
                "                    fail_count += 1\n",
                "                    \n",
                "                # Rate limiting to avoid API throttling\n",
                "                time.sleep(5)\n",
                "        \n",
                "        # Save the updated documents with context\n",
                "        print(f\"Saving documents with context to {chunked_file}...\")\n",
                "        with open(chunked_file, \"w\", encoding='utf-8') as f:\n",
                "            json.dump(documents, f, indent=4)\n",
                "            \n",
                "        print(\"✅ Context generation complete!\")\n",
                "        \n",
                "    except FileNotFoundError:\n",
                "        print(f\"❌ File not found: {chunked_file}\")\n",
                "        raise\n",
                "    except Exception as e:\n",
                "        print(f\"❌ Error during context generation: {str(e)}\")\n",
                "        raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2-2. Create OpenSearch Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure index name\n",
                "index_prefix = \"aws_\"\n",
                "index_name = (f\"{index_prefix}contextual_{document_name}\" if add_contextual and not document_name.startswith(\"contextual_\") else document_name) + f\"_{chunk_size}\"\n",
                "\n",
                "# Set to True to overwrite existing index, False to keep existing\n",
                "overwrite_index = True\n",
                "\n",
                "# Define OpenSearch index configuration\n",
                "opensearch_index_configuration = {\n",
                "    \"settings\": {\n",
                "        \"index.knn\": True,\n",
                "        \"index.knn.algo_param.ef_search\": 512\n",
                "    },\n",
                "    \"mappings\": {\n",
                "        \"properties\": {\n",
                "            \"metadata\": {\n",
                "                \"properties\": {\n",
                "                    \"source\": {\n",
                "                        \"type\": \"keyword\"\n",
                "                    },\n",
                "                    \"doc_id\": {\n",
                "                        \"type\": \"keyword\"\n",
                "                    },\n",
                "                    \"timestamp\": {\n",
                "                        \"type\": \"date\"\n",
                "                    }\n",
                "                }\n",
                "            },\n",
                "            \"content\": {\n",
                "                \"type\": \"text\",\n",
                "                \"analyzer\": \"standard\"\n",
                "            },\n",
                "            \"content_embedding\": {\n",
                "                \"type\": \"knn_vector\",\n",
                "                \"dimension\": 1024,  # Embedding dimension for Titan Embeddings\n",
                "                \"method\": {\n",
                "                    \"engine\": \"faiss\",\n",
                "                    \"name\": \"hnsw\",\n",
                "                    \"parameters\": {\n",
                "                        \"ef_construction\": 512,\n",
                "                        \"m\": 16\n",
                "                    },\n",
                "                    \"space_type\": \"l2\"\n",
                "                }\n",
                "            }\n",
                "        }\n",
                "    }\n",
                "}\n",
                "\n",
                "print(f\"Index name: {index_name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create or update the OpenSearch index\n",
                "try:\n",
                "    # Check if index needs to be deleted and recreated\n",
                "    if overwrite_index:\n",
                "        if opensearch_service.opensearch_client.indices.exists(index=index_name):\n",
                "            print(f\"Deleting existing index: {index_name}\")\n",
                "            opensearch_service.opensearch_client.indices.delete(index=index_name)\n",
                "        \n",
                "        print(f\"Creating new index: {index_name}\")\n",
                "        opensearch_service.opensearch_client.indices.create(index=index_name, body=opensearch_index_configuration)\n",
                "    else:\n",
                "        if not opensearch_service.opensearch_client.indices.exists(index=index_name):\n",
                "            print(f\"Index doesn't exist. Creating: {index_name}\")\n",
                "            opensearch_service.opensearch_client.indices.create(index=index_name, body=opensearch_index_configuration)\n",
                "        else:\n",
                "            print(f\"Index {index_name} already exists. Skipping creation.\")\n",
                "\n",
                "    # List all indices matching the prefix\n",
                "    index_pattern = f\"{index_prefix}*\" if index_prefix else \"*\"\n",
                "    indices = opensearch_service.opensearch_client.cat.indices(index=index_pattern, format=\"json\")\n",
                "    \n",
                "    # Extract and display index names\n",
                "    indices_name = [item['index'] for item in indices]\n",
                "    print(\"\\nAvailable indices:\")\n",
                "    for idx in indices_name:\n",
                "        print(f\" - {idx}\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"❌ Error configuring OpenSearch index: {str(e)}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2-3. Embed Documents and Store in OpenSearch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load documents to embed\n",
                "chunked_file = f\"output/{document_name}_{chunk_size}_{'situated' if add_contextual else ''}_chunks.json\"\n",
                "\n",
                "try:\n",
                "    print(f\"Loading documents for embedding from {chunked_file}...\")\n",
                "    with open(chunked_file, 'r', encoding='utf-8') as f:\n",
                "        documents = json.load(f)\n",
                "\n",
                "    print(f\"Generating embeddings and storing in OpenSearch index: {index_name}\")\n",
                "    \n",
                "    # Track embedded documents\n",
                "    embedded_documents = []\n",
                "    total_chunks = sum(len(doc.get('chunks', [])) for doc in documents)\n",
                "    \n",
                "    # Process each document\n",
                "    for document in tqdm(documents, desc=\"Documents\", total=len(documents)):\n",
                "        doc_id = document['doc_id']\n",
                "        \n",
                "        # Process each chunk in the document\n",
                "        for chunk in tqdm(document['chunks'], desc=\"Chunks\", leave=False):\n",
                "            # Get chunk content\n",
                "            context = chunk['content']\n",
                "            \n",
                "            # Generate embedding\n",
                "            try:\n",
                "                chunk_embedding = bedrock_service.embedding(text=context)\n",
                "                \n",
                "                if chunk_embedding:\n",
                "                    # Create document ID for OpenSearch\n",
                "                    chunk_id = chunk['chunk_id']\n",
                "                    _id = f\"{doc_id}_{chunk_id}\"\n",
                "                    \n",
                "                    # Create document for OpenSearch\n",
                "                    embedded_chunk = {\n",
                "                        \"metadata\": {\n",
                "                            \"source\": document_name, \n",
                "                            \"doc_id\": doc_id,\n",
                "                            \"chunk_id\": chunk_id,\n",
                "                            \"timestamp\": datetime.now().isoformat()\n",
                "                        },\n",
                "                        \"content\": context,\n",
                "                        \"content_embedding\": chunk_embedding\n",
                "                    }\n",
                "                    \n",
                "                    # Store in tracking list\n",
                "                    embedded_documents.append(_id)\n",
                "                    \n",
                "                    # Index in OpenSearch\n",
                "                    opensearch_service.opensearch_client.index(\n",
                "                        index=index_name,\n",
                "                        id=_id,  # Explicitly set document ID\n",
                "                        body=embedded_chunk\n",
                "                    )\n",
                "                else:\n",
                "                    print(f\"⚠️ Warning: Empty embedding for chunk {doc_id}_{chunk['chunk_id']}\")\n",
                "            \n",
                "            except Exception as e:\n",
                "                print(f\"❌ Error embedding chunk {doc_id}_{chunk['chunk_id']}: {str(e)}\")\n",
                "                \n",
                "            # Brief delay to prevent API throttling if needed\n",
                "            time.sleep(0.1)\n",
                "    \n",
                "    # Force index refresh to make documents searchable immediately\n",
                "    opensearch_service.opensearch_client.indices.refresh(index=index_name)\n",
                "    \n",
                "    print(f\"✅ Successfully embedded and stored {len(embedded_documents)} chunks in index '{index_name}'\")\n",
                "    \n",
                "except FileNotFoundError:\n",
                "    print(f\"❌ File not found: {chunked_file}\")\n",
                "except Exception as e:\n",
                "    print(f\"❌ Error during embedding process: {str(e)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2-4. Test Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test a simple query to verify the setup\n",
                "test_question = \"What is Amazon Bedrock?\"\n",
                "\n",
                "try:\n",
                "    print(f\"Testing search with question: '{test_question}'\")\n",
                "    \n",
                "    # Generate embedding for the question\n",
                "    question_embedding = bedrock_service.embedding(text=test_question)\n",
                "    \n",
                "    if not question_embedding:\n",
                "        raise ValueError(\"Failed to generate embedding for the question\")\n",
                "    \n",
                "    # Search using KNN\n",
                "    search_results = opensearch_service.search_by_knn(\n",
                "        question_embedding,  # Query embedding vector\n",
                "        index_name,          # Index to search\n",
                "        top_n=3,             # Number of results to return\n",
                "    )\n",
                "    \n",
                "    # Display results\n",
                "    print(\"\\n=== Search Results ===\")\n",
                "    if not search_results:\n",
                "        print(\"No results found\")\n",
                "    else:\n",
                "        for i, result in enumerate(search_results, 1):\n",
                "            print(f\"\\nResult {i} (Score: {result.get('score', 'N/A')}):\")\n",
                "            # Based on _format_search_result likely returning content directly\n",
                "            content = result.get('content', 'No content')\n",
                "            \n",
                "            # Truncate content if too long\n",
                "            if len(content) > 300:\n",
                "                content = content[:300] + \"...\"\n",
                "                \n",
                "            print(content)\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error testing query: {str(e)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. Proceed to the Question Generator notebook to generate test questions from your document\n",
                "2. Or go directly to the RAG notebook to start querying your indexed documents\n",
                "3. For any issues, check the configuration in the Configuration notebook"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "conda_python3",
            "language": "python",
            "name": "conda_python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

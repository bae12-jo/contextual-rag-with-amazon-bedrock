{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Contextual RAG Evaluation",
                "",
                "This notebook evaluates the effectiveness of our Contextual RAG system by:",
                "1. Loading questions generated in the previous notebook",
                "2. Querying the RAG system to get answers",
                "3. Comparing the generated answers to ground truth",
                "4. Scoring the correctness of responses",
                "5. Analyzing performance across different question types"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Prerequisites"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "# Install required packages\n",
                "%pip install ipywidgets python-dotenv tqdm pandas matplotlib\n",
                "\n",
                "# Import basic dependencies\n",
                "import os\n",
                "import sys\n",
                "import json\n",
                "import time\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Create output directory\n",
                "os.makedirs(\"output\", exist_ok=True)\n",
                "\n",
                "# Load environment variables from .env file\n",
                "try:\n",
                "    from dotenv import load_dotenv\n",
                "    load_dotenv('.env')\n",
                "    print(\"Environment variables loaded from .env file\")\n",
                "except ImportError:\n",
                "    print(\"python-dotenv not installed, skipping .env loading\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configure Evaluation Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set evaluation parameters\n",
                "qa_file = 'output/bedrock-ug_sample_questions.jsonl'\n",
                "document_name = 'bedrock-ug'\n",
                "chunk_size = 1000\n",
                "use_contextual = True\n",
                "num_questions = 10  # Set number of questions to evaluate (set to -1 for all)\n",
                "\n",
                "# Construct index name based on parameters\n",
                "index_prefix = \"aws_\"  # Match prefix used in file_processor notebook\n",
                "index_name = f\"{index_prefix}{'contextual_' if use_contextual else ''}{document_name}_{chunk_size}\"\n",
                "\n",
                "# Create output filename for results\n",
                "results_file = f\"output/eval_results_{document_name}_{'contextual' if use_contextual else 'standard'}.json\"\n",
                "\n",
                "print(f\"Evaluation configuration:\")\n",
                "print(f\"- Questions file: {qa_file}\")\n",
                "print(f\"- Document: {document_name}\")\n",
                "print(f\"- Chunk size: {chunk_size}\")\n",
                "print(f\"- Using contextual retrieval: {use_contextual}\")\n",
                "print(f\"- OpenSearch index: {index_name}\")\n",
                "print(f\"- Results will be saved to: {results_file}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Initialize Services"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    # Import required services and configuration\n",
                "    from config import Config\n",
                "    from libs.bedrock_service import BedrockService\n",
                "    from libs.contextual_rag_service import ContextualRAGService\n",
                "    from libs.opensearch_service import OpensearchService\n",
                "    from libs.reranker import RerankerService\n",
                "    \n",
                "    # Load configuration\n",
                "    config = Config.load()\n",
                "    \n",
                "    # Update config with environment variables if available\n",
                "    config.aws.region = os.environ.get(\"AWS_DEFAULT_REGION\", config.aws.region)\n",
                "    config.aws.profile = os.environ.get(\"AWS_PROFILE\", config.aws.profile)\n",
                "    config.bedrock.model_id = os.environ.get(\"BEDROCK_MODEL_ID\", config.bedrock.model_id)\n",
                "    config.bedrock.embed_model_id = os.environ.get(\"EMBED_MODEL_ID\", config.bedrock.embed_model_id)\n",
                "    config.opensearch.prefix = os.environ.get(\"OPENSEARCH_PREFIX\", config.opensearch.prefix)\n",
                "    config.opensearch.domain_name = os.environ.get(\"OPENSEARCH_DOMAIN_NAME\", config.opensearch.domain_name)\n",
                "    config.opensearch.user = os.environ.get(\"OPENSEARCH_USER\", config.opensearch.user)\n",
                "    config.opensearch.password = os.environ.get(\"OPENSEARCH_PASSWORD\", config.opensearch.password)\n",
                "    config.reranker.reranker_model_id = os.environ.get(\"RERANKER_MODEL_ID\", config.reranker.reranker_model_id)\n",
                "    \n",
                "    print(\"Configuration loaded successfully\")\n",
                "    print(f\"- LLM Model: {config.bedrock.model_id}\")\n",
                "    print(f\"- Embedding Model: {config.bedrock.embed_model_id}\")\n",
                "    print(f\"- OpenSearch Domain: {config.opensearch.domain_name}\")\n",
                "    print(f\"- Reranker Model: {config.reranker.reranker_model_id if config.reranker.reranker_model_id else 'Not configured'}\")\n",
                "    \n",
                "except ImportError as e:\n",
                "    print(f\"❌ Error importing required modules: {str(e)}\")\n",
                "    print(\"Make sure all dependencies are installed and the paths are correct\")\n",
                "    sys.path.append('..')\n",
                "    print(\"Added parent directory to Python path. Try running the cell again.\")\n",
                "    raise\n",
                "except Exception as e:\n",
                "    print(f\"❌ Error loading configuration: {str(e)}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    # Initialize all required services\n",
                "    print(\"Initializing services...\")\n",
                "    \n",
                "    # Initialize Bedrock service\n",
                "    bedrock_service = BedrockService(\n",
                "        config.aws.region, \n",
                "        config.aws.profile, \n",
                "        config.bedrock.retries, \n",
                "        config.bedrock.embed_model_id, \n",
                "        config.bedrock.model_id, \n",
                "        config.model.max_tokens, \n",
                "        config.model.temperature, \n",
                "        config.model.top_p\n",
                "    )\n",
                "    print(\"✅ Bedrock service initialized\")\n",
                "    \n",
                "    # Initialize OpenSearch service\n",
                "    opensearch_service = OpensearchService(\n",
                "        config.aws.region, \n",
                "        config.aws.profile, \n",
                "        config.opensearch.prefix, \n",
                "        config.opensearch.domain_name, \n",
                "        config.opensearch.document_name, \n",
                "        config.opensearch.user, \n",
                "        config.opensearch.password\n",
                "    )\n",
                "    print(\"✅ OpenSearch service initialized\")\n",
                "    \n",
                "    # Initialize Reranker service (if configured)\n",
                "    if config.reranker.reranker_model_id:\n",
                "        reranker_service = RerankerService(\n",
                "            config.reranker.aws_region, \n",
                "            config.reranker.aws_profile, \n",
                "            config.reranker.reranker_model_id, \n",
                "            config.bedrock.retries\n",
                "        )\n",
                "        print(\"✅ Reranker service initialized\")\n",
                "    else:\n",
                "        reranker_service = None\n",
                "        print(\"ℹ️ Reranker service not configured, will use default ranking\")\n",
                "    \n",
                "    # Initialize Contextual RAG service\n",
                "    rag_service = ContextualRAGService(\n",
                "        bedrock_service=bedrock_service, \n",
                "        opensearch_service=opensearch_service, \n",
                "        reranker_service=reranker_service\n",
                "    )\n",
                "    print(\"✅ Contextual RAG service initialized\")\n",
                "    \n",
                "    # Verify OpenSearch index exists\n",
                "    if not opensearch_service.opensearch_client.indices.exists(index=index_name):\n",
                "        print(f\"⚠️ Warning: OpenSearch index '{index_name}' does not exist!\")\n",
                "        print(\"Please make sure you've run the file_processor notebook and created the index.\")\n",
                "        \n",
                "        # List available indices for reference\n",
                "        indices = opensearch_service.opensearch_client.cat.indices(format=\"json\")\n",
                "        available_indices = [idx['index'] for idx in indices]\n",
                "        print(\"\\nAvailable indices:\")\n",
                "        for idx in available_indices:\n",
                "            print(f\"- {idx}\")\n",
                "            \n",
                "        # Ask for confirmation to continue or specify a different index\n",
                "        if input(f\"\\nDo you want to continue anyway? (y/n): \").lower() != 'y':\n",
                "            raise ValueError(f\"Index '{index_name}' not found. Please create it first.\")\n",
                "    else:\n",
                "        print(f\"✅ Index '{index_name}' exists\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error initializing services: {str(e)}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Define Evaluation Methods"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the system prompt for evaluation\n",
                "evaluate_system_prompt = \"\"\"\n",
                "Evaluate the correctness of the generation on a continuous scale from 0 to 1. A generation can be considered correct (Score: 1) if it includes all the key facts from the ground truth and if every fact presented in the generation is factually supported by the ground truth or common sense.\n",
                "\n",
                "Example:\n",
                "Query: Can eating carrots improve your vision?\n",
                "Answer: Yes, eating carrots significantly improves your vision, especially at night. This is why people who eat lots of carrots never need glasses. Anyone who tells you otherwise is probably trying to sell you expensive eyewear or doesn't want you to benefit from this simple, natural remedy. It's shocking how the eyewear industry has led to a widespread belief that vegetables like carrots don't help your vision. People are so gullible to fall for these money-making schemes.\n",
                "Ground truth: Well, yes and no. Carrots won't improve your visual acuity if you have less than perfect vision. A diet of carrots won't give a blind person 20/20 vision. But, the vitamins found in the vegetable can help promote overall eye health. Carrots contain beta-carotene, a substance that the body converts to vitamin A, an important nutrient for eye health. An extreme lack of vitamin A can cause blindness. Vitamin A can prevent the formation of cataracts and macular degeneration, the world's leading cause of blindness. However, if your vision problems aren't related to vitamin A, your vision won't change no matter how many carrots you eat.\n",
                "Score: 0.1\n",
                "Reasoning: While the generation mentions that carrots can improve vision, it fails to outline the reason for this phenomenon and the circumstances under which this is the case. The rest of the response contains misinformation and exaggerations regarding the benefits of eating carrots for vision improvement. It deviates significantly from the more accurate and nuanced explanation provided in the ground truth.\n",
                "\"\"\"\n",
                "\n",
                "# Define tool configuration for evaluation\n",
                "eval_tools = {\n",
                "    \"tools\": [\n",
                "        {\n",
                "            \"toolSpec\": {\n",
                "                \"name\": \"CorrectressGrader\",\n",
                "                \"description\": \"Evaluate the correctness of the answer on a continuous scale from 0 to 1, and reasoning why the score is. A generation can be considered correct (Score: 1) if it includes all the key facts from the ground truth and if every fact presented in the generation is factually supported by the ground truth.\",\n",
                "                \"inputSchema\": {\n",
                "                    \"json\": {\n",
                "                        \"type\": \"object\",\n",
                "                        \"properties\": {\n",
                "                            \"score\": {\n",
                "                                \"type\": \"number\",\n",
                "                                \"description\": \"The correctress score [0.0, 1.0]\"\n",
                "                            },\n",
                "                            \"reason\": {\n",
                "                                \"type\": \"string\",\n",
                "                                \"description\": \"The reason about the score\"\n",
                "                            }\n",
                "                        },\n",
                "                        \"required\": [\"score\", \"reason\"]\n",
                "                    }\n",
                "                }\n",
                "            }\n",
                "        }\n",
                "    ]\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Run Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    # Verify questions file exists\n",
                "    if not os.path.exists(qa_file):\n",
                "        raise FileNotFoundError(f\"Questions file '{qa_file}' not found! Please run the question generation notebook first.\")\n",
                "    \n",
                "    # Load questions from file\n",
                "    with open(qa_file, 'r') as f:\n",
                "        lines = f.readlines()\n",
                "    \n",
                "    total_questions = len(lines)\n",
                "    print(f\"Loaded {total_questions} questions from {qa_file}\")\n",
                "    \n",
                "    # Determine how many questions to evaluate\n",
                "    if num_questions <= 0 or num_questions > total_questions:\n",
                "        num_questions = total_questions\n",
                "        eval_lines = lines\n",
                "    else:\n",
                "        eval_lines = lines[:num_questions]\n",
                "    \n",
                "    print(f\"Will evaluate {len(eval_lines)} questions\")\n",
                "    \n",
                "    # Initialize results storage\n",
                "    results = []\n",
                "    token_usage_total = {\"inputTokens\": 0, \"outputTokens\": 0, \"totalTokens\": 0}\n",
                "    \n",
                "    # Process each question\n",
                "    print(\"\\nStarting evaluation...\")\n",
                "    for i, line in enumerate(tqdm(eval_lines, desc=\"Processing questions\")):\n",
                "        try:\n",
                "            # Parse question data\n",
                "            question_data = json.loads(line)\n",
                "            question = question_data['question']\n",
                "            ground_truth = question_data['ground_truth']\n",
                "            question_type = question_data.get('question_type', 'unknown')\n",
                "            \n",
                "            print(f\"\\n[{i+1}/{len(eval_lines)}] Evaluating {question_type} question: {question}\")\n",
                "            \n",
                "            # 1. Query the RAG system\n",
                "            start_time = time.time()\n",
                "            generated = rag_service.do(\n",
                "                question=question, \n",
                "                document_name=document_name, \n",
                "                index_name=index_name,\n",
                "                chunk_size=chunk_size, \n",
                "                use_hybrid=True, \n",
                "                use_contextual=use_contextual, \n",
                "                search_limit=5\n",
                "            )\n",
                "            rag_time = time.time() - start_time\n",
                "            \n",
                "            # Track token usage\n",
                "            if 'usage' in generated:\n",
                "                token_usage = generated['usage']\n",
                "                for key in token_usage:\n",
                "                    if key in token_usage_total:\n",
                "                        token_usage_total[key] += token_usage[key]\n",
                "            \n",
                "            # 2. Prepare evaluation prompt\n",
                "            evaluate_user_template = f\"\"\"\n",
                "            Query: {question}\n",
                "            Answer: {generated['answer']}\n",
                "            Ground Truth: {ground_truth}\n",
                "            \"\"\"\n",
                "            \n",
                "            user_prompt = [{\"role\": \"user\", \"content\": [{\"text\": evaluate_user_template}]}]\n",
                "            temperature = 0.0\n",
                "            top_p = 0.5\n",
                "            \n",
                "            # 3. Evaluate the answer\n",
                "            eval_start_time = time.time()\n",
                "            response = bedrock_service.converse_with_tools(\n",
                "                messages=user_prompt,\n",
                "                system_prompt=evaluate_system_prompt,\n",
                "                tools=eval_tools,\n",
                "                temperature=temperature,\n",
                "                top_p=top_p,\n",
                "                max_tokens=4096\n",
                "            )\n",
                "            eval_time = time.time() - eval_start_time\n",
                "            \n",
                "            # 4. Process evaluation results\n",
                "            stop_reason = response['stopReason']\n",
                "            \n",
                "            if stop_reason == 'tool_use':\n",
                "                tool_requests = response['output']['message']['content']\n",
                "                \n",
                "                for tool_request in [x for x in tool_requests if 'toolUse' in x]:\n",
                "                    if tool_request['toolUse']['name'] == 'CorrectressGrader':\n",
                "                        eval_result = tool_request['toolUse']['input']\n",
                "                        \n",
                "                        # Create result record\n",
                "                        result = {\n",
                "                            \"question\": question,\n",
                "                            \"question_type\": question_type,\n",
                "                            \"generated_answer\": generated['answer'],\n",
                "                            \"ground_truth\": ground_truth,\n",
                "                            \"score\": eval_result['score'],\n",
                "                            \"reason\": eval_result['reason'],\n",
                "                            \"rag_time_seconds\": round(rag_time, 2),\n",
                "                            \"eval_time_seconds\": round(eval_time, 2),\n",
                "                            \"token_usage\": token_usage if 'usage' in generated else None\n",
                "                        }\n",
                "                        \n",
                "                        results.append(result)\n",
                "                        print(f\"Score: {eval_result['score']:.2f}\")\n",
                "                        print(f\"Reason: {eval_result['reason'][:150]}...\")\n",
                "            else:\n",
                "                print(f\"⚠️ Warning: Evaluation stopped with reason '{stop_reason}' instead of 'tool_use'\")\n",
                "                \n",
                "            # Save results after each question in case of interruption\n",
                "            with open(results_file, 'w') as f:\n",
                "                json.dump(results, f, indent=2)\n",
                "                \n",
                "        except Exception as e:\n",
                "            print(f\"❌ Error processing question {i+1}: {str(e)}\")\n",
                "            continue\n",
                "    \n",
                "    print(f\"\\n✅ Evaluation complete! Processed {len(results)} questions.\")\n",
                "    print(f\"Results saved to {results_file}\")\n",
                "    \n",
                "    # Print token usage summary\n",
                "    print(f\"\\nToken Usage Summary:\")\n",
                "    print(f\"- Input tokens: {token_usage_total['inputTokens']}\")\n",
                "    print(f\"- Output tokens: {token_usage_total['outputTokens']}\")\n",
                "    print(f\"- Total tokens: {token_usage_total['totalTokens']}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error during evaluation: {str(e)}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Analyze Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    if len(results) == 0:\n",
                "        if os.path.exists(results_file):\n",
                "            with open(results_file, 'r') as f:\n",
                "                results = json.load(f)\n",
                "            print(f\"Loaded {len(results)} results from {results_file}\")\n",
                "        else:\n",
                "            raise ValueError(\"No results available for analysis\")\n",
                "    \n",
                "    # Convert to DataFrame for analysis\n",
                "    df = pd.DataFrame(results)\n",
                "    \n",
                "    # Print summary statistics\n",
                "    print(\"\\n=== Overall Performance ===\")\n",
                "    print(f\"Average score: {df['score'].mean():.4f}\")\n",
                "    print(f\"Median score: {df['score'].median():.4f}\")\n",
                "    print(f\"Min score: {df['score'].min():.4f}\")\n",
                "    print(f\"Max score: {df['score'].max():.4f}\")\n",
                "    \n",
                "    # Break down by question type\n",
                "    if 'question_type' in df.columns:\n",
                "        print(\"\\n=== Performance by Question Type ===\")\n",
                "        question_types = df['question_type'].unique()\n",
                "        \n",
                "        for q_type in question_types:\n",
                "            type_df = df[df['question_type'] == q_type]\n",
                "            print(f\"\\n{q_type.capitalize()} Questions ({len(type_df)} total):\")\n",
                "            print(f\"- Average score: {type_df['score'].mean():.4f}\")\n",
                "            print(f\"- Median score: {type_df['score'].median():.4f}\")\n",
                "            print(f\"- Min score: {type_df['score'].min():.4f}\")\n",
                "            print(f\"- Max score: {type_df['score'].max():.4f}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error analyzing results: {str(e)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize results\n",
                "try:\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    \n",
                "    # Plot scores histogram\n",
                "    plt.subplot(1, 2, 1)\n",
                "    plt.hist(df['score'], bins=10, alpha=0.7, color='blue')\n",
                "    plt.title('Distribution of Scores')\n",
                "    plt.xlabel('Score')\n",
                "    plt.ylabel('Count')\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    \n",
                "    # Plot scores by question type\n",
                "    if 'question_type' in df.columns:\n",
                "        plt.subplot(1, 2, 2)\n",
                "        df.boxplot(column='score', by='question_type')\n",
                "        plt.title('Scores by Question Type')\n",
                "        plt.suptitle('')  # Remove pandas-generated title\n",
                "        plt.grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error creating visualizations: {str(e)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. View Individual Examples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display high and low performing examples\n",
                "try:\n",
                "    # Find highest and lowest scoring examples\n",
                "    high_score = df.loc[df['score'].idxmax()]\n",
                "    low_score = df.loc[df['score'].idxmin()]\n",
                "    \n",
                "    print(\"=== Highest Scoring Example ===\")\n",
                "    print(f\"Score: {high_score['score']:.2f}\")\n",
                "    print(f\"Question ({high_score['question_type']}): {high_score['question']}\")\n",
                "    print(f\"\\nGenerated Answer:\\n{high_score['generated_answer']}\")\n",
                "    print(f\"\\nGround Truth:\\n{high_score['ground_truth']}\")\n",
                "    print(f\"\\nReason for high score:\\n{high_score['reason']}\")\n",
                "    \n",
                "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
                "    \n",
                "    print(\"=== Lowest Scoring Example ===\")\n",
                "    print(f\"Score: {low_score['score']:.2f}\")\n",
                "    print(f\"Question ({low_score['question_type']}): {low_score['question']}\")\n",
                "    print(f\"\\nGenerated Answer:\\n{low_score['generated_answer']}\")\n",
                "    print(f\"\\nGround Truth:\\n{low_score['ground_truth']}\")\n",
                "    print(f\"\\nReason for low score:\\n{low_score['reason']}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error displaying examples: {str(e)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Conclusion",
                "",
                "This evaluation has measured the performance of our Contextual RAG system against a variety of question types. The scores reflect how well the system's answers align with the ground truth.",
                "",
                "### Next Steps",
                "",
                "1. **Improve performance**: Analyze low-scoring responses to identify patterns of failure and opportunities for improvement",
                "2. **Compare configurations**: Run this evaluation with different settings (standard vs. contextual, different chunk sizes)",
                "3. **Expand test set**: Generate more questions to get a more comprehensive evaluation",
                "4. **Fine-tune parameters**: Adjust retrieval parameters, number of chunks returned, or LLM prompting",
                "5. **Benchmark**: Compare performance against other RAG implementations or baseline approaches"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Optional: Try a Custom Question"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the RAG system with your own question\n",
                "custom_question = \"What are the key features of Amazon Bedrock?\"  # Change this to your own question\n",
                "\n",
                "try:\n",
                "    print(f\"Question: {custom_question}\\n\")\n",
                "    \n",
                "    # Query the RAG system\n",
                "    start_time = time.time()\n",
                "    response = rag_service.do(\n",
                "        question=custom_question,\n",
                "        document_name=document_name,\n",
                "        index_name=index_name,\n",
                "        chunk_size=chunk_size,\n",
                "        use_hybrid=True,\n",
                "        use_contextual=use_contextual,\n",
                "        search_limit=5\n",
                "    )\n",
                "    elapsed_time = time.time() - start_time\n",
                "    \n",
                "    # Print the response\n",
                "    print(f\"Answer:\\n{response['answer']}\\n\")\n",
                "    \n",
                "    # Print metadata\n",
                "    print(f\"Response generated in {elapsed_time:.2f} seconds\")\n",
                "    if 'usage' in response:\n",
                "        print(f\"Token usage: {response['usage']['totalTokens']} tokens\")\n",
                "        \n",
                "    # Print retrieved contexts (optional)\n",
                "    if 'contexts' in response and input(\"\\nShow retrieved contexts? (y/n): \").lower() == 'y':\n",
                "        print(\"\\n=== Retrieved Contexts ===\")\n",
                "        for i, ctx in enumerate(response['contexts'], 1):\n",
                "            print(f\"\\nContext {i}:\")\n",
                "            print(f\"{ctx[:300]}...\" if len(ctx) > 300 else ctx)\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error processing custom question: {str(e)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Final Results",
                "",
                "The following cell merges the analysis with the results and saves everything to a comprehensive report file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create and save a comprehensive report\n",
                "try:\n",
                "    # Calculate summary statistics\n",
                "    summary = {\n",
                "        \"evaluation_config\": {\n",
                "            \"document_name\": document_name,\n",
                "            \"index_name\": index_name,\n",
                "            \"chunk_size\": chunk_size,\n",
                "            \"use_contextual\": use_contextual,\n",
                "            \"questions_evaluated\": len(results),\n",
                "            \"evaluation_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
                "            \"llm_model\": config.bedrock.model_id,\n",
                "            \"embedding_model\": config.bedrock.embed_model_id,\n",
                "            \"reranker_model\": config.reranker.reranker_model_id if config.reranker.reranker_model_id else \"Not used\"\n",
                "        },\n",
                "        \"overall_performance\": {\n",
                "            \"mean_score\": df['score'].mean(),\n",
                "            \"median_score\": df['score'].median(),\n",
                "            \"min_score\": df['score'].min(),\n",
                "            \"max_score\": df['score'].max(),\n",
                "            \"std_dev\": df['score'].std()\n",
                "        },\n",
                "        \"token_usage\": token_usage_total,\n",
                "        \"question_type_breakdown\": {},\n",
                "        \"detailed_results\": results\n",
                "    }\n",
                "    \n",
                "    # Add question type breakdown if available\n",
                "    if 'question_type' in df.columns:\n",
                "        question_types = df['question_type'].unique()\n",
                "        for q_type in question_types:\n",
                "            type_df = df[df['question_type'] == q_type]\n",
                "            summary[\"question_type_breakdown\"][q_type] = {\n",
                "                \"count\": len(type_df),\n",
                "                \"mean_score\": type_df['score'].mean(),\n",
                "                \"median_score\": type_df['score'].median(),\n",
                "                \"min_score\": type_df['score'].min(),\n",
                "                \"max_score\": type_df['score'].max()\n",
                "            }\n",
                "    \n",
                "    # Save the comprehensive report\n",
                "    report_file = f\"output/evaluation_report_{document_name}_{'contextual' if use_contextual else 'standard'}_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
                "    with open(report_file, 'w') as f:\n",
                "        json.dump(summary, f, indent=2)\n",
                "    \n",
                "    print(f\"✅ Comprehensive evaluation report saved to {report_file}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error creating final report: {str(e)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "conda_python3",
            "language": "python",
            "name": "conda_python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
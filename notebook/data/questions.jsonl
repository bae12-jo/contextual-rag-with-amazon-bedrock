{"question": "How does Amazon Bedrock handle the lifecycle of primary and secondary grants when using customer managed keys for custom imported models, and what specific KMS operations are required in the key policy?", "ground_truth": "Amazon Bedrock handles primary and secondary grants differently for customer managed keys used with custom imported models. Primary grants remain active as long as the associated custom models are in use and are automatically retired when the custom imported model is deleted. Secondary grants are short-lived and automatically retired once the operation Amazon Bedrock performs on behalf of the customer is completed, such as after a custom model import job.\n\nTo use a customer managed key for encrypting custom imported models, the key policy must include specific KMS operations:\n\n1. kms:CreateGrant: Allows Amazon Bedrock to create grants for accessing the KMS key.\n2. kms:DescribeKey: Enables Amazon Bedrock to validate the key by providing its details.\n3. kms:GenerateDataKey: Allows Amazon Bedrock to generate and store ciphertext alongside the imported custom model for additional user access validation.\n4. kms:Decrypt: Permits decryption of the stored ciphertext to validate proper access to the KMS key.\n\nThese operations must be included in the key policy attached to the customer managed key to ensure proper functionality and security when encrypting and managing custom imported models in Amazon Bedrock.", "question_type": "complex", "context": "pan and remain active as long as the associated custom models are still in use. When a custom imported model is deleted, the corresponding primary grant is automatically retired. \u2022 Secondary grants are short-lived. They are automatically retired as soon as the operation that Amazon Bedrock performs on behalf of the customers is completed. For example, once a custom model import job is finished, the secondary grant that allowed Amazon Bedrock to encrypt the custom imported model will be retired immediately. Data encryption 1106 Amazon Bedrock User Guide Using customer managed key (CMK) If you are planning to use customer managed key to encrypt your custom imported model, complete the following steps: 1. Create a customer managed key with the AWS Key Management Service. 2. Attach a resource-based policy with permissions for the specified-roles to create and use custom imported models. Create a customer managed key First ensure that you have CreateKey permissions. Then follow the steps at creating keys to create a customer managed keys either in the AWS KMS console or the CreateKey API operation. omer managed keys either in the AWS KMS console or the CreateKey API operation. Make sure to create a symmetric encryption key. Creation of the key returns an Arn for the key that you can use as the importedModelKmsKeyId when importing a custom model with custom model import. Create a key policy and attach it to the customer managed key Key policies are resource-based policy that you attach to your customer managed key to control access to it. Every customer managed key must have exactly one key policy, which contains statements that determine who can use the key and how they can use it. You can specify a key policy when you create your customer managed key. You can modify the key policy at any time, but there might be a brief delay before the change becomes available throughout AWS KMS. For more information, see Managing access to customer managed keys in the AWS Key Management Service Developer Guide. Encrypt a resulting imported custom model To use your customer managed key to encrypt an imported custom model, you must include the following AWS KMS operations in the key policy: \u2022 kms:CreateGrant \u2013 creates a grant for a customer managed key by allowing the Amazon Bedrock service principal access to the specified KMS key through grant operations. ock service principal access to the specified KMS key through grant operations. For more information about grants, see Grants in AWS KMS in the AWS Key Management Service Developer Guide. Data encryption 1107 Amazon Bedrock User Guide Note Amazon Bedrock also sets up a retiring principal and automatically retires the grant after it is no longer required. \u2022 kms:DescribeKey \u2013 provides the customer managed key details to allow Amazon Bedrock to validate the key. \u2022 kms:GenerateDataKey \u2013 Provides the customer managed key details to allow Amazon Bedrock to validate user access. Amazon Bedrock stores generated ciphertext alongside the imported custom model to be used as an additional validation check against imported custom model users \u2022 kms:Decrypt \u2013 Decrypts the stored ciphertext to validate that the role has proper access to the KMS key that encrypts the imported custom model. The following is an example policy that you can attach to a key for a role that you'll use to encrypt an imported custom model: { \"Version\": \"2012-10-17\", \"Id\": \"KMS key policy for a key to encrypt an imported custom model\", \"Statement\": [ { \"Sid\": \"Permissions for model import API invocation role\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::$"}
{"question": "What is the maximum image size allowed for Anthropic Claude models?", "ground_truth": "The maximum image size allowed for Anthropic Claude models is 3.75MB, with a maximum height and width of 8000 pixels.", "question_type": "simple", "context": "erent Anthropic Claude models have different maximum values for this parameter. For more information, see Model comparison. \u2022 messages \u2013 (Required) The input messages. \u2022 role \u2013 The role of the conversation turn. Valid values are user and assistant. Anthropic Claude models 144 Amazon Bedrock User Guide \u2022 content \u2013 (required) The content of the conversation turn. \u2022 type \u2013 (required) The type of the content. Valid values are image and text. If you specify image, you must also specify the image source in the following format source \u2013 (required) The content of the conversation turn. \u2022 type \u2013 (required) The encoding type for the image. You can specify base64. \u2022 media_type \u2013 (required) The type of the image. You can specify the following image formats. \u2022 image/jpeg \u2022 image/png \u2022 image/webp \u2022 image/gif \u2022 data \u2013 (required) The base64 encoded image bytes for the image. The maximum image size is 3.75MB. The maximum height and width of an image is 8000 pixels. If you specify text, you must also specify the prompt in text. is 8000 pixels. If you specify text, you must also specify the prompt in text. The following are optional parameters. \u2022 system \u2013 (Optional) The system prompt for the request. A system prompt is a way of providing context and instructions to Anthropic Claude, such as specifying a particular goal or role. For more information, see System prompts in the Anthropic documentation. Note You can use system prompts with Anthropic Claude version 2.1 or higher. \u2022 stop_sequences \u2013 (Optional) Custom text sequences that cause the model to stop generating. Anthropic Claude models normally stop when they have naturally completed their turn, in this case the value of the stop_reason response field is end_turn. If you want the model to stop generating when it encounters custom strings of text, you can use the stop_sequences parameter. If the model encounters one of the custom text strings, the value of the Anthropic Claude models 145 Amazon Bedrock User Guide stop_reason response field is stop_sequence and the value of stop_sequence contains the matched stop sequence. top_sequence and the value of stop_sequence contains the matched stop sequence. The maximum number of entries is 8191. \u2022 temperature \u2013 (Optional) The amount of randomness injected into the response. Default Minimum Maximum 1 0 1 \u2022 top_p \u2013 (Optional) Use nucleus sampling. In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both. Default Minimum Maximum 0.999 0 1 \u2022 top_k \u2013 (Optional) Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses. Default Minimum Maximum Disabled by default 0 500 \u2022 tools \u2013 (Optional) Definitions of tools that the model may use. Note Requires an Anthropic Claude 3 model. If you include tools in your request, the model may return tool_use content blocks that represent the model's use of those tools."}
{"question": "How does the Barber paradox demonstrate the limitations of self-referential logical statements, and what implications does this have for formal systems in mathematics and logic?", "ground_truth": "The Barber paradox demonstrates the limitations of self-referential logical statements by creating an unresolvable contradiction. In this paradox, a barber is defined as \"one who shaves all those, and those only, who do not shave themselves.\" This leads to a logical impossibility when we consider whether the barber shaves himself.\n\nIf the barber shaves himself, he violates the condition of shaving only those who don't shave themselves. If he doesn't shave himself, he falls into the category of people he must shave. This circular logic creates a paradox that cannot be resolved within the confines of the statement.\n\nThis paradox has significant implications for formal systems in mathematics and logic:\n\n1. Incompleteness: It relates to G\u00f6del's Incompleteness Theorems, which show that in any sufficiently complex formal system, there are statements that can neither be proved nor disproved within that system.\n\n2. Limitations of set theory: The paradox is similar to Russell's paradox in set theory, which led to the development of more sophisticated axiomatic systems to avoid such contradictions.\n\n3. Self-reference problems: It highlights the challenges of self-referential statements in logical systems, leading to the development of type theory and other frameworks to manage these issues.\n\n4. Consistency vs. completeness: The paradox underscores the tension between consistency and completeness in formal systems, showing that systems strong enough to describe themselves may inevitably lead to contradictions.\n\n5. Metamathematics: It contributes to the field of metamathematics, which studies the properties and limitations of mathematical and logical systems themselves.\n\nIn essence, the Barber paradox serves as a simple yet powerful example of how self-referential statements can lead to logical breakdowns, prompting mathematicians and logicians to develop more robust and nuanced approaches to formal reasoning and set theory.", "question_type": "complex", "context": "nking, we can ask the model to make logical deductions and explain its answers. Prompt template for Titan and AI21 Labs User prompt: Question: Which word is Jurassic: \"\"\"Question: {{questio the odd one out? A. accomplished n}} Please output the answer and then B. good C. horrible D. outstanding explain your answer:\"\"\" Please output the answer and then explain your answer: Output: The odd one out is C. horrible. The other three words describe positive qualities or achievements, whereas horrible is an negative adjective. (Source of prompt: AWS, model used: Amazon Titan Text) Here is another example using the Anthropic Claude model: Prompt template for Anthropic User prompt Human: The barber is the Claude: \"\"\" Human: {{question}} \"one who shaves all those, and those Please provide the answer and show the only, who do not shave themselves\". reasoning. Assistant:\"\"\" Does the barber shave himself? Why is this a paradox? Please provide the answer and show the reasoning. Assistant: Output: This is an example of a paradox known as the Barber paradox. Assistant: Output: This is an example of a paradox known as the Barber paradox. Here is an explanation: - The statement defines a barber who shaves all and only those people who do not shave themselves. - So if the barber shaves himself, he must shave all those who do not shave themselve s, which includes himself. But the Reasoning/logical thinking 371 Amazon Bedrock User Guide barber cannot shave himself as per the initial statement. - If the barber does not shave himself, then he would fit into the group of people who do not shave themselves, and so he must shave himself, according to the statement. But again this contradicts the initial statement. - So the barber cannot shave himself and also cannot not shave himself. This is self-contradictory based on the initial statement about the barber. - Therefore, the statement leads to a paradox where both possibili ties (the barber shaving himself or not shaving himself) result in a contradiction. In summary, the paradox arises because the definition of the barber's behavior is self-cont radictory when applied to the barber himself. he barber's behavior is self-cont radictory when applied to the barber himself. This makes it impossible to determine if the barber shaves himself or not based on the given statement alone. (Source of prompt: https://en.wikipedia.org/wiki/Barber_paradox, model used: Anthropic Claude) Entity extraction For entity extraction from a provided input question. Extract entities from generated text and place them in XML tags for further processing. Prompt template for Titan \"\"\"You are an expert entity extractor from provided input question. You are responsible for extracting following entities: {{ list of entities}} Entity extraction 372 Amazon Bedrock User Guide Please follow below instructions while extracting the entity A, and reply in <entityA> </entityA> XML Tags: {{ entity A extraction instructi ons}} Please follow below instructions while extracting the entity B, and reply in <entityB> </entityB> XML Tags: {{ entity B extraction instructi ons}} Below are some examples: {{ some few shot examples showing model extracting entities from give input }} (Source of prompt: AWS, model used: Amazon Titan Text G1- Premier) Example: User: You are an expert entity extractor who extracts entities from provided input question."}
{"question": "What are the two types of model evaluation jobs in Amazon Bedrock?", "ground_truth": "The two types of model evaluation jobs in Amazon Bedrock are automatic model evaluation jobs and model evaluation jobs that use human workers.", "question_type": "simple", "context": "undation models, and for models that you have purchased Provisioned Throughput. Topics \u2022 Prerequisites for model evaluations in Amazon Bedrock \u2022 Choose the best performing model using Amazon Bedrock evaluations \u2022 Manage a work team for human evaluations of models in Amazon Bedrock Prerequisites for model evaluations in Amazon Bedrock You can create an automatic evaluation job for a model in Amazon Bedrock, and use human workers to assist in the evaluation. There are prerequisites for creating an evaluation job, such as access to the resources required for evaluation. To create an automatic model evaluation job, you must have access to at least one model. Model evaluation jobs support using foundation models in Amazon Bedrock. To learn more about which models are supported in model evaluations, see Model support by feature. To gain access to models in Amazon Bedrock, see Access Amazon Bedrock foundation models. To create a model evaluation job, you must include the required access permissions. reate a model evaluation job, you must include the required access permissions. Prerequisites 484 Amazon Bedrock User Guide Choose the best performing model using Amazon Bedrock evaluations Amazon Bedrock supports model evaluation jobs. The results of a model evaluation job allow you to compare model or inference profile outputs, and then choose the model best suited for your downstream generative AI applications. Model evaluation jobs support common use cases for large language models (LLMs) such as text generation, text classification, question answering, and text summarization. To evaluate a model's performance for automatic model evaluation jobs, you can use either built-in prompt datasets or your own prompt datasets. For model evaluation jobs that use human workers, you must use your own dataset. You can choose to create either an automatic model evaluation job or a model evaluation job that uses a human workforce. Overview: Automatic model evaluation jobs Automatic model evaluation jobs allow you to quickly evaluate a model's ability to perform a task. luation jobs allow you to quickly evaluate a model's ability to perform a task. You can either provide your own custom prompt dataset that you've tailored to a specific use case, or you can use an available built-in dataset. Overview: Model evaluation jobs that use human workers Model evaluation jobs that use human workers allow you to bring human input to the model evaluation process. They can be employees of your company or a group of subject-matter experts from your industry. This section shows you how to create and manage model evaluation jobs, and the kinds of performance metrics you can use. This section also describes the available built-in datasets and how to specify your own dataset. Topics \u2022 Create a model evaluation job in Amazon Bedrock \u2022 Stop a model evaluation job in Amazon Bedrock \u2022 List model evaluation jobs in Amazon Bedrock \u2022 Model evaluation task types in Amazon Bedrock Model evaluation jobs 485 Amazon Bedrock User Guide \u2022 Use prompt datasets for model evaluation in Amazon Bedrock \u2022 Review model evaluation job reports and metrics in Amazon Bedrock \u2022 Required permissions and IAM service roles for model evaluation in Amazon Bedrock \u2022 Delete a model evaluation job in Amazon Bedrock Create a model evaluation job in Amazon Bedrock You can create a model evaluation job using the AWS Management Console, AWS CLI, or a supported AWS SDK."}
{"question": "How does Amazon Bedrock assess semantic robustness in automated model evaluation, and why might this be important for real-world applications of language models?", "ground_truth": "Amazon Bedrock assesses semantic robustness by perturbing prompts in several ways: converting text to lowercase, introducing keyboard typos, changing numbers to words, randomly altering uppercase letters, and adding or deleting whitespace. This approach is crucial for real-world applications as it simulates various input scenarios that models might encounter, such as user typos or inconsistent formatting. By evaluating model performance under these conditions, developers can ensure their models are robust and capable of handling diverse, imperfect inputs, thus improving overall reliability and user experience in practical deployments.", "question_type": "complex", "context": "nt the model to use during inference\",\"category\":\"(Optional) Specify an optional category\",\"referenceResponse\":\"(Optional) Specify a ground truth response.\"} {\"prompt\":\"Provide the prompt you want the model to use during inference\",\"category\":\"(Optional) Specify an optional category\",\"referenceResponse\":\"(Optional) Specify a ground truth response.\"} {\"prompt\":\"Provide the prompt you want the model to use during inference\",\"category\":\"(Optional) Specify an optional category\",\"referenceResponse\":\"(Optional) Specify a ground truth response.\"} {\"prompt\":\"Provide the prompt you want the model to use during inference\",\"category\":\"(Optional) Specify an optional category\",\"referenceResponse\":\"(Optional) Specify a ground truth response.\"} {\"prompt\":\"Provide the prompt you want the model to use during inference\",\"category\":\"(Optional) Specify an optional category\",\"referenceResponse\":\"(Optional) Specify a ground truth response.\"} {\"prompt\":\"Provide the prompt you want the model to use during inference\",\"category\":\"(Optional) Specify an optional category\",\"referenceResponse\":\"(Optional) Specify a ground truth response.\"} Prompt datasets for model evaluation 511 Amazon Bedrock User Guide The following example is a single entry expanded for clarity { \"prompt\": \"What is high intensity interval training?\", \"category\": \"Fitness\", \"referenceResponse\": \"High-Intensity Interval Training (HIIT) is a cardiovascular exercise approach that involves short, intense bursts of exercise followed by brief recovery or rest periods.\" } Review model evaluation job reports and metrics in Amazon Bedrock The results of a model evaluation job are presented in a report, and include key metrics that can help you assess the model performance and effectiveness. e key metrics that can help you assess the model performance and effectiveness. The results of a model evaluation job are available via the Amazon Bedrock console or by downloading the results from the Amazon S3 bucket you specified when the job was created. Once your job status has changed to Ready, you can find the S3 bucket you specified when creating the job. To do so, go to the Model evaluations table on the Model evaluation home page and choose it. Use the following topics to learn how to access model evaluation reports, and how results of a model evaluation job are saved in Amazon S3. Topics \u2022 Review metrics for an automated model evaluation job in Amazon Bedrock (console) \u2022 Review a human-based model evaluation job in Amazon Bedrock (console) \u2022 Understand how the results of your model evaluation job are saved in Amazon S3 Review metrics for an automated model evaluation job in Amazon Bedrock (console) You can review the metrics presented in a report for an automatic model evaluation job using the Amazon Bedrock console. report for an automatic model evaluation job using the Amazon Bedrock console. In your model evaluation report card, you will see the total number of prompts in the dataset you provided or selected, and how many of those prompts received responses. If the number of responses is less than the number of input prompts, make sure to check the data output file in your Reports and metrics for model evaluation 512 Amazon Bedrock User Guide Amazon S3 bucket. It is possible that the prompt caused an error with the model and there was no inference retrieved. Only responses from the model will be used in metric calculations. Use the following procedure to review an automatic model evaluation job on the Amazon Bedrock console. 1. Open the Amazon Bedrock console. 2. From the navigation pane, choose Model evaluation. 3. Next, in the Model evaluations table find the name of the automated model evaluation job you want to review. Then, choose it. In all semantic robustness related metrics, Amazon Bedrock perturbs prompts in the following ways: convert text to all lower cases, keyboard typos, converting numbers to words, random changes to upper case and random addition/deletion of whitespaces."}
{"question": "What is the purpose of the tagSuffix in Amazon Bedrock guardrails?", "ground_truth": "The tagSuffix is a dynamic value used in input tagging to help mitigate potential prompt injection attacks by making the tag structure unpredictable. It's recommended to use a new, random string for each request.", "question_type": "simple", "context": "the XML tag that is a combination of a reserved prefix and a custom tagSuffix. For example: { \"text\": \"\"\" You are a helpful assistant. Here is some information about my account: - There are 10,543 objects in an S3 bucket. - There are no active EC2 instances. Based on the above, answer the following question: Question: <amazon-bedrock-guardrails-guardContent_xyz> How many objects do I have in my S3 bucket? </amazon-bedrock-guardrails-guardContent_xyz> ... Here are other user queries: <amazon-bedrock-guardrails-guardContent_xyz> How do I download files from my S3 bucket? </amazon-bedrock-guardrails-guardContent_xyz> \"\"\", \"amazon-bedrock-guardrailConfig\": { \"tagSuffix\": \"xyz\" } } Use the inference operations 460 Amazon Bedrock User Guide In the preceding example, the content `How many objects do I have in my S3 bucket?` and \"\"How do I download files from my S3 bucket?\" is tagged for guardrails processing using the tag <amazon- bedrock-guardrails-guardContent_xyz>. Note that the prefix amazon-bedrock- guardrails-guardContent is reserved by guardrails. t the prefix amazon-bedrock- guardrails-guardContent is reserved by guardrails. Tag Suffix The tag suffix (xyz in the preceding example) is a dynamic value that you must provide in the tagSuffix field in amazon-bedrock-guardrailConfig to use input tagging. It is recommended to use a new, random string as the tagSuffix for every request. This helps mitigate potential prompt injection attacks by making the tag structure unpredictable. A static tag can result in a malicious user closing the XML tag and appending malicious content after the tag closure, resulting in an injection attack. You are limited to alphanumeric characters with a length between 1 and 20 characters, inclusive. With the example suffix xyz, you must enclose all the content to be guarded using the XML tags with your suffix: <amazon-bedrock- guardrails-guardContent_xyz>. and your content </amazon-bedrock-guardrails- guardContent_xyz>. We recommend to use a dynamic UUID for each request as a tag suffix Multiple tags You can use the same tag structure multiple times in the input text to mark different parts of the content for guardrails processing. he input text to mark different parts of the content for guardrails processing. Nesting of tags is not allowed. Untagged Content Any content outside of the input tags will not be processed by guardrails. This allows you to include instructions, sample conversations, knowledge bases, or other content that you deem safe and do not want to be processed by guardrails. If there are no tags in the input prompt, the complete prompt will be processed by guardrails. The only exception is Prompt attacks filters which require input tags to be present. You can try out input tagging in the test pane for your guardrail by following these steps: 1. Navigating to the test pane for your guardrail (this method isn't supported for the text or chat playgrounds, only the guardrails test pane). 2. Use the default playground input tag suffix playground. VIOLENT STATEMENT: I think I could fight a grizzly bear. Use the inference operations 461 Amazon Bedrock User Guide <amazon-bedrock-guardrails-guardContent_playground BENIGN INPUT: How's the weather?"}
{"question": "How does Amazon Bedrock handle encryption for different aspects of model customization, and what options are available for encrypting custom imported models?", "ground_truth": "Amazon Bedrock handles encryption differently for various aspects of model customization. For input files in model customization jobs, data is stored in the user's Amazon S3 bucket. Output metrics files are stored in the user-specified S3 bucket, while custom model artifacts are stored in an AWS-controlled S3 bucket. The encryption of output files depends on the S3 bucket's configuration, using either SSE-S3 or SSE-KMS encryption. For custom imported models (currently in preview), Amazon Bedrock provides two encryption options: by default, it uses AWS owned keys, but users can also choose their own encryption method. The system uses AWS KMS for operations related to these processes, as evidenced by the CreateGrant CloudTrail event example, which shows Bedrock requesting permissions for Decrypt, CreateGrant, GenerateDataKey, and DescribeKey operations on a KMS key.", "question_type": "complex", "context": "KMS. Data encryption 1102 Amazon Bedrock User Guide The following is an example AWS CloudTrail event for CreateGrant to monitor KMS operations called by Amazon Bedrock to create a primary grant: { \"eventVersion\": \"1.09\", \"userIdentity\": { \"type\": \"AssumedRole\", \"principalId\": \"AROAIGDTESTANDEXAMPLE:SampleUser01\", \"arn\": \"arn:aws:sts::111122223333:assumed-role/RoleForModelCopy/SampleUser01\", \"accountId\": \"111122223333\", \"accessKeyId\": \"EXAMPLE\", \"sessionContext\": { \"sessionIssuer\": { \"type\": \"Role\", \"principalId\": \"AROAIGDTESTANDEXAMPLE\", \"arn\": \"arn:aws:iam::111122223333:role/RoleForModelCopy\", \"accountId\": \"111122223333\", \"userName\": \"RoleForModelCopy\" }, \"attributes\": { \"creationDate\": \"2024-05-07T21:46:28Z\", \"mfaAuthenticated\": \"false\" } }, \"invokedBy\": \"bedrock.amazonaws.com\" }, \"eventTime\": \"2024-05-07T21:49:44Z\", \"eventSource\": \"kms.amazonaws.com\", \"eventName\": \"CreateGrant\", \"awsRegion\": \"us-east-1\", \"sourceIPAddress\": \"bedrock.amazonaws.com\", \"userAgent\": \"bedrock.amazonaws.com\", sourceIPAddress\": \"bedrock.amazonaws.com\", \"userAgent\": \"bedrock.amazonaws.com\", \"requestParameters\": { \"granteePrincipal\": \"bedrock.amazonaws.com\", \"retiringPrincipal\": \"bedrock.amazonaws.com\", \"keyId\": \"arn:aws:kms:us- east-1:111122223333:key/1234abcd-12ab-34cd-56ef-123456SAMPLE\", \"operations\": [ \"Decrypt\", \"CreateGrant\", \"GenerateDataKey\", \"DescribeKey\" ] Data encryption 1103 Amazon Bedrock User Guide }, \"responseElements\": { \"grantId\": \"0ab0ac0d0b000f00ea00cc0a0e00fc00bce000c000f0000000c0bc0a0000aaafSAMPLE\", \"keyId\": \"arn:aws:kms:us- east-1:111122223333:key/1234abcd-12ab-34cd-56ef-123456SAMPLE\" }, \"requestID\": \"ff000af-00eb-00ce-0e00-ea000fb0fba0SAMPLE\", \"eventID\": \"ff000af-00eb-00ce-0e00-ea000fb0fba0SAMPLE\", \"readOnly\": false, \"resources\": [ { \"accountId\": \"111122223333\", \"type\": \"AWS::KMS::Key\", \"ARN\": \"arn:aws:kms:us- east-1:111122223333:key/1234abcd-12ab-34cd-56ef-123456SAMPLE\" } ], \"eventType\": \"AwsApiCall\", \"managementEvent\": true, \"recipientAccountId\": \"111122223333\", \"eventCategory\": \"Management\" } Encryption of training, validation, and output data When you use Amazon Bedrock to run a model customization job, you store the input files in your Amazon S3 bucket. a model customization job, you store the input files in your Amazon S3 bucket. When the job completes, Amazon Bedrock stores the output metrics files in the S3 bucket that you specifed when creating the job and the resulting custom model artifacts in an S3 bucket controlled by AWS. The output files are encrypted with the encryption configurations of the S3 bucket. These are encrypted either with SSE-S3 server-side encryption or with AWS KMS SSE-KMS encryption, depending on how you set up the S3 bucket. Encryption of custom model import Custom Model Import is in preview release for Amazon Bedrock and is subject to change. Amazon Bedrock supports creating a custom model by using the custom model import feature to import models that you have created in other environments, such as Amazon SageMaker. Your Data encryption 1104 Amazon Bedrock User Guide custom imported models are managed and stored by AWS. For more information, see Import a model. For encryption of your custom imported model, Amazon Bedrock provides the following options: \u2022 AWS owned keys \u2013 By default, Amazon Bedrock encrypts custom imported models with AWS owned keys."}
{"question": "Which foundation models are supported by Amazon Bedrock?", "ground_truth": "Amazon Bedrock supports several foundation models, including Amazon Titan, Anthropic Claude, Cohere, AI21 Labs, Meta Llama, Mistral AI, and Stability AI models.", "question_type": "simple", "context": "............................................................................... 32 Foundation model information .................................................................................................... 34 Get model information ............................................................................................................................. 35 Model support by AWS Region ............................................................................................................... 36 Model support by feature ........................................................................................................................ 49 Supported Foundation models ............................................................................................................... 58 Amazon Bedrock model IDs ..................................................................................................................... 62 Base models IDs (on-demand) ........................................................................................................... ............................................................................... 63 Base model IDs (for Provisioned Throughput) ................................................................................ 67 Model inference parameters and responses ......................................................................................... 71 Amazon Titan models .......................................................................................................................... 72 Anthropic Claude models .................................................................................................................. 131 iii Amazon Bedrock User Guide Cohere models.................................................................................................................................... 155 AI21 Labs models ............................................................................................................................... 179 Meta Llama models ............................................................................................................................ ............................................................................... 188 Mistral AI models ................................................................................................................................ 194 Stability AI models ............................................................................................................................. 213 Custom model hyperparameters .......................................................................................................... 241 Amazon Titan text models ............................................................................................................... 242 Amazon Titan Image Generator G1 models ................................................................................. 245 Amazon Titan Multimodal Embeddings G1 .................................................................................. 246 Anthropic Claude 3 models .............................................................................................................."}
{"question": "How does Amazon Bedrock Studio's encryption approach differ from the encryption options available for data sources in Amazon S3, and what additional control does it offer users?", "ground_truth": "Amazon Bedrock Studio uses a two-layer encryption approach, which differs from the options available for data sources in Amazon S3. By default, it uses AWS-owned keys to automatically encrypt data at rest, which users cannot view, manage, or audit. However, users can add a second layer of encryption by choosing a customer-managed key when creating Amazon Bedrock Studio domains. This additional layer provides users with more control, allowing them to establish key policies, manage IAM policies, rotate key cryptographic material, add tags, create aliases, and schedule keys for deletion. In contrast, for data sources in Amazon S3, users have the option to use server-side encryption with Amazon S3-managed keys (SSE-S3) or custom AWS KMS keys, requiring specific IAM policies to be attached to the Amazon Bedrock service role for decryption.", "question_type": "complex", "context": "naged encryption keys (SSE-S3) in the Amazon Simple Storage Service User Guide. If you encrypted your data sources in Amazon S3 with a custom AWS KMS key, attach the following policy to your Amazon Bedrock service role to allow Amazon Bedrock to decrypt your key. Replace region and account-id with the region and account ID to which the key belongs. Replace key- id with the ID of your AWS KMS key. { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"KMS:Decrypt\", ], \"Resource\": [ \"arn:aws:kms:region:account-id:key/key-id\" ], \"Condition\": { \"StringEquals\": { \"kms:ViaService\": [ \"s3.region.amazonaws.com\" ] } } }] } Data encryption 1120 Amazon Bedrock User Guide Permissions to decrypt an AWS Secrets Manager secret for the vector store containing your knowledge base If the vector store containing your knowledge base is configured with an AWS Secrets Manager secret, you can encrypt the secret with a custom AWS KMS key by following the steps at Secret encryption and decryption in AWS Secrets Manager. following the steps at Secret encryption and decryption in AWS Secrets Manager. If you do so, you attach the following policy to your Amazon Bedrock service role to allow it to decrypt your key. Replace region and account-id with the region and account ID to which the key belongs. Replace key-id with the ID of your AWS KMS key. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"kms:Decrypt\" ], \"Resource\": [ \"arn:aws:kms:region:account-id:key/key-id\" ] } ] } Encryption of Amazon Bedrock Studio Amazon Bedrock Studio is in preview release for Amazon Bedrock and is subject to change. Encryption of data at rest by default helps reduce the operational overhead and complexity involved in protecting sensitive data. At the same time, it enables you to build secure applications that meet strict encryption compliance and regulatory requirements. Amazon Bedrock Studio uses default AWS-owned keys to automatically encrypt your data at rest. You can't view, manage, or audit the use of AWS owned keys. your data at rest. You can't view, manage, or audit the use of AWS owned keys. For more information, see AWS owned keys. While you can't disable this layer of encryption or select an alternate encryption type, you can add a second layer of encryption over the existing AWS owned encryption keys by choosing a customer- Data encryption 1121 Amazon Bedrock User Guide managed key when you create your Amazon Bedrock Studio domains. Amazon Bedrock Studio supports the use of a symmetric customer managed keys that you can create, own, and manage to add a second layer of encryption over the existing AWS owned encryption. Because you have full control of this layer of encryption, in it you can perform the following tasks: \u2022 Establish and maintain key policies \u2022 Establish and maintain IAM policies and grants \u2022 Enable and disable key policies \u2022 Rotate key cryptographic material \u2022 Add tags \u2022 Create key aliases \u2022 Schedule keys for deletion For more information, see Customer managed keys. Note Amazon Bedrock Studio automatically enables encryption at rest using AWS owned keys to protect customer data at no charge."}
{"question": "What is the model ID used for AI21 Labs Jurassic-2 Mid in the code?", "ground_truth": "The model ID used for AI21 Labs Jurassic-2 Mid is \"ai21.j2-mid-v1\".", "question_type": "simple", "context": "on Bedrock to run an inference using the input // provided in the request body. func (wrapper InvokeModelWrapper) InvokeJurassic2(ctx context.Context, prompt string) (string, error) { modelId := \"ai21.j2-mid-v1\" body, err := json.Marshal(Jurassic2Request{ Prompt: prompt, MaxTokens: 200, Temperature: 0.5, }) if err != nil { log.Fatal(\"failed to marshal\", err) } output, err := wrapper.BedrockRuntimeClient.InvokeModel(ctx, &bedrockruntime.InvokeModelInput{ ModelId: aws.String(modelId), ContentType: aws.String(\"application/json\"), Body: body, }) if err != nil { ProcessError(err, modelId) } var response Jurassic2Response if err := json.Unmarshal(output.Body, &response); err != nil { log.Fatal(\"failed to unmarshal\", err) } return response.Completions[0].Data.Text, nil } \u2022 For API details, see InvokeModel in AWS SDK for Go API Reference. AI21 Labs Jurassic-2 1328 Amazon Bedrock User Guide Java SDK for Java 2.x Note There's more on GitHub. Find the complete example and learn how to set up and run in the AWS Code Examples Repository. te example and learn how to set up and run in the AWS Code Examples Repository. Use the Invoke Model API to send a text message. // Use the native inference API to send a text message to AI21 Labs Jurassic-2. import org.json.JSONObject; import org.json.JSONPointer; import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider; import software.amazon.awssdk.core.SdkBytes; import software.amazon.awssdk.core.exception.SdkClientException; import software.amazon.awssdk.regions.Region; import software.amazon.awssdk.services.bedrockruntime.BedrockRuntimeClient; public class InvokeModel { public static String invokeModel() { // Create a Bedrock Runtime client in the AWS Region you want to use. // Replace the DefaultCredentialsProvider with your preferred credentials provider. var client = BedrockRuntimeClient.builder() .credentialsProvider(DefaultCredentialsProvider.create()) .region(Region.US_EAST_1) .build(); // Set the model ID, e.g., Jurassic-2 Mid. var modelId = \"ai21.j2-mid-v1\"; // The InvokeModel API uses the model's native payload. lId = \"ai21.j2-mid-v1\"; // The InvokeModel API uses the model's native payload. // Learn more about the available inference parameters and response fields at: // https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters- jurassic2.html var nativeRequestTemplate = \"{ \\\"prompt\\\": \\\"{{prompt}}\\\" }\"; AI21 Labs Jurassic-2 1329 Amazon Bedrock User Guide // Define the prompt for the model. var prompt = \"Describe the purpose of a 'hello world' program in one line.\"; // Embed the prompt in the model's native request payload. String nativeRequest = nativeRequestTemplate.replace(\"{{prompt}}\", prompt); try { // Encode and send the request to the Bedrock Runtime. var response = client.invokeModel(request -> request .body(SdkBytes.fromUtf8String(nativeRequest)) .modelId(modelId) ); // Decode the response body. var responseBody = new JSONObject(response.body().asUtf8String()); // Retrieve the generated text from the model's response. var text = new JSONPointer(\"/completions/0/data/ text\").queryFrom(responseBody).toString(); System.out.println(text); return text; } catch (SdkClientException e) { System.err.printf(\"ERROR: Can't invoke '%s'."}
